question,oracle_context,contexts
What is the email address of Jean Lahoud?,"Jean Lahoud2
jean.lahoud@mbzuai.ac.ae","Jean Lahoud2
jean.lahoud@mbzuai.ac.ae

arXiv:2502.17429v3  [cs.CV]  21 Nov 2025
THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced
3D Instance Segmentation
Vishal Thengane1
v.thengane@surrey.ac.uk
Jean Lahoud2
jean.lahoud@mbzuai.ac.ae
Hisham Cholakkal2
hisham.cholakkal@mbzuai.ac.ae
Rao Muhammad Anwer2
rao.anwer@mbzuai.ac.ae
Lu Yin1
l.yin@surrey.ac.uk
Xiatian Zhu1
xiatian.zhu@surrey.ac.uk
Salman Khan2, 3
salman.khan@mbzuai.ac.ae
1 University of Surrey,
Guildford, UK
2 Mohamed bin Zayed University of
Artificial Intelligence,
Abu Dhabi, UAE
3 Australian National University,
Canberra, Australia
Abstract
While 3D instance segmentation (3DIS) has advanced significantly, most existing
methods assume that all object classes are known in advance and uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new
classes emerge gradually and exhibit natural imbalance. Although some approaches ad-
dress the emergence of new classes, they often overlook class imbalance, which leads
to suboptimal performance, particularly on rare categories. To tackle this, we propose
CLIMB-3D, a unified framework forCLass-incrementalImbalance-aware3DIS. Build-
ing upon established exemplar replay (ER) strategies, we show that ER alone is insuf-
ficient to achieve robust performance under memory constraints. To mitigate this, we
introduce a novel pseudo-label generator (PLG) that extends supervision to previously
learned categories by leveraging predictions from a frozen model trained on prior tasks. Despite its promise, PLG tends to be biased towards frequent classes. Therefore, we
propose a class-balanced re-weighting (CBR) scheme that estimates object frequencies
from pseudo-labels and dynamically adjusts training bias, without requiring access to
past data. We design and evaluate three incremental scenarios for 3DIS on the challeng-
ing ScanNet200 dataset and additionally validate our method for semantic segmentation
on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by
up to 16.76% mAP for instance segmentation and approximately 30% mIoU for semantic
segmentation, demonstrating strong generalisation across both frequent and rare classes. Code is available at:https://github.com/vgthengane/CLIMB3D. Â© 2025.

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710."
Where is Salman Khan affiliated with?,"Salman Khan2, 3
salman.khan@mbzuai.ac.ae","[30] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for
continual learning.arXiv preprint arXiv:1710.10368, 2017. [31] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence
of similar and dissimilar tasks.NeurIPS, 33:18493â€“18504, 2020. [32] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019. [33] Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and
Roberto Togneri. Cost-sensitive learning of deep feature representations from im-
balanced data.IEEE transactions on neural networks and learning systems, 29(8):
3573â€“3587, 2017. [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 33:18661â€“18673, 2020. [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-
jardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Pro-
ceedings of the national academy of sciences, 114(13):3521â€“3526, 2017. [36] Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual
learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024. [37] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3d instance
segmentation via multi-task metric learning. InICCV, pages 9256â€“9266, 2019. [38] Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and
Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint
and sampling. InInternational Conference on Robotics and Automation (ICRA), pages
5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029. [39] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,
and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022. [40] Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing
Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In
CVPR, pages 5212â€“5221, 2021. [41] Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-
tailed visual recognition. InICCV, pages 630â€“639, 2021. [42] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In
ICML, pages 3925â€“3934.

[4] Mohamed El Amine Boudjoghra, Salwa Al Khatib, Jean Lahoud, Hisham Cholakkal,
Rao Anwer, Salman H Khan, and Fahad Shahbaz Khan. 3d indoor instance segmenta-
tion in an open-world.NeurIPS, 36, 2024. [5] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calder-
ara. Dark experience for general continual learning: a strong, simple baseline.NeurIPS,
33:15920â€“15930, 2020. [6] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning
imbalanced datasets with label-distribution-aware margin loss.NeurIPS, 32, 2019. [7] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrastive continual learning. In
ICCV, pages 9516â€“9525, 2021. [8] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique.Journal of artificial intelligence
research, 16:321â€“357, 2002. [9] Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and Xinggang Wang. Hierarchical
aggregation for 3d instance segmentation. InICCV, pages 15467â€“15476, 2021. [10] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification is not
all you need for semantic segmentation.NeurIPS, 34:17864â€“17875, 2021. [11] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Gird-
har. Masked-attention mask transformer for universal image segmentation. InCVPR,
pages 1290â€“1299, 2022. [12] Julian Chibane, Francis Engelmann, Tuan Anh Tran, and Gerard Pons-Moll. Box2mask: Weakly supervised 3d semantic instance segmentation using bounding
boxes. InECCV, pages 681â€“699.

arXiv:2502.17429v3  [cs.CV]  21 Nov 2025
THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced
3D Instance Segmentation
Vishal Thengane1
v.thengane@surrey.ac.uk
Jean Lahoud2
jean.lahoud@mbzuai.ac.ae
Hisham Cholakkal2
hisham.cholakkal@mbzuai.ac.ae
Rao Muhammad Anwer2
rao.anwer@mbzuai.ac.ae
Lu Yin1
l.yin@surrey.ac.uk
Xiatian Zhu1
xiatian.zhu@surrey.ac.uk
Salman Khan2, 3
salman.khan@mbzuai.ac.ae
1 University of Surrey,
Guildford, UK
2 Mohamed bin Zayed University of
Artificial Intelligence,
Abu Dhabi, UAE
3 Australian National University,
Canberra, Australia
Abstract
While 3D instance segmentation (3DIS) has advanced significantly, most existing
methods assume that all object classes are known in advance and uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new
classes emerge gradually and exhibit natural imbalance. Although some approaches ad-
dress the emergence of new classes, they often overlook class imbalance, which leads
to suboptimal performance, particularly on rare categories. To tackle this, we propose
CLIMB-3D, a unified framework forCLass-incrementalImbalance-aware3DIS. Build-
ing upon established exemplar replay (ER) strategies, we show that ER alone is insuf-
ficient to achieve robust performance under memory constraints. To mitigate this, we
introduce a novel pseudo-label generator (PLG) that extends supervision to previously
learned categories by leveraging predictions from a frozen model trained on prior tasks. Despite its promise, PLG tends to be biased towards frequent classes. Therefore, we
propose a class-balanced re-weighting (CBR) scheme that estimates object frequencies
from pseudo-labels and dynamically adjusts training bias, without requiring access to
past data. We design and evaluate three incremental scenarios for 3DIS on the challeng-
ing ScanNet200 dataset and additionally validate our method for semantic segmentation
on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by
up to 16.76% mAP for instance segmentation and approximately 30% mIoU for semantic
segmentation, demonstrating strong generalisation across both frequent and rare classes. Code is available at:https://github.com/vgthengane/CLIMB3D. Â© 2025.

Salman Khan2, 3
salman.khan@mbzuai.ac.ae"
What is the title of the paper by Vishal Thengane?,THENGANE ET AL.: CLIMB-3D1,"PMLR, 2019. 14THENGANE ET AL.: CLIMB-3D
[43] Zhizhong Li and Derek Hoiem. Learning without forgetting.IEEE PAMI, 40(12):
2935â€“2947, 2017. [44] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmenta-
tion in 3d scenes using semantic superpoint tree networks. InICCV, pages 2783â€“2792,
2021. [45] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss
for dense object detection. InICCV, pages 2980â€“2988, 2017.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

arXiv:2502.17429v3  [cs.CV]  21 Nov 2025
THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced
3D Instance Segmentation
Vishal Thengane1
v.thengane@surrey.ac.uk
Jean Lahoud2
jean.lahoud@mbzuai.ac.ae
Hisham Cholakkal2
hisham.cholakkal@mbzuai.ac.ae
Rao Muhammad Anwer2
rao.anwer@mbzuai.ac.ae
Lu Yin1
l.yin@surrey.ac.uk
Xiatian Zhu1
xiatian.zhu@surrey.ac.uk
Salman Khan2, 3
salman.khan@mbzuai.ac.ae
1 University of Surrey,
Guildford, UK
2 Mohamed bin Zayed University of
Artificial Intelligence,
Abu Dhabi, UAE
3 Australian National University,
Canberra, Australia
Abstract
While 3D instance segmentation (3DIS) has advanced significantly, most existing
methods assume that all object classes are known in advance and uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new
classes emerge gradually and exhibit natural imbalance. Although some approaches ad-
dress the emergence of new classes, they often overlook class imbalance, which leads
to suboptimal performance, particularly on rare categories. To tackle this, we propose
CLIMB-3D, a unified framework forCLass-incrementalImbalance-aware3DIS. Build-
ing upon established exemplar replay (ER) strategies, we show that ER alone is insuf-
ficient to achieve robust performance under memory constraints. To mitigate this, we
introduce a novel pseudo-label generator (PLG) that extends supervision to previously
learned categories by leveraging predictions from a frozen model trained on prior tasks. Despite its promise, PLG tends to be biased towards frequent classes. Therefore, we
propose a class-balanced re-weighting (CBR) scheme that estimates object frequencies
from pseudo-labels and dynamically adjusts training bias, without requiring access to
past data. We design and evaluate three incremental scenarios for 3DIS on the challeng-
ing ScanNet200 dataset and additionally validate our method for semantic segmentation
on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by
up to 16.76% mAP for instance segmentation and approximately 30% mIoU for semantic
segmentation, demonstrating strong generalisation across both frequent and rare classes. Code is available at:https://github.com/vgthengane/CLIMB3D. Â© 2025.

THENGANE ET AL.: CLIMB-3D1"
Which university is Lu Yin associated with?,"Lu Yin1
l.yin@surrey.ac.uk","Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019.

Springer,
2016.

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

Lu Yin1
l.yin@surrey.ac.uk"
What is the proposed framework for Class-Incremental Imbalance-aware 3D Instance Segmentation?,"To tackle this, we propose CLIMB-3D, a unified framework for CLass-incremental Imbalance-aware 3DIS.","arXiv:2502.17429v3  [cs.CV]  21 Nov 2025
THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced
3D Instance Segmentation
Vishal Thengane1
v.thengane@surrey.ac.uk
Jean Lahoud2
jean.lahoud@mbzuai.ac.ae
Hisham Cholakkal2
hisham.cholakkal@mbzuai.ac.ae
Rao Muhammad Anwer2
rao.anwer@mbzuai.ac.ae
Lu Yin1
l.yin@surrey.ac.uk
Xiatian Zhu1
xiatian.zhu@surrey.ac.uk
Salman Khan2, 3
salman.khan@mbzuai.ac.ae
1 University of Surrey,
Guildford, UK
2 Mohamed bin Zayed University of
Artificial Intelligence,
Abu Dhabi, UAE
3 Australian National University,
Canberra, Australia
Abstract
While 3D instance segmentation (3DIS) has advanced significantly, most existing
methods assume that all object classes are known in advance and uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new
classes emerge gradually and exhibit natural imbalance. Although some approaches ad-
dress the emergence of new classes, they often overlook class imbalance, which leads
to suboptimal performance, particularly on rare categories. To tackle this, we propose
CLIMB-3D, a unified framework forCLass-incrementalImbalance-aware3DIS. Build-
ing upon established exemplar replay (ER) strategies, we show that ER alone is insuf-
ficient to achieve robust performance under memory constraints. To mitigate this, we
introduce a novel pseudo-label generator (PLG) that extends supervision to previously
learned categories by leveraging predictions from a frozen model trained on prior tasks. Despite its promise, PLG tends to be biased towards frequent classes. Therefore, we
propose a class-balanced re-weighting (CBR) scheme that estimates object frequencies
from pseudo-labels and dynamically adjusts training bias, without requiring access to
past data. We design and evaluate three incremental scenarios for 3DIS on the challeng-
ing ScanNet200 dataset and additionally validate our method for semantic segmentation
on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by
up to 16.76% mAP for instance segmentation and approximately 30% mIoU for semantic
segmentation, demonstrating strong generalisation across both frequent and rare classes. Code is available at:https://github.com/vgthengane/CLIMB3D. Â© 2025.

[78] Biao Zhang and Peter Wonka. Point cloud instance segmentation using probabilistic
embeddings. InCVPR, pages 8883â€“8892, 2021. [79] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsuper-
vised lifelong learning.Neural Networks, 149:95â€“106, 2022. [80] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang,
and Jiaya Jia. Understanding imbalanced semantic segmentation through neural col-
lapse. InCVPR, pages 19550â€“19560, 2023. [81] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch
network with cumulative learning for long-tailed visual recognition. InCVPR, pages
9719â€“9728, 2020. [82] Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang
Jiang. Balanced contrastive learning for long-tailed visual recognition. InCVPR, pages
6908â€“6917, 2022. THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced 3D Instance
Segmentation
Supplementary Material
Appendix A Illustrating Incremental scenarios
Frequent Scenario Semantic Scenario
background
frequent common
rare semantic 
cluster 1
semantic 
cluster 2
semantic 
cluster 3
background
random 
cluster 1
random 
cluster 2
random 
cluster 3
background
Random Scenario
Figure 3: Tasks are grouped into incremental scenarios based on object frequency, semantic
similarity, and random assignment. , , and denote different tasks; shapes indicate
object categories; marks the background.Left:Grouped by category frequency.Middle:
Grouped by semantic similarity (e.g., similar shapes).Right:Randomly grouped, mixing
semantic and frequency variations. Appendix B Per Phase Analysis
We extend the analysis from Tab.

To tackle this, we propose CLIMB-3D, a unified framework for CLass-incremental Imbalance-aware 3DIS.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019."
What does PLG stand for in the context of the document?,"Although some approaches address the emergence of new classes, they often overlook class imbalance, which leads to suboptimal performance, particularly on rare categories. To mitigate this, we introduce a novel pseudo-label generator (PLG) that extends supervision to previously learned categories by leveraging predictions from a frozen model trained on prior tasks.","[30] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for
continual learning.arXiv preprint arXiv:1710.10368, 2017. [31] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence
of similar and dissimilar tasks.NeurIPS, 33:18493â€“18504, 2020. [32] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019. [33] Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and
Roberto Togneri. Cost-sensitive learning of deep feature representations from im-
balanced data.IEEE transactions on neural networks and learning systems, 29(8):
3573â€“3587, 2017. [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 33:18661â€“18673, 2020. [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-
jardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Pro-
ceedings of the national academy of sciences, 114(13):3521â€“3526, 2017. [36] Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual
learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024. [37] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3d instance
segmentation via multi-task metric learning. InICCV, pages 9256â€“9266, 2019. [38] Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and
Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint
and sampling. InInternational Conference on Robotics and Automation (ICRA), pages
5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029. [39] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,
and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022. [40] Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing
Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In
CVPR, pages 5212â€“5221, 2021. [41] Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-
tailed visual recognition. InICCV, pages 630â€“639, 2021. [42] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In
ICML, pages 3925â€“3934.

arXiv:2502.17429v3  [cs.CV]  21 Nov 2025
THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced
3D Instance Segmentation
Vishal Thengane1
v.thengane@surrey.ac.uk
Jean Lahoud2
jean.lahoud@mbzuai.ac.ae
Hisham Cholakkal2
hisham.cholakkal@mbzuai.ac.ae
Rao Muhammad Anwer2
rao.anwer@mbzuai.ac.ae
Lu Yin1
l.yin@surrey.ac.uk
Xiatian Zhu1
xiatian.zhu@surrey.ac.uk
Salman Khan2, 3
salman.khan@mbzuai.ac.ae
1 University of Surrey,
Guildford, UK
2 Mohamed bin Zayed University of
Artificial Intelligence,
Abu Dhabi, UAE
3 Australian National University,
Canberra, Australia
Abstract
While 3D instance segmentation (3DIS) has advanced significantly, most existing
methods assume that all object classes are known in advance and uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new
classes emerge gradually and exhibit natural imbalance. Although some approaches ad-
dress the emergence of new classes, they often overlook class imbalance, which leads
to suboptimal performance, particularly on rare categories. To tackle this, we propose
CLIMB-3D, a unified framework forCLass-incrementalImbalance-aware3DIS. Build-
ing upon established exemplar replay (ER) strategies, we show that ER alone is insuf-
ficient to achieve robust performance under memory constraints. To mitigate this, we
introduce a novel pseudo-label generator (PLG) that extends supervision to previously
learned categories by leveraging predictions from a frozen model trained on prior tasks. Despite its promise, PLG tends to be biased towards frequent classes. Therefore, we
propose a class-balanced re-weighting (CBR) scheme that estimates object frequencies
from pseudo-labels and dynamically adjusts training bias, without requiring access to
past data. We design and evaluate three incremental scenarios for 3DIS on the challeng-
ing ScanNet200 dataset and additionally validate our method for semantic segmentation
on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by
up to 16.76% mAP for instance segmentation and approximately 30% mIoU for semantic
segmentation, demonstrating strong generalisation across both frequent and rare classes. Code is available at:https://github.com/vgthengane/CLIMB3D. Â© 2025.

It may be distributed unchanged freely in print or electronic forms. 2THENGANE ET AL.: CLIMB-3D
1 Introduction
Table Chair Couch Pillow Coffee Table Sofa Chair Recycling BinCabinet Unknown
Task-1 Task-2 Task-3
Figure 1:Overview of the CI-3DIS setting.New object categories are introduced incre-
mentally with each task. After every phase, the model must recognise both newly added
and previously learned classes. For instance, in Task 2, categories such asPillow,Coffee
Table, andSofa Chairare introduced, while the model is expected to retain recognition
of earlier classes likeTable,Chair, andCouch. 3D instance segmentation (3DIS) is a fundamental task in computer vision that involves
identifying precise object boundaries and class labels in 3D space, with broad applications in
graphics, robotics, and augmented reality [4, 39]. Traditional 3DIS methods, including top-
down [28, 70, 78], bottom-up [26, 73], and transformer-based approaches [59], perform well
under the assumption that all classes are available and balanced during training. However,
this assumption does not hold in real-world environments, where new object classes emerge
over time and exhibit natural imbalance. This gap motivates the need for Class-Incremental Learning (CIL) [17, 54], which sup-
ports learning new categories while retaining knowledge of previous ones [51]. While CIL
has seen success in 2D image tasks [1, 43, 54, 60], extensions to 3D point clouds remain
limited, often focusing on object-level classification [13, 18, 48]. Recent works on scene-
level class-incremental 3DIS (CI-3DIS) [4] and class-incremental semantic segmentation
(CI-3DSS) [74] show promise, but depend heavily on large exemplar [56] and overlook class
imbalance, limiting their practicality. To address this, we proposeCLIMB-3D, a unified framework forCLass-incremental
IMBalance-aware3DIS that jointly tackles catastrophic forgetting and class imbalance. The
framework begins with Exemplar Replay (ER), storing a small number of samples from
past classes for later replay. However, this alone does not yield promising results under the
strict memory constraints required in CI-3DIS. To address this, we introduce a Pseudo-Label
Generator (PLG), which uses a frozen model from previous task to generate supervision for
earlier classes. However, we observed that PLG tends to favour frequent classes while ig-
noring under-represented ones. To mitigate this, we propose a Class-Balanced Re-weighting
(CBR), which estimates class frequencies from the pseudo-labels to enhance the learning of
rare classes without accessing past data. Together, these components form a practical and
effective solution for real-world 3DIS. To evaluate CLIMB-3D, we designed three benchmark scenarios for CI-3DIS based on
the ScanNet200 dataset [57]. These scenarios simulate incremental learning setup under nat-
ural class imbalance, where new classes emerge based on (A) object frequency, (B) semantic
similarity, or (C) random grouping. Additionally, for comparison with existing methods,
we evaluate our approach on CI-3DSS using the ScanNetV2 dataset [16]. Experimental re-
sults show that our method significantly reduces forgetting and improves performance across
both frequent and rare categories, outperforming previous methods in both settings. Fig. 1
illustrates the CI-3DIS setup. THENGANE ET AL.: CLIMB-3D3
In summary, our contributions are: (i) a novel problem setting of imbalanced class-
incremental 3DIS with an effective method to balance learning and mitigate forgetting; (ii)
three benchmarks modelling continual object emergence with natural imbalance; and (iii)
strong experimental results, achieving up to 16.76% mAP improvement over baselines. 2 Related Work
3D Instance Segmentation.Various methods have been proposed for 3DIS. Grouping-
based approaches adopt a bottom-up pipeline that learns latent embeddings for point cluster-
ing [9, 21, 22, 29, 37, 44, 70, 78]. In contrast, proposal-based methods follow a top-down
strategy by detecting 3D boxes and segmenting objects within them [19, 26, 46, 73, 76]. Recently, transformer-based models [68] have also been applied to 3DIS and 3DSS [59, 64],
inspired by advances in 2D vision [10, 11]. However, these approaches require full anno-
tations for all classes and are not designed for progressive learning, where only new class
annotations are available and previous data is inaccessible. Another line of work aims to re-
duce annotation costs by proposing weakly supervised 3DIS methods based on sparse cues
[12, 27, 72]. While effective with limited annotations, these methods assume a fixed set of
classes and are susceptible to catastrophic forgetting in incremental settings. Incremental Learning.Continual learning involves training models sequentially to miti-
gate catastrophic forgetting. One common strategy is model regularisation, which constrains
parameter updates using techniques such as elastic weight consolidation or knowledge dis-
tillation [1, 24, 35, 43, 60]. Another approach is exemplar replay, where past data is stored
or generated to rehearse older tasks while learning new ones [5, 7, 30, 54]. A third direc-
tion involves dynamically expanding the model architecture or using modular sub-networks
to accommodate new tasks without interfering with old ones [31, 42, 53, 58, 71, 79]. Re-
cent efforts in class-incremental 3D segmentation [63, 74] have shown early promise but
often rely on basic architectures and focus primarily on semantic segmentation. Other works
[4, 36] explore continual learning in 3D settings, though they often depend on large memory
buffers. In contrast, our work introduces a tailored framework for 3D instance segmentation
that effectively transfers knowledge across tasks, even under class imbalance. Long-tailed Recognition.Imbalanced class distributions lead to poor recognition of rare
(long-tail) classes. To address this, re-sampling methods balance the data via input [8, 20, 50,
52, 61] or feature space [3, 14, 40], avoiding naÃ¯ve under-/over-sampling. Loss re-weighting
offers another direction, using class-based [6, 15, 23, 32, 33, 69] or per-example [45, 55, 62]
adjustments to ensure fair contribution. Parameter regularisation improves generalisation
through weight constraints [2], albeit requiring careful tuning. Other approaches leverage
transfer learning [77, 81], self-supervision [41, 75], or contrastive learning [34, 41, 82] to
improve rare-class representations. Long-tailed recognition is well-studied in 2D with large-
scale datasets [15, 49, 67]; in 3D, ScanNet200 [16, 57] enables similar efforts. Prior 3D
work focused on re-weighting, re-sampling, and transfer learning [57], while regularisation
remains underexplored. CeCo [80] balances class centres via auxiliary loss but overlooks
sampling and augmentation. Lahoud et al. [38] propose adaptive classifier regularisation
for 3D segmentation, outperforming prior approaches without threshold tuning. However,
neither method considers incremental learning. In contrast, we jointly address long-tail and
continual 3D semantic segmentation. 4THENGANE ET AL.: CLIMB-3D
Feature 
Backbone
Transformer 
Decoder
Feature 
Backbone
Transformer 
Decoder
(ð‘·,ð’€ð‘¡)
Current model (ð›·
ð‘¡
)
Pseudo Label Generator (PLG) 
Selection
à·¨ð‘Œð¾
ð‘¡
â„’CIâˆ’3DIS
à·¨ð‘Œð¾
ð‘¡,ð‘¤
à´¤ð‘Œð¾
ð‘¡
à· ð‘Œð‘¡
(ð·ð‘¡)
Current Dataset 
Exemplar 
Replay (ER)
+
~
Class-Balanced  
Re-weighting 
(CBR)
1
ð‘“(ð‘)
Figure 2: Overview ofCLIMB-3Dfor CI-3DIS. The model incrementally learns new classes
across sequential phases. During taskt, point cloudsPand their corresponding labelsY t are
sampled from a combination of the current training datasetD t and Exemplar Replay (ER),
which maintains a small memory of past examples. These are then passed to the current
modelÎ¦ t to produce predicted labelsY t . The Pseudo-Label Generator (PLG) selects the
top-Kpredictions from the previous modelÎ¦ tâˆ’1 . These pseudo-labels are then weighted
based on class frequencyf(c)using Class-Balanced Re-weighting (CBR), and the top-Kre-
weighted labels are selected to form the balanced pseudo-label set Â¯Yt . This pseudo-label set
is then concatenated with the ground-truth labels to form a final augmented supervision set
Yt for taskt, which is used to optimise the modelÎ¦ t using Eq. (3). 3 Methodology
3.1 Problem Formulation
3DIS.The objective of this task is to detect and segment each object instance within a point
cloud. Formally, the training dataset is denoted asD={(P i,Yi)}N
i=1, whereNis the number
of training samples. Each inputP i âˆˆR MÃ—6 is a coloured point cloud consisting ofMpoints,
where each point is represented by its 3D coordinates and RGB values. The corresponding
annotationY i ={(m i,j ,ci,j )}Ji
j=1 containsJ i object instances, wheremi,j âˆˆ {0,1}M is a binary
mask indicating which points belong to thej-th instance, andc i,j âˆˆ C={1,...,C}is the
semantic class label of that instance. GivenP i, the modelÎ¦predicts instance-level outputs
Ë†Yi ={(Ë†mi,j ,Ë†ci,j )}
Ë†Ji
j=1, where Ë†mi,j and Ë†ci,j denote the predicted mask and category label for
thej-th instance, respectively. The number of predicted instances Ë†Ji varies depending on the
modelâ€™s inference. The model is optimised using the following objective:
L3DIS(D;Î¦) = 1
|D| âˆ‘
(P,Y)âˆˆD
1
|Y| âˆ‘
(mj,cj)âˆˆY
(Lmask(mj,Ë†mj)+Î»Â·L cls(cj,Ë†cj)),(1)
whereL cls andL mask are the average classification and mask losses over all instances, andÎ»
controls their trade-off. CI-3DIS.Unlike conventional 3DIS, CI-3DIS involves sequential learning, where object
categories are introduced incrementally overTtraining tasks. Each tasktintroduces a dis-
joint set of classesC t , withC= ST
t=1 Ct andC t âˆ©C tâ€²
=/ 0 fortÌ¸=t â€². During each taskt, the THENGANE ET AL.: CLIMB-3D5
model receives a datasetD t ={(P i,Yt
i)}N
i=1, where each coloured point cloudP i âˆˆR MÃ—6
containsMpoints, and the corresponding annotationY t
i ={(m t
i,j ,ct
i,j )}
Jt
i
j=1 includes instance
masksm t
i,j âˆˆ {0,1}M and semantic class labelsc t
i,j âˆˆ Ct . The modelÎ¦ t , initialized from
Î¦tâˆ’1 , is trained onD t to predict instance-level outputs Ë†Yt
i ={(Ë†mt
i,j ,Ë†ct
i,j )}
Ë†Jt
i
j=1, where each
predicted semantic label Ë†ct
i,j belongs to the cumulative set of all classes observed so far:St
k=1 Ck. The key challenge in this setup is to learn new classes without forgetting those
encountered in previous tasks, despite limited supervision and the absence of past labels. 3.2 Method: CLIMB-3D
Overview.As shown in Fig. 2 and formulated in Sec. 3.1, the proposed framework for
CI-3DIS follows aphase-wisetraining strategy. In each phase, the model is exposed to a
carefully curated subset of the dataset, simulating real-world scenarios discussed in Sec. 3.3. Training naÃ¯vely on such phased data leads tocatastrophic forgetting[51], where the model
forgets prior knowledge when learning new tasks. To address this, we first incorporateEx-
emplar Replay(ER) [5], a strategy inspired by 2D incremental methods [1, 43, 54] and
recently extended to 3D settings [4], to mitigate forgetting by storing a small set of repre-
sentative samples from earlier phases. However, ER alone is insufficient to achieve robust
performance. To address this, two additional components are introduced: aPseudo-Label
Generator(PLG), which leverages a frozen model to generate supervision signals for previ-
ously seen classes, and aClass-Balanced Re-weighting(CBR) module that compensates for
class imbalance across phases. Each of these components is described in detail below 1. Exemplar Replay (ER).Inspired by Buzzega et al. [5], ER addresses the issue of catas-
trophic forgetting by allowing the model to retain a subset of data from earlier phases. During phaset, the model is trained not only on the current task dataD t , but also on a
set of exemplarsE 1:tâˆ’1, collected from previous phases. These exemplars are accumulated
incrementally:E 1:tâˆ’1 =E 1 âˆªÂ·Â·Â·âˆªE tâˆ’1 .Here,E t denotes the exemplar subset stored after
phaset, and|E t |denotes the exemplar replay size. The combined replay dataset is denoted
as:D t
ER =D t âˆªE 1:tâˆ’1.Epoch training consists of two stages: first, the model is trained on
the current data,D t ; then it is trained using exemplars fromE 1:tâˆ’1. While previous CI-3DIS approaches rely on large exemplar sets to mitigate forgetting [4],
such strategies become impractical under memory constraints. Consequently, although ER
aids knowledge retention, it is insufficient on its own for robust performance with a limited
exemplar budget. Therefore, PLG and CBR are introduced to better preserve past represen-
tations and enhance generalisation across phases. Pseudo-Label Generator (PLG).In the CI-3DIS setting, during phaset(wheret>1),
although ground-truth labels for previously seen classes are unavailable, the model from the
previous phaseÎ¦ tâˆ’1 , which preserves knowledge of past tasks, is retained. We use this
model to generate pseudo-labels for previously seen classes, thereby providing approximate
supervision during training. This allows the current modelÎ¦t , to retain prior knowledge and
reduce forgetting, even without access to ground-truth annotations. Given a point cloud and label pair(P,Y t ), fromD t , the previous modelÎ¦ tâˆ’1 , generates
pseudo-labels for the previously seen classes as Ë†Y1:tâˆ’1 =Î¦ tâˆ’1 (P), which we denote as ËœYt
1For clarity, modifications introduced by each component (ER, PLG, and CBR) are highlighted in green, blue,
and red, respectively. 6THENGANE ET AL.: CLIMB-3D
for brevity. However, we observe that utilising all predictions fromÎ¦tâˆ’1 introduces noisy or
incorrect labels, which degrade overall performance. To mitigate this, we select the top-K
most confident instance predictions, denoted as ËœYt
K, and concatenate them with the ground-
truth labels of the current task to obtain the supervision set: Â¯Yt =Y t âˆ¥ ËœYt
K. Following this,
the loss is computed over both the current taskâ€™s ground-truth labels and the top-Kconfi-
dent pseudo-labels from earlier phases, enabling the model to retain prior knowledge while
adapting to new classes. Class-Balanced Re-weighting (CBR).While ER and PLG contribute to preserving prior
knowledge, we observe that the modelâ€™s predictions are biased towards frequent classes,
resulting in the forgetting of rare categories. This issue is further amplified by the top-K
pseudo-labels selected by PLG, which predominantly represent dominant classes in the data. To address this, we propose aClass-Balanced Re-weighting(CBR) scheme that compensates
for class imbalance using object frequency statistics. At taskt, only the current datasetD t
and its label distributionp t (c)over classescâˆˆ Ct are available. Datasets and class distribu-
tions from earlier tasks{D i,p i(c)}tâˆ’1
i=1 are no longer accessible, which makes it challenging
to directly account for class imbalance across all previously seen tasks. Therefore, we pro-
pose to leverage the pseudo-label predictions of the frozen modelÎ¦ tâˆ’1 as a proxy for the
class distribution across previously learned categories. During each training iteration, the previous modelÎ¦ tâˆ’1 is applied to the current inputP
to generate pseudo-labels ËœYt , from which we accumulate class-wise frequency statistics for
previously learned classes. LetC1:t = St
i=1 Ci represent the union of all classes seen up to task
t. At the end of each epoch, we compute the overall class frequencyfâˆˆR |C1:t | by combining
the pseudo-label distribution Ëœpt (c)derived from ËœYt with the ground-truth label distribution
pt (c)from the current task. The resulting frequency vectorf= [f(c)] câˆˆC1:t is then used to
compute class-wise weightsw= [w(c)] câˆˆC1:t , which guide balanced pseudo-label selection
and help the model focus on rare classes. In the next epoch, the modelÎ¦ tâˆ’1 produces soft pseudo-label predictions ËœYt (as de-
scribed in PLG) , where each component ËœYt [c]denotes the confidence score for classcâˆˆ
C1:tâˆ’1. To promote balanced pseudo-label selection, class-wise re-weighting is applied using
w(c) = 1
f(c)+Îµ , whereÎµis a small constant for numerical stability. The adjusted predictions
are computed by applying the class-wise weights as: ËœYt,w [c] =w(c)Â· ËœYt [c],âˆ€câˆˆ C 1:tâˆ’1. Top-Kselection is then applied to both the original and the re-weighted scores, yielding
two pseudo-label sets: ËœYt
K and ËœYt,w
K , respectively. The final augmented target set, Yt , is
constructed by concatenating the ground-truth labelsY t with both sets of pseudo-labels:
Yt =Y t âˆ¥ ËœYt
K âˆ¥ ËœYt,w
K .(2)
Although the above re-weighting mitigates bias from prior tasks, it does not address class
imbalance within the current taskt. To resolve this, the class-balanced weightsw c are ex-
tended to incorporate statistics from both previously seen and current classes, i.e., overC 1:t
rather than onlyC 1:tâˆ’1. The updated weights, denoted asw â€²
c, are used to re-weights both
pseudo-label selection and classification loss, ensuring balanced supervision across all seen
categories, including rare ones. This unified strategy reduces bias and enhances performance
across phases. The pseudo-code for constructing the supervision set in Eq. (2), which incor-
porates both PLG and CBR components, is provided in Appendix A. Final Objective.The final training objective ofCLIMB-3D, incorporating ER, PLG, and THENGANE ET AL.: CLIMB-3D7
CBR, is formulated in Eq. (1) as:
L(Dt
ER;Î¦t ) = 1
|Dt
ER| âˆ‘
(P,Yt )âˆˆDt
ER
1
|Yt | âˆ‘
(mt
j,ct
j)âˆˆYt
 
Lmask(mt
j,Ë†mt
j)+w â€²â€²
c Â·Lcls(ct
j,Ë†ct
j)

,(3)
wherew â€²â€²
c =w â€²
c Â·Î» cls denotes the scaled weight vector, andÎ» cls is a hyperparameter for bal-
ancing the loss terms. 3.3 Benchmarking Incremental Scenarios
While CI-3DIS methods offer a wide range of practical applications, they frequently rely
on the assumption of uniform sample distribution, which rarely holds in real-world settings. In practice, the number of object categories, denoted byC, is often large and characterised
by substantial variability in category frequency, shape, structure, and size. To address these
challenges, we propose three incremental learning scenarios, each designed to capture a
different facet of real-world complexity. The design of these scenarios is detailed below; for
further information and illustrations, refer to Appendix E of the supplementary material. Split-A: Frequency Scenarios.This scenario acknowledges that datasets are often la-
belled based on the frequency of categories. To accommodate this, we propose a split where
the model learns from the most frequent categories and subsequently incorporates the less
frequent ones in later stages. By prioritising the training of frequently occurring categories,
the model can establish a strong foundation before expanding to handle rare categories. Split-B: Semantic Scenarios.Beyond frequency, semantic similarity is crucial in real-
world deployments. While objects often share visual or functional traits, models may en-
counter semantically different categories in new environments. To simulate this challenge,
we introduce theSplit-Bscenario. Here, categories are grouped based on their semantic
relationships, and the model is incrementally trained on one semantic group at a time. This
setup encourages the model to generalise across semantically similar categories and adapt
more effectively when exposed to new ones. Unlike theSplit-Ascenario, which organises
learning based on category frequency, theSplit-Bscenario may contain both frequent and
infrequent categories within a single task, focusing on semantic continuity and transfer. Split-C: Random Scenarios.In some cases, data labelling is driven by the availability of
objects rather than predefined criteria. To capture this, we introduce theSplit-Cscenario,
which represents a fully random setting where any class can appear in any task, resulting in
varying levels of class imbalance. By exposing the model to such diverse and unpredictable
distributions, we aim to improve its robustness in real-world situations where labelled data
availability is inconsistent. These incremental scenarios are designed to provide a more realistic representation of
object distributions, frequencies, and dynamics encountered in the real world. 4 Experiments
4.1 Setup
Datasets.We evaluateCLIMB-3Don ScanNet200 [57], which comprises 200 object cat-
egories and exhibits significant class imbalance, which makes it ideal for simulating and 8THENGANE ET AL.: CLIMB-3D
assessing real-world scenarios. In addition, we compare our approach against existing in-
cremental learning methods using ScanNetV2 [16] in the 3DSS setting. For this evaluation,
we follow the standard training and validation splits defined in prior works [74] to ensure
consistency with existing methods. Evaluation Metrics.We evaluate our method usingMean Average Precision(mAP), a
standard metric for 3DIS which provides a comprehensive measure of segmentation quality
by accounting for both precision and recall. For comparison with existing 3DSS methods,
we report themean Intersection over Union(mIoU), which quantifies the overlap between
predicted and ground truth segments. To assess the modelâ€™s ability to mitigate catastrophic
forgetting in incremental settings, we use theForgetting Percentage Points(FPP) metric, as
defined in [47]. FPP captures performance degradation by measuring the accuracy drop on
the initially seen categories between the first and final training phases. Detailed descriptions of the incremental scenarios (Split-A,Split-B,Split-C) and im-
plementation are provided in Appendix B. 4.2 Results and Discussion
Table 1: Comparison between the proposed method and the baseline in the 3DIS setting,
evaluated using mAP25, mAP50, mAP, and FPP after training across all phase. Scenarios Methods Average Precisionâ†‘FPPâ†“
mAP25 mAP50 mAP mAP 25 mAP50
Baseline 16.46 14.29 10.44 51.30 46.82Split-A CLIMB-3D(Ours) 35.69 31.05 22.72 3.44 2.63
Baseline 17.22 15.07 10.93 46.27 42.1Split-B CLIMB-3D(Ours) 35.48 31.56 23.69 8.00 5.51
Baseline 25.65 21.08 14.85 31.68 28.84Split-C CLIMB-3D(Ours) 31.59 26.78 18.93 9.10 7.89
To evaluate our proposed approach, we construct a baseline using ER [5] for the 3DIS
setting, as no existing incremental baselines are available for this task. In contrast, for the
3DSS setting, where incremental baselines do exist, we compare our method against three
prior approaches [43, 60, 74]. Results on CI-3DIS.Tab. 1 compares CLIMB-3D against the ER baseline across the three
CI-3DIS scenarios, evaluated after all phases using mAP 25, mAP50, overall mAP, and FPP. In theSplit-Ascenario, characterised by significant distribution shifts, our method achieves
gains of +19.23%, +16.76%, and +12.28% in mAP 25, mAP 50, and overall mAP, respec-
tively, while drastically reducing forgetting with FPP scores of 3.44% (mAP 25) and 2.63%
(mAP50), both over 45 points lower than the baseline. Under theSplit-Bscenario, with
semantically related new categories, improvements of +18.26%, +16.49%, and +12.76%
are observed alongside a reduction in forgetting to 8.00% and 5.51% from baseline levels
above 42%. For theSplit-Cscenario, featuring increasing geometric complexity, CLIMB-
3D maintains solid gains of +5.94%, +5.70%, and +4.08% across mAP metrics, with forget-
ting reduced by over 22 points. Overall, these results demonstrate robust forward transfer,
minimal forgetting, and stable learning, highlighting the methodâ€™s effectiveness for scalable
continual 3D semantic understanding. THENGANE ET AL.: CLIMB-3D9
Table 2: Comparison between the proposed method and existing baselines in the 3DSS set-
ting on ScanNetV2 [16], evaluated using mIoU. Methods Phase=1 Phase=2 All
EWC [60] 17.75 13.22 16.62
LwF [43] 30.38 13.37 26.13
Yanget al. [74] 34.16 13.43 28.98
CLIMB-3D(Ours) 69.39 32.56 59.38
Results on CI-3DSS.Although our primary focus is on CI-3DIS, we also evaluate our
method under the CI-3DSS setting to enable comparisons with existing approaches. To do
so, we adapt our predictions to assign each point the label corresponding to the highest-
confidence mask and exclude background classes (e.g., floor and wall), as these are not part
of the semantic segmentation targets. Tab. 2 presents a comparison between our method and
existing baselines on the ScanNet V2 dataset [16], evaluated using mIoU across two training
phases and overall. Although originally designed for instance segmentation, our method
generalises effectively to semantic segmentation, achieving substantial gains of +35.23%
mIoU in Phase 1 and +19.1% in Phase 2. Overall, it achieves 59.38% mIoU, significantly
higher than theâˆ¼30% mIoU of prior methods, highlighting its robustness and transferability. Refer to Appendix C, D, and F of the supplementary material for detailed analyses of per-
phase performance, rare-class evaluation, and qualitative result comparisons, respectively. 4.3 Ablation
Table 3: Ablation study illustrating the impact of each component in a three-phase setup. Each split (s) and corresponding number indicates data introduced at that phase. Best results
are highlighted inbold. Row Modules p=1â†‘p=2â†‘p=3â†‘ FPPâ†“
s1 s1 s2 All s1 s2 s3 All
1. Oracle - - - - 55.14 30.77 25.30 37.68 -
2. NaÃ¯ve 56.82 0.00 28.09 14.15 0.00 0.00 19.67 5.80 56.82
3. + ER 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28 46.44
4. + PLG 56.82 50.0034.3942.13 49.78 11.41 26.47 29.28 7.04
5. + CBR 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05 2.63
We conduct an ablation study to evaluate the contribution of each component in our
framework. As an upper bound, we report the performance of anOraclemodel, which is
trained jointly on the full dataset. For the incremental setting, we follow the previously
defined splits and first train the model naively across phases. We then incrementally add
each module to isolate its impact. Tab. 3 presents results for theSplit-Ascenario using both
mAP50 and FPP metrics. NaÃ¯ve Training.When trained naÃ¯vely without any dedicated modules, the model suffers
from severe catastrophic forgetting, as evident in row 2. The model entirely forgets pre-
viously learned classes upon entering new phases, with performance dropping to zero for
earlier splits and FPP rising to 56.82. 10THENGANE ET AL.: CLIMB-3D
Effect of ER.Adding exemplar replay (row 3) partially alleviates forgetting by maintaining
a buffer of past examples. This yields notable gains fors1in Phase 2 (+18.51%) and Phase 3
(+10.38%), and also improvess2in Phase 3 (+9.43%). However, the overall forgetting
remains substantial (FPP: 46.44), showing ER alone is insufficient. Effect of PLG.The addition of the pseudo-label generator (PLG), which generates labels for
previous classes by retaining a copy of the model from an earlier phase, facilitates knowledge
retention and forward transfer. As shown in row 4, PLG significantly reduces forgetting and
enhances performance on current tasks. Fors1, it improves mAP 50 by 31.49% in Phase 2
and 39.40% in Phase 3 over exemplar replay. Overall, PLG yields a 15.00% increase in
performance and reduces forgetting by 39.40%. Effect of CBR.Finally, class-balanced re-weighting (CBR) mitigates class imbalance dur-
ing both pseudo-labelling and current task learning by adjusting each classâ€™s contribution
based on its frequency. As shown in row 5, CBR enhancess1retention over PLG in Phases 2
and 3 (+4.67% and +4.41%), and further improves performance ons2ands3. It also achieves
the lowest FPP of 2.63, reflecting strong forgetting mitigation. Overall, CBR offers the best
trade-off between retaining past knowledge and acquiring new tasks, outperforming PLG
and ER by 4.41% and 43.81%, respectively. 4.4 Limitations & Future Work
While CLIMB-3D achieves strong performance across CI-3DIS and CI-3DSS settings, sev-
eral limitations remain and warrant discussion. The experiments are currently limited to
indoor datasets (ScanNet200 and ScanNetV2), therefore evaluating performance on outdoor
scenes would help assess generalisability across diverse environments. The current setup
also considers only three incremental phases, whereas real-world applications often involve
longer sequences; incorporating such extended setups would better reflect practical chal-
lenges. Moreover, the model operates in a uni-modal setting. Integrating multi-modal cues,
such as vision-language models, could further enhance performance, as suggested by recent
2D and 3D studies [25, 65, 66]. 5 Conclusion
This work addresses the challenge of catastrophic forgetting in class-incremental 3D in-
stance segmentation by introducing a modular framework that integrates exemplar replay
(ER), a pseudo-label generator (PLG), and class-balanced re-weighting (CBR). The pro-
posed method incrementally adapts to new classes while preserving prior knowledge, all
without requiring access to the full dataset. Extensive experiments on a three-phase bench-
mark validate the individual and combined effectiveness of each component: ER enables
efficient memory-based retention; PLG facilitates knowledge preservation through pseudo-
supervision; and CBR mitigates class imbalance to enhance learning stability. Together,
these components significantly reduce forgetting and improve segmentation performance
across all phases. By achieving a strong balance between stability and plasticity, our ap-
proach advances continual 3D scene understanding and sets a new baseline for future re-
search in this area. THENGANE ET AL.: CLIMB-3D11
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne
Tuytelaars. Memory aware synapses: Learning what (not) to forget. InECCV, pages
139â€“154, 2018. [2] Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long-tailed
recognition via weight balancing. InCVPR, pages 6897â€“6907, 2022. [3] Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying
imbalanced data. InJoint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 770â€“785.

Although some approaches address the emergence of new classes, they often overlook class imbalance, which leads to suboptimal performance, particularly on rare categories. To mitigate this, we introduce a novel pseudo-label generator (PLG) that extends supervision to previously learned categories by leveraging predictions from a frozen model trained on prior tasks."
What dataset is mentioned for evaluating the incremental scenarios for 3D instance segmentation?,We design and evaluate three incremental scenarios for 3DIS on the challenging ScanNet200 dataset and additionally validate our method for semantic segmentation on ScanNetV2.,"arXiv:2502.17429v3  [cs.CV]  21 Nov 2025
THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced
3D Instance Segmentation
Vishal Thengane1
v.thengane@surrey.ac.uk
Jean Lahoud2
jean.lahoud@mbzuai.ac.ae
Hisham Cholakkal2
hisham.cholakkal@mbzuai.ac.ae
Rao Muhammad Anwer2
rao.anwer@mbzuai.ac.ae
Lu Yin1
l.yin@surrey.ac.uk
Xiatian Zhu1
xiatian.zhu@surrey.ac.uk
Salman Khan2, 3
salman.khan@mbzuai.ac.ae
1 University of Surrey,
Guildford, UK
2 Mohamed bin Zayed University of
Artificial Intelligence,
Abu Dhabi, UAE
3 Australian National University,
Canberra, Australia
Abstract
While 3D instance segmentation (3DIS) has advanced significantly, most existing
methods assume that all object classes are known in advance and uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new
classes emerge gradually and exhibit natural imbalance. Although some approaches ad-
dress the emergence of new classes, they often overlook class imbalance, which leads
to suboptimal performance, particularly on rare categories. To tackle this, we propose
CLIMB-3D, a unified framework forCLass-incrementalImbalance-aware3DIS. Build-
ing upon established exemplar replay (ER) strategies, we show that ER alone is insuf-
ficient to achieve robust performance under memory constraints. To mitigate this, we
introduce a novel pseudo-label generator (PLG) that extends supervision to previously
learned categories by leveraging predictions from a frozen model trained on prior tasks. Despite its promise, PLG tends to be biased towards frequent classes. Therefore, we
propose a class-balanced re-weighting (CBR) scheme that estimates object frequencies
from pseudo-labels and dynamically adjusts training bias, without requiring access to
past data. We design and evaluate three incremental scenarios for 3DIS on the challeng-
ing ScanNet200 dataset and additionally validate our method for semantic segmentation
on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by
up to 16.76% mAP for instance segmentation and approximately 30% mIoU for semantic
segmentation, demonstrating strong generalisation across both frequent and rare classes. Code is available at:https://github.com/vgthengane/CLIMB3D. Â© 2025.

We design and evaluate three incremental scenarios for 3DIS on the challenging ScanNet200 dataset and additionally validate our method for semantic segmentation on ScanNetV2.

5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig."
What percentage improvement is achieved for semantic segmentation compared to prior work?,"Our approach achieves state-of-the-art results, surpassing prior work by up to 16.76% mAP for instance segmentation and approximately 30% mIoU for semantic segmentation, demonstrating strong generalisation across both frequent and rare classes.","[4] Mohamed El Amine Boudjoghra, Salwa Al Khatib, Jean Lahoud, Hisham Cholakkal,
Rao Anwer, Salman H Khan, and Fahad Shahbaz Khan. 3d indoor instance segmenta-
tion in an open-world.NeurIPS, 36, 2024. [5] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calder-
ara. Dark experience for general continual learning: a strong, simple baseline.NeurIPS,
33:15920â€“15930, 2020. [6] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning
imbalanced datasets with label-distribution-aware margin loss.NeurIPS, 32, 2019. [7] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrastive continual learning. In
ICCV, pages 9516â€“9525, 2021. [8] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique.Journal of artificial intelligence
research, 16:321â€“357, 2002. [9] Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and Xinggang Wang. Hierarchical
aggregation for 3d instance segmentation. InICCV, pages 15467â€“15476, 2021. [10] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification is not
all you need for semantic segmentation.NeurIPS, 34:17864â€“17875, 2021. [11] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Gird-
har. Masked-attention mask transformer for universal image segmentation. InCVPR,
pages 1290â€“1299, 2022. [12] Julian Chibane, Francis Engelmann, Tuan Anh Tran, and Gerard Pons-Moll. Box2mask: Weakly supervised 3d semantic instance segmentation using bounding
boxes. InECCV, pages 681â€“699.

Our approach achieves state-of-the-art results, surpassing prior work by up to 16.76% mAP for instance segmentation and approximately 30% mIoU for semantic segmentation, demonstrating strong generalisation across both frequent and rare classes.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916."
Where can the code for the method be accessed?,Code is available at:https://github.com/vgthengane/CLIMB3D.,"arXiv:2502.17429v3  [cs.CV]  21 Nov 2025
THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced
3D Instance Segmentation
Vishal Thengane1
v.thengane@surrey.ac.uk
Jean Lahoud2
jean.lahoud@mbzuai.ac.ae
Hisham Cholakkal2
hisham.cholakkal@mbzuai.ac.ae
Rao Muhammad Anwer2
rao.anwer@mbzuai.ac.ae
Lu Yin1
l.yin@surrey.ac.uk
Xiatian Zhu1
xiatian.zhu@surrey.ac.uk
Salman Khan2, 3
salman.khan@mbzuai.ac.ae
1 University of Surrey,
Guildford, UK
2 Mohamed bin Zayed University of
Artificial Intelligence,
Abu Dhabi, UAE
3 Australian National University,
Canberra, Australia
Abstract
While 3D instance segmentation (3DIS) has advanced significantly, most existing
methods assume that all object classes are known in advance and uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new
classes emerge gradually and exhibit natural imbalance. Although some approaches ad-
dress the emergence of new classes, they often overlook class imbalance, which leads
to suboptimal performance, particularly on rare categories. To tackle this, we propose
CLIMB-3D, a unified framework forCLass-incrementalImbalance-aware3DIS. Build-
ing upon established exemplar replay (ER) strategies, we show that ER alone is insuf-
ficient to achieve robust performance under memory constraints. To mitigate this, we
introduce a novel pseudo-label generator (PLG) that extends supervision to previously
learned categories by leveraging predictions from a frozen model trained on prior tasks. Despite its promise, PLG tends to be biased towards frequent classes. Therefore, we
propose a class-balanced re-weighting (CBR) scheme that estimates object frequencies
from pseudo-labels and dynamically adjusts training bias, without requiring access to
past data. We design and evaluate three incremental scenarios for 3DIS on the challeng-
ing ScanNet200 dataset and additionally validate our method for semantic segmentation
on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by
up to 16.76% mAP for instance segmentation and approximately 30% mIoU for semantic
segmentation, demonstrating strong generalisation across both frequent and rare classes. Code is available at:https://github.com/vgthengane/CLIMB3D. Â© 2025.

PMLR, 2019. 14THENGANE ET AL.: CLIMB-3D
[43] Zhizhong Li and Derek Hoiem. Learning without forgetting.IEEE PAMI, 40(12):
2935â€“2947, 2017. [44] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmenta-
tion in 3d scenes using semantic superpoint tree networks. InICCV, pages 2783â€“2792,
2021. [45] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss
for dense object detection. InICCV, pages 2980â€“2988, 2017.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

Code is available at:https://github.com/vgthengane/CLIMB3D."
Who is the contact person for Hisham Cholakkal?,"Hisham Cholakkal2
hisham.cholakkal@mbzuai.ac.ae","Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710.

Hisham Cholakkal2
hisham.cholakkal@mbzuai.ac.ae

arXiv:2502.17429v3  [cs.CV]  21 Nov 2025
THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced
3D Instance Segmentation
Vishal Thengane1
v.thengane@surrey.ac.uk
Jean Lahoud2
jean.lahoud@mbzuai.ac.ae
Hisham Cholakkal2
hisham.cholakkal@mbzuai.ac.ae
Rao Muhammad Anwer2
rao.anwer@mbzuai.ac.ae
Lu Yin1
l.yin@surrey.ac.uk
Xiatian Zhu1
xiatian.zhu@surrey.ac.uk
Salman Khan2, 3
salman.khan@mbzuai.ac.ae
1 University of Surrey,
Guildford, UK
2 Mohamed bin Zayed University of
Artificial Intelligence,
Abu Dhabi, UAE
3 Australian National University,
Canberra, Australia
Abstract
While 3D instance segmentation (3DIS) has advanced significantly, most existing
methods assume that all object classes are known in advance and uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new
classes emerge gradually and exhibit natural imbalance. Although some approaches ad-
dress the emergence of new classes, they often overlook class imbalance, which leads
to suboptimal performance, particularly on rare categories. To tackle this, we propose
CLIMB-3D, a unified framework forCLass-incrementalImbalance-aware3DIS. Build-
ing upon established exemplar replay (ER) strategies, we show that ER alone is insuf-
ficient to achieve robust performance under memory constraints. To mitigate this, we
introduce a novel pseudo-label generator (PLG) that extends supervision to previously
learned categories by leveraging predictions from a frozen model trained on prior tasks. Despite its promise, PLG tends to be biased towards frequent classes. Therefore, we
propose a class-balanced re-weighting (CBR) scheme that estimates object frequencies
from pseudo-labels and dynamically adjusts training bias, without requiring access to
past data. We design and evaluate three incremental scenarios for 3DIS on the challeng-
ing ScanNet200 dataset and additionally validate our method for semantic segmentation
on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by
up to 16.76% mAP for instance segmentation and approximately 30% mIoU for semantic
segmentation, demonstrating strong generalisation across both frequent and rare classes. Code is available at:https://github.com/vgthengane/CLIMB3D. Â© 2025.

[4] Mohamed El Amine Boudjoghra, Salwa Al Khatib, Jean Lahoud, Hisham Cholakkal,
Rao Anwer, Salman H Khan, and Fahad Shahbaz Khan. 3d indoor instance segmenta-
tion in an open-world.NeurIPS, 36, 2024. [5] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calder-
ara. Dark experience for general continual learning: a strong, simple baseline.NeurIPS,
33:15920â€“15930, 2020. [6] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning
imbalanced datasets with label-distribution-aware margin loss.NeurIPS, 32, 2019. [7] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrastive continual learning. In
ICCV, pages 9516â€“9525, 2021. [8] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique.Journal of artificial intelligence
research, 16:321â€“357, 2002. [9] Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and Xinggang Wang. Hierarchical
aggregation for 3d instance segmentation. InICCV, pages 15467â€“15476, 2021. [10] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification is not
all you need for semantic segmentation.NeurIPS, 34:17864â€“17875, 2021. [11] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Gird-
har. Masked-attention mask transformer for universal image segmentation. InCVPR,
pages 1290â€“1299, 2022. [12] Julian Chibane, Francis Engelmann, Tuan Anh Tran, and Gerard Pons-Moll. Box2mask: Weakly supervised 3d semantic instance segmentation using bounding
boxes. InECCV, pages 681â€“699."
Who holds the copyright of this document?,The copyright of this document resides with its authors.,"Elsevier, 1989.

Springer, 2017.

The copyright of this document resides with its authors.

Springer,
2016."
Where does the copyright of this document reside?,The copyright of this document resides with its authors.,"The copyright of this document resides with its authors.

Springer,
2016.

Elsevier, 1989.

Springer, 2017."
What is the fundamental task in computer vision that involves identifying precise object boundaries and class labels in 3D space?,"3D instance segmentation (3DIS) is a fundamental task in computer vision that involves identifying precise object boundaries and class labels in 3D space, with broad applications in graphics, robotics, and augmented reality [4, 39].","3D instance segmentation (3DIS) is a fundamental task in computer vision that involves identifying precise object boundaries and class labels in 3D space, with broad applications in graphics, robotics, and augmented reality [4, 39].

arXiv:2502.17429v3  [cs.CV]  21 Nov 2025
THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced
3D Instance Segmentation
Vishal Thengane1
v.thengane@surrey.ac.uk
Jean Lahoud2
jean.lahoud@mbzuai.ac.ae
Hisham Cholakkal2
hisham.cholakkal@mbzuai.ac.ae
Rao Muhammad Anwer2
rao.anwer@mbzuai.ac.ae
Lu Yin1
l.yin@surrey.ac.uk
Xiatian Zhu1
xiatian.zhu@surrey.ac.uk
Salman Khan2, 3
salman.khan@mbzuai.ac.ae
1 University of Surrey,
Guildford, UK
2 Mohamed bin Zayed University of
Artificial Intelligence,
Abu Dhabi, UAE
3 Australian National University,
Canberra, Australia
Abstract
While 3D instance segmentation (3DIS) has advanced significantly, most existing
methods assume that all object classes are known in advance and uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new
classes emerge gradually and exhibit natural imbalance. Although some approaches ad-
dress the emergence of new classes, they often overlook class imbalance, which leads
to suboptimal performance, particularly on rare categories. To tackle this, we propose
CLIMB-3D, a unified framework forCLass-incrementalImbalance-aware3DIS. Build-
ing upon established exemplar replay (ER) strategies, we show that ER alone is insuf-
ficient to achieve robust performance under memory constraints. To mitigate this, we
introduce a novel pseudo-label generator (PLG) that extends supervision to previously
learned categories by leveraging predictions from a frozen model trained on prior tasks. Despite its promise, PLG tends to be biased towards frequent classes. Therefore, we
propose a class-balanced re-weighting (CBR) scheme that estimates object frequencies
from pseudo-labels and dynamically adjusts training bias, without requiring access to
past data. We design and evaluate three incremental scenarios for 3DIS on the challeng-
ing ScanNet200 dataset and additionally validate our method for semantic segmentation
on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by
up to 16.76% mAP for instance segmentation and approximately 30% mIoU for semantic
segmentation, demonstrating strong generalisation across both frequent and rare classes. Code is available at:https://github.com/vgthengane/CLIMB3D. Â© 2025.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019."
What are the three components of the CLIMB-3D framework for Class-Incremental IMBalance-aware 3DIS?,"To address this, we propose CLIMB-3D, a unified framework for Class-incremental IMBalance-aware 3DIS that jointly tackles catastrophic forgetting and class imbalance. The framework begins with Exemplar Replay (ER), storing a small number of samples from past classes for later replay. However, this alone does not yield promising results under the strict memory constraints required in CI-3DIS. To address this, we introduce a Pseudo-Label Generator (PLG), which uses a frozen model from previous task to generate supervision for earlier classes. However, we observed that PLG tends to favour frequent classes while ignoring under-represented ones. To mitigate this, we propose a Class-Balanced Re-weighting (CBR), which estimates class frequencies from the pseudo-labels to enhance the learning of rare classes without accessing past data.","To address this, we propose CLIMB-3D, a unified framework for Class-incremental IMBalance-aware 3DIS that jointly tackles catastrophic forgetting and class imbalance. The framework begins with Exemplar Replay (ER), storing a small number of samples from past classes for later replay. However, this alone does not yield promising results under the strict memory constraints required in CI-3DIS. To address this, we introduce a Pseudo-Label Generator (PLG), which uses a frozen model from previous task to generate supervision for earlier classes. However, we observed that PLG tends to favour frequent classes while ignoring under-represented ones. To mitigate this, we propose a Class-Balanced Re-weighting (CBR), which estimates class frequencies from the pseudo-labels to enhance the learning of rare classes without accessing past data.

1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

arXiv:2502.17429v3  [cs.CV]  21 Nov 2025
THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced
3D Instance Segmentation
Vishal Thengane1
v.thengane@surrey.ac.uk
Jean Lahoud2
jean.lahoud@mbzuai.ac.ae
Hisham Cholakkal2
hisham.cholakkal@mbzuai.ac.ae
Rao Muhammad Anwer2
rao.anwer@mbzuai.ac.ae
Lu Yin1
l.yin@surrey.ac.uk
Xiatian Zhu1
xiatian.zhu@surrey.ac.uk
Salman Khan2, 3
salman.khan@mbzuai.ac.ae
1 University of Surrey,
Guildford, UK
2 Mohamed bin Zayed University of
Artificial Intelligence,
Abu Dhabi, UAE
3 Australian National University,
Canberra, Australia
Abstract
While 3D instance segmentation (3DIS) has advanced significantly, most existing
methods assume that all object classes are known in advance and uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new
classes emerge gradually and exhibit natural imbalance. Although some approaches ad-
dress the emergence of new classes, they often overlook class imbalance, which leads
to suboptimal performance, particularly on rare categories. To tackle this, we propose
CLIMB-3D, a unified framework forCLass-incrementalImbalance-aware3DIS. Build-
ing upon established exemplar replay (ER) strategies, we show that ER alone is insuf-
ficient to achieve robust performance under memory constraints. To mitigate this, we
introduce a novel pseudo-label generator (PLG) that extends supervision to previously
learned categories by leveraging predictions from a frozen model trained on prior tasks. Despite its promise, PLG tends to be biased towards frequent classes. Therefore, we
propose a class-balanced re-weighting (CBR) scheme that estimates object frequencies
from pseudo-labels and dynamically adjusts training bias, without requiring access to
past data. We design and evaluate three incremental scenarios for 3DIS on the challeng-
ing ScanNet200 dataset and additionally validate our method for semantic segmentation
on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by
up to 16.76% mAP for instance segmentation and approximately 30% mIoU for semantic
segmentation, demonstrating strong generalisation across both frequent and rare classes. Code is available at:https://github.com/vgthengane/CLIMB3D. Â© 2025.

It may be distributed unchanged freely in print or electronic forms. 2THENGANE ET AL.: CLIMB-3D
1 Introduction
Table Chair Couch Pillow Coffee Table Sofa Chair Recycling BinCabinet Unknown
Task-1 Task-2 Task-3
Figure 1:Overview of the CI-3DIS setting.New object categories are introduced incre-
mentally with each task. After every phase, the model must recognise both newly added
and previously learned classes. For instance, in Task 2, categories such asPillow,Coffee
Table, andSofa Chairare introduced, while the model is expected to retain recognition
of earlier classes likeTable,Chair, andCouch. 3D instance segmentation (3DIS) is a fundamental task in computer vision that involves
identifying precise object boundaries and class labels in 3D space, with broad applications in
graphics, robotics, and augmented reality [4, 39]. Traditional 3DIS methods, including top-
down [28, 70, 78], bottom-up [26, 73], and transformer-based approaches [59], perform well
under the assumption that all classes are available and balanced during training. However,
this assumption does not hold in real-world environments, where new object classes emerge
over time and exhibit natural imbalance. This gap motivates the need for Class-Incremental Learning (CIL) [17, 54], which sup-
ports learning new categories while retaining knowledge of previous ones [51]. While CIL
has seen success in 2D image tasks [1, 43, 54, 60], extensions to 3D point clouds remain
limited, often focusing on object-level classification [13, 18, 48]. Recent works on scene-
level class-incremental 3DIS (CI-3DIS) [4] and class-incremental semantic segmentation
(CI-3DSS) [74] show promise, but depend heavily on large exemplar [56] and overlook class
imbalance, limiting their practicality. To address this, we proposeCLIMB-3D, a unified framework forCLass-incremental
IMBalance-aware3DIS that jointly tackles catastrophic forgetting and class imbalance. The
framework begins with Exemplar Replay (ER), storing a small number of samples from
past classes for later replay. However, this alone does not yield promising results under the
strict memory constraints required in CI-3DIS. To address this, we introduce a Pseudo-Label
Generator (PLG), which uses a frozen model from previous task to generate supervision for
earlier classes. However, we observed that PLG tends to favour frequent classes while ig-
noring under-represented ones. To mitigate this, we propose a Class-Balanced Re-weighting
(CBR), which estimates class frequencies from the pseudo-labels to enhance the learning of
rare classes without accessing past data. Together, these components form a practical and
effective solution for real-world 3DIS. To evaluate CLIMB-3D, we designed three benchmark scenarios for CI-3DIS based on
the ScanNet200 dataset [57]. These scenarios simulate incremental learning setup under nat-
ural class imbalance, where new classes emerge based on (A) object frequency, (B) semantic
similarity, or (C) random grouping. Additionally, for comparison with existing methods,
we evaluate our approach on CI-3DSS using the ScanNetV2 dataset [16]. Experimental re-
sults show that our method significantly reduces forgetting and improves performance across
both frequent and rare categories, outperforming previous methods in both settings. Fig. 1
illustrates the CI-3DIS setup. THENGANE ET AL.: CLIMB-3D3
In summary, our contributions are: (i) a novel problem setting of imbalanced class-
incremental 3DIS with an effective method to balance learning and mitigate forgetting; (ii)
three benchmarks modelling continual object emergence with natural imbalance; and (iii)
strong experimental results, achieving up to 16.76% mAP improvement over baselines. 2 Related Work
3D Instance Segmentation.Various methods have been proposed for 3DIS. Grouping-
based approaches adopt a bottom-up pipeline that learns latent embeddings for point cluster-
ing [9, 21, 22, 29, 37, 44, 70, 78]. In contrast, proposal-based methods follow a top-down
strategy by detecting 3D boxes and segmenting objects within them [19, 26, 46, 73, 76]. Recently, transformer-based models [68] have also been applied to 3DIS and 3DSS [59, 64],
inspired by advances in 2D vision [10, 11]. However, these approaches require full anno-
tations for all classes and are not designed for progressive learning, where only new class
annotations are available and previous data is inaccessible. Another line of work aims to re-
duce annotation costs by proposing weakly supervised 3DIS methods based on sparse cues
[12, 27, 72]. While effective with limited annotations, these methods assume a fixed set of
classes and are susceptible to catastrophic forgetting in incremental settings. Incremental Learning.Continual learning involves training models sequentially to miti-
gate catastrophic forgetting. One common strategy is model regularisation, which constrains
parameter updates using techniques such as elastic weight consolidation or knowledge dis-
tillation [1, 24, 35, 43, 60]. Another approach is exemplar replay, where past data is stored
or generated to rehearse older tasks while learning new ones [5, 7, 30, 54]. A third direc-
tion involves dynamically expanding the model architecture or using modular sub-networks
to accommodate new tasks without interfering with old ones [31, 42, 53, 58, 71, 79]. Re-
cent efforts in class-incremental 3D segmentation [63, 74] have shown early promise but
often rely on basic architectures and focus primarily on semantic segmentation. Other works
[4, 36] explore continual learning in 3D settings, though they often depend on large memory
buffers. In contrast, our work introduces a tailored framework for 3D instance segmentation
that effectively transfers knowledge across tasks, even under class imbalance. Long-tailed Recognition.Imbalanced class distributions lead to poor recognition of rare
(long-tail) classes. To address this, re-sampling methods balance the data via input [8, 20, 50,
52, 61] or feature space [3, 14, 40], avoiding naÃ¯ve under-/over-sampling. Loss re-weighting
offers another direction, using class-based [6, 15, 23, 32, 33, 69] or per-example [45, 55, 62]
adjustments to ensure fair contribution. Parameter regularisation improves generalisation
through weight constraints [2], albeit requiring careful tuning. Other approaches leverage
transfer learning [77, 81], self-supervision [41, 75], or contrastive learning [34, 41, 82] to
improve rare-class representations. Long-tailed recognition is well-studied in 2D with large-
scale datasets [15, 49, 67]; in 3D, ScanNet200 [16, 57] enables similar efforts. Prior 3D
work focused on re-weighting, re-sampling, and transfer learning [57], while regularisation
remains underexplored. CeCo [80] balances class centres via auxiliary loss but overlooks
sampling and augmentation. Lahoud et al. [38] propose adaptive classifier regularisation
for 3D segmentation, outperforming prior approaches without threshold tuning. However,
neither method considers incremental learning. In contrast, we jointly address long-tail and
continual 3D semantic segmentation. 4THENGANE ET AL.: CLIMB-3D
Feature 
Backbone
Transformer 
Decoder
Feature 
Backbone
Transformer 
Decoder
(ð‘·,ð’€ð‘¡)
Current model (ð›·
ð‘¡
)
Pseudo Label Generator (PLG) 
Selection
à·¨ð‘Œð¾
ð‘¡
â„’CIâˆ’3DIS
à·¨ð‘Œð¾
ð‘¡,ð‘¤
à´¤ð‘Œð¾
ð‘¡
à· ð‘Œð‘¡
(ð·ð‘¡)
Current Dataset 
Exemplar 
Replay (ER)
+
~
Class-Balanced  
Re-weighting 
(CBR)
1
ð‘“(ð‘)
Figure 2: Overview ofCLIMB-3Dfor CI-3DIS. The model incrementally learns new classes
across sequential phases. During taskt, point cloudsPand their corresponding labelsY t are
sampled from a combination of the current training datasetD t and Exemplar Replay (ER),
which maintains a small memory of past examples. These are then passed to the current
modelÎ¦ t to produce predicted labelsY t . The Pseudo-Label Generator (PLG) selects the
top-Kpredictions from the previous modelÎ¦ tâˆ’1 . These pseudo-labels are then weighted
based on class frequencyf(c)using Class-Balanced Re-weighting (CBR), and the top-Kre-
weighted labels are selected to form the balanced pseudo-label set Â¯Yt . This pseudo-label set
is then concatenated with the ground-truth labels to form a final augmented supervision set
Yt for taskt, which is used to optimise the modelÎ¦ t using Eq. (3). 3 Methodology
3.1 Problem Formulation
3DIS.The objective of this task is to detect and segment each object instance within a point
cloud. Formally, the training dataset is denoted asD={(P i,Yi)}N
i=1, whereNis the number
of training samples. Each inputP i âˆˆR MÃ—6 is a coloured point cloud consisting ofMpoints,
where each point is represented by its 3D coordinates and RGB values. The corresponding
annotationY i ={(m i,j ,ci,j )}Ji
j=1 containsJ i object instances, wheremi,j âˆˆ {0,1}M is a binary
mask indicating which points belong to thej-th instance, andc i,j âˆˆ C={1,...,C}is the
semantic class label of that instance. GivenP i, the modelÎ¦predicts instance-level outputs
Ë†Yi ={(Ë†mi,j ,Ë†ci,j )}
Ë†Ji
j=1, where Ë†mi,j and Ë†ci,j denote the predicted mask and category label for
thej-th instance, respectively. The number of predicted instances Ë†Ji varies depending on the
modelâ€™s inference. The model is optimised using the following objective:
L3DIS(D;Î¦) = 1
|D| âˆ‘
(P,Y)âˆˆD
1
|Y| âˆ‘
(mj,cj)âˆˆY
(Lmask(mj,Ë†mj)+Î»Â·L cls(cj,Ë†cj)),(1)
whereL cls andL mask are the average classification and mask losses over all instances, andÎ»
controls their trade-off. CI-3DIS.Unlike conventional 3DIS, CI-3DIS involves sequential learning, where object
categories are introduced incrementally overTtraining tasks. Each tasktintroduces a dis-
joint set of classesC t , withC= ST
t=1 Ct andC t âˆ©C tâ€²
=/ 0 fortÌ¸=t â€². During each taskt, the THENGANE ET AL.: CLIMB-3D5
model receives a datasetD t ={(P i,Yt
i)}N
i=1, where each coloured point cloudP i âˆˆR MÃ—6
containsMpoints, and the corresponding annotationY t
i ={(m t
i,j ,ct
i,j )}
Jt
i
j=1 includes instance
masksm t
i,j âˆˆ {0,1}M and semantic class labelsc t
i,j âˆˆ Ct . The modelÎ¦ t , initialized from
Î¦tâˆ’1 , is trained onD t to predict instance-level outputs Ë†Yt
i ={(Ë†mt
i,j ,Ë†ct
i,j )}
Ë†Jt
i
j=1, where each
predicted semantic label Ë†ct
i,j belongs to the cumulative set of all classes observed so far:St
k=1 Ck. The key challenge in this setup is to learn new classes without forgetting those
encountered in previous tasks, despite limited supervision and the absence of past labels. 3.2 Method: CLIMB-3D
Overview.As shown in Fig. 2 and formulated in Sec. 3.1, the proposed framework for
CI-3DIS follows aphase-wisetraining strategy. In each phase, the model is exposed to a
carefully curated subset of the dataset, simulating real-world scenarios discussed in Sec. 3.3. Training naÃ¯vely on such phased data leads tocatastrophic forgetting[51], where the model
forgets prior knowledge when learning new tasks. To address this, we first incorporateEx-
emplar Replay(ER) [5], a strategy inspired by 2D incremental methods [1, 43, 54] and
recently extended to 3D settings [4], to mitigate forgetting by storing a small set of repre-
sentative samples from earlier phases. However, ER alone is insufficient to achieve robust
performance. To address this, two additional components are introduced: aPseudo-Label
Generator(PLG), which leverages a frozen model to generate supervision signals for previ-
ously seen classes, and aClass-Balanced Re-weighting(CBR) module that compensates for
class imbalance across phases. Each of these components is described in detail below 1. Exemplar Replay (ER).Inspired by Buzzega et al. [5], ER addresses the issue of catas-
trophic forgetting by allowing the model to retain a subset of data from earlier phases. During phaset, the model is trained not only on the current task dataD t , but also on a
set of exemplarsE 1:tâˆ’1, collected from previous phases. These exemplars are accumulated
incrementally:E 1:tâˆ’1 =E 1 âˆªÂ·Â·Â·âˆªE tâˆ’1 .Here,E t denotes the exemplar subset stored after
phaset, and|E t |denotes the exemplar replay size. The combined replay dataset is denoted
as:D t
ER =D t âˆªE 1:tâˆ’1.Epoch training consists of two stages: first, the model is trained on
the current data,D t ; then it is trained using exemplars fromE 1:tâˆ’1. While previous CI-3DIS approaches rely on large exemplar sets to mitigate forgetting [4],
such strategies become impractical under memory constraints. Consequently, although ER
aids knowledge retention, it is insufficient on its own for robust performance with a limited
exemplar budget. Therefore, PLG and CBR are introduced to better preserve past represen-
tations and enhance generalisation across phases. Pseudo-Label Generator (PLG).In the CI-3DIS setting, during phaset(wheret>1),
although ground-truth labels for previously seen classes are unavailable, the model from the
previous phaseÎ¦ tâˆ’1 , which preserves knowledge of past tasks, is retained. We use this
model to generate pseudo-labels for previously seen classes, thereby providing approximate
supervision during training. This allows the current modelÎ¦t , to retain prior knowledge and
reduce forgetting, even without access to ground-truth annotations. Given a point cloud and label pair(P,Y t ), fromD t , the previous modelÎ¦ tâˆ’1 , generates
pseudo-labels for the previously seen classes as Ë†Y1:tâˆ’1 =Î¦ tâˆ’1 (P), which we denote as ËœYt
1For clarity, modifications introduced by each component (ER, PLG, and CBR) are highlighted in green, blue,
and red, respectively. 6THENGANE ET AL.: CLIMB-3D
for brevity. However, we observe that utilising all predictions fromÎ¦tâˆ’1 introduces noisy or
incorrect labels, which degrade overall performance. To mitigate this, we select the top-K
most confident instance predictions, denoted as ËœYt
K, and concatenate them with the ground-
truth labels of the current task to obtain the supervision set: Â¯Yt =Y t âˆ¥ ËœYt
K. Following this,
the loss is computed over both the current taskâ€™s ground-truth labels and the top-Kconfi-
dent pseudo-labels from earlier phases, enabling the model to retain prior knowledge while
adapting to new classes. Class-Balanced Re-weighting (CBR).While ER and PLG contribute to preserving prior
knowledge, we observe that the modelâ€™s predictions are biased towards frequent classes,
resulting in the forgetting of rare categories. This issue is further amplified by the top-K
pseudo-labels selected by PLG, which predominantly represent dominant classes in the data. To address this, we propose aClass-Balanced Re-weighting(CBR) scheme that compensates
for class imbalance using object frequency statistics. At taskt, only the current datasetD t
and its label distributionp t (c)over classescâˆˆ Ct are available. Datasets and class distribu-
tions from earlier tasks{D i,p i(c)}tâˆ’1
i=1 are no longer accessible, which makes it challenging
to directly account for class imbalance across all previously seen tasks. Therefore, we pro-
pose to leverage the pseudo-label predictions of the frozen modelÎ¦ tâˆ’1 as a proxy for the
class distribution across previously learned categories. During each training iteration, the previous modelÎ¦ tâˆ’1 is applied to the current inputP
to generate pseudo-labels ËœYt , from which we accumulate class-wise frequency statistics for
previously learned classes. LetC1:t = St
i=1 Ci represent the union of all classes seen up to task
t. At the end of each epoch, we compute the overall class frequencyfâˆˆR |C1:t | by combining
the pseudo-label distribution Ëœpt (c)derived from ËœYt with the ground-truth label distribution
pt (c)from the current task. The resulting frequency vectorf= [f(c)] câˆˆC1:t is then used to
compute class-wise weightsw= [w(c)] câˆˆC1:t , which guide balanced pseudo-label selection
and help the model focus on rare classes. In the next epoch, the modelÎ¦ tâˆ’1 produces soft pseudo-label predictions ËœYt (as de-
scribed in PLG) , where each component ËœYt [c]denotes the confidence score for classcâˆˆ
C1:tâˆ’1. To promote balanced pseudo-label selection, class-wise re-weighting is applied using
w(c) = 1
f(c)+Îµ , whereÎµis a small constant for numerical stability. The adjusted predictions
are computed by applying the class-wise weights as: ËœYt,w [c] =w(c)Â· ËœYt [c],âˆ€câˆˆ C 1:tâˆ’1. Top-Kselection is then applied to both the original and the re-weighted scores, yielding
two pseudo-label sets: ËœYt
K and ËœYt,w
K , respectively. The final augmented target set, Yt , is
constructed by concatenating the ground-truth labelsY t with both sets of pseudo-labels:
Yt =Y t âˆ¥ ËœYt
K âˆ¥ ËœYt,w
K .(2)
Although the above re-weighting mitigates bias from prior tasks, it does not address class
imbalance within the current taskt. To resolve this, the class-balanced weightsw c are ex-
tended to incorporate statistics from both previously seen and current classes, i.e., overC 1:t
rather than onlyC 1:tâˆ’1. The updated weights, denoted asw â€²
c, are used to re-weights both
pseudo-label selection and classification loss, ensuring balanced supervision across all seen
categories, including rare ones. This unified strategy reduces bias and enhances performance
across phases. The pseudo-code for constructing the supervision set in Eq. (2), which incor-
porates both PLG and CBR components, is provided in Appendix A. Final Objective.The final training objective ofCLIMB-3D, incorporating ER, PLG, and THENGANE ET AL.: CLIMB-3D7
CBR, is formulated in Eq. (1) as:
L(Dt
ER;Î¦t ) = 1
|Dt
ER| âˆ‘
(P,Yt )âˆˆDt
ER
1
|Yt | âˆ‘
(mt
j,ct
j)âˆˆYt
 
Lmask(mt
j,Ë†mt
j)+w â€²â€²
c Â·Lcls(ct
j,Ë†ct
j)

,(3)
wherew â€²â€²
c =w â€²
c Â·Î» cls denotes the scaled weight vector, andÎ» cls is a hyperparameter for bal-
ancing the loss terms. 3.3 Benchmarking Incremental Scenarios
While CI-3DIS methods offer a wide range of practical applications, they frequently rely
on the assumption of uniform sample distribution, which rarely holds in real-world settings. In practice, the number of object categories, denoted byC, is often large and characterised
by substantial variability in category frequency, shape, structure, and size. To address these
challenges, we propose three incremental learning scenarios, each designed to capture a
different facet of real-world complexity. The design of these scenarios is detailed below; for
further information and illustrations, refer to Appendix E of the supplementary material. Split-A: Frequency Scenarios.This scenario acknowledges that datasets are often la-
belled based on the frequency of categories. To accommodate this, we propose a split where
the model learns from the most frequent categories and subsequently incorporates the less
frequent ones in later stages. By prioritising the training of frequently occurring categories,
the model can establish a strong foundation before expanding to handle rare categories. Split-B: Semantic Scenarios.Beyond frequency, semantic similarity is crucial in real-
world deployments. While objects often share visual or functional traits, models may en-
counter semantically different categories in new environments. To simulate this challenge,
we introduce theSplit-Bscenario. Here, categories are grouped based on their semantic
relationships, and the model is incrementally trained on one semantic group at a time. This
setup encourages the model to generalise across semantically similar categories and adapt
more effectively when exposed to new ones. Unlike theSplit-Ascenario, which organises
learning based on category frequency, theSplit-Bscenario may contain both frequent and
infrequent categories within a single task, focusing on semantic continuity and transfer. Split-C: Random Scenarios.In some cases, data labelling is driven by the availability of
objects rather than predefined criteria. To capture this, we introduce theSplit-Cscenario,
which represents a fully random setting where any class can appear in any task, resulting in
varying levels of class imbalance. By exposing the model to such diverse and unpredictable
distributions, we aim to improve its robustness in real-world situations where labelled data
availability is inconsistent. These incremental scenarios are designed to provide a more realistic representation of
object distributions, frequencies, and dynamics encountered in the real world. 4 Experiments
4.1 Setup
Datasets.We evaluateCLIMB-3Don ScanNet200 [57], which comprises 200 object cat-
egories and exhibits significant class imbalance, which makes it ideal for simulating and 8THENGANE ET AL.: CLIMB-3D
assessing real-world scenarios. In addition, we compare our approach against existing in-
cremental learning methods using ScanNetV2 [16] in the 3DSS setting. For this evaluation,
we follow the standard training and validation splits defined in prior works [74] to ensure
consistency with existing methods. Evaluation Metrics.We evaluate our method usingMean Average Precision(mAP), a
standard metric for 3DIS which provides a comprehensive measure of segmentation quality
by accounting for both precision and recall. For comparison with existing 3DSS methods,
we report themean Intersection over Union(mIoU), which quantifies the overlap between
predicted and ground truth segments. To assess the modelâ€™s ability to mitigate catastrophic
forgetting in incremental settings, we use theForgetting Percentage Points(FPP) metric, as
defined in [47]. FPP captures performance degradation by measuring the accuracy drop on
the initially seen categories between the first and final training phases. Detailed descriptions of the incremental scenarios (Split-A,Split-B,Split-C) and im-
plementation are provided in Appendix B. 4.2 Results and Discussion
Table 1: Comparison between the proposed method and the baseline in the 3DIS setting,
evaluated using mAP25, mAP50, mAP, and FPP after training across all phase. Scenarios Methods Average Precisionâ†‘FPPâ†“
mAP25 mAP50 mAP mAP 25 mAP50
Baseline 16.46 14.29 10.44 51.30 46.82Split-A CLIMB-3D(Ours) 35.69 31.05 22.72 3.44 2.63
Baseline 17.22 15.07 10.93 46.27 42.1Split-B CLIMB-3D(Ours) 35.48 31.56 23.69 8.00 5.51
Baseline 25.65 21.08 14.85 31.68 28.84Split-C CLIMB-3D(Ours) 31.59 26.78 18.93 9.10 7.89
To evaluate our proposed approach, we construct a baseline using ER [5] for the 3DIS
setting, as no existing incremental baselines are available for this task. In contrast, for the
3DSS setting, where incremental baselines do exist, we compare our method against three
prior approaches [43, 60, 74]. Results on CI-3DIS.Tab. 1 compares CLIMB-3D against the ER baseline across the three
CI-3DIS scenarios, evaluated after all phases using mAP 25, mAP50, overall mAP, and FPP. In theSplit-Ascenario, characterised by significant distribution shifts, our method achieves
gains of +19.23%, +16.76%, and +12.28% in mAP 25, mAP 50, and overall mAP, respec-
tively, while drastically reducing forgetting with FPP scores of 3.44% (mAP 25) and 2.63%
(mAP50), both over 45 points lower than the baseline. Under theSplit-Bscenario, with
semantically related new categories, improvements of +18.26%, +16.49%, and +12.76%
are observed alongside a reduction in forgetting to 8.00% and 5.51% from baseline levels
above 42%. For theSplit-Cscenario, featuring increasing geometric complexity, CLIMB-
3D maintains solid gains of +5.94%, +5.70%, and +4.08% across mAP metrics, with forget-
ting reduced by over 22 points. Overall, these results demonstrate robust forward transfer,
minimal forgetting, and stable learning, highlighting the methodâ€™s effectiveness for scalable
continual 3D semantic understanding. THENGANE ET AL.: CLIMB-3D9
Table 2: Comparison between the proposed method and existing baselines in the 3DSS set-
ting on ScanNetV2 [16], evaluated using mIoU. Methods Phase=1 Phase=2 All
EWC [60] 17.75 13.22 16.62
LwF [43] 30.38 13.37 26.13
Yanget al. [74] 34.16 13.43 28.98
CLIMB-3D(Ours) 69.39 32.56 59.38
Results on CI-3DSS.Although our primary focus is on CI-3DIS, we also evaluate our
method under the CI-3DSS setting to enable comparisons with existing approaches. To do
so, we adapt our predictions to assign each point the label corresponding to the highest-
confidence mask and exclude background classes (e.g., floor and wall), as these are not part
of the semantic segmentation targets. Tab. 2 presents a comparison between our method and
existing baselines on the ScanNet V2 dataset [16], evaluated using mIoU across two training
phases and overall. Although originally designed for instance segmentation, our method
generalises effectively to semantic segmentation, achieving substantial gains of +35.23%
mIoU in Phase 1 and +19.1% in Phase 2. Overall, it achieves 59.38% mIoU, significantly
higher than theâˆ¼30% mIoU of prior methods, highlighting its robustness and transferability. Refer to Appendix C, D, and F of the supplementary material for detailed analyses of per-
phase performance, rare-class evaluation, and qualitative result comparisons, respectively. 4.3 Ablation
Table 3: Ablation study illustrating the impact of each component in a three-phase setup. Each split (s) and corresponding number indicates data introduced at that phase. Best results
are highlighted inbold. Row Modules p=1â†‘p=2â†‘p=3â†‘ FPPâ†“
s1 s1 s2 All s1 s2 s3 All
1. Oracle - - - - 55.14 30.77 25.30 37.68 -
2. NaÃ¯ve 56.82 0.00 28.09 14.15 0.00 0.00 19.67 5.80 56.82
3. + ER 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28 46.44
4. + PLG 56.82 50.0034.3942.13 49.78 11.41 26.47 29.28 7.04
5. + CBR 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05 2.63
We conduct an ablation study to evaluate the contribution of each component in our
framework. As an upper bound, we report the performance of anOraclemodel, which is
trained jointly on the full dataset. For the incremental setting, we follow the previously
defined splits and first train the model naively across phases. We then incrementally add
each module to isolate its impact. Tab. 3 presents results for theSplit-Ascenario using both
mAP50 and FPP metrics. NaÃ¯ve Training.When trained naÃ¯vely without any dedicated modules, the model suffers
from severe catastrophic forgetting, as evident in row 2. The model entirely forgets pre-
viously learned classes upon entering new phases, with performance dropping to zero for
earlier splits and FPP rising to 56.82. 10THENGANE ET AL.: CLIMB-3D
Effect of ER.Adding exemplar replay (row 3) partially alleviates forgetting by maintaining
a buffer of past examples. This yields notable gains fors1in Phase 2 (+18.51%) and Phase 3
(+10.38%), and also improvess2in Phase 3 (+9.43%). However, the overall forgetting
remains substantial (FPP: 46.44), showing ER alone is insufficient. Effect of PLG.The addition of the pseudo-label generator (PLG), which generates labels for
previous classes by retaining a copy of the model from an earlier phase, facilitates knowledge
retention and forward transfer. As shown in row 4, PLG significantly reduces forgetting and
enhances performance on current tasks. Fors1, it improves mAP 50 by 31.49% in Phase 2
and 39.40% in Phase 3 over exemplar replay. Overall, PLG yields a 15.00% increase in
performance and reduces forgetting by 39.40%. Effect of CBR.Finally, class-balanced re-weighting (CBR) mitigates class imbalance dur-
ing both pseudo-labelling and current task learning by adjusting each classâ€™s contribution
based on its frequency. As shown in row 5, CBR enhancess1retention over PLG in Phases 2
and 3 (+4.67% and +4.41%), and further improves performance ons2ands3. It also achieves
the lowest FPP of 2.63, reflecting strong forgetting mitigation. Overall, CBR offers the best
trade-off between retaining past knowledge and acquiring new tasks, outperforming PLG
and ER by 4.41% and 43.81%, respectively. 4.4 Limitations & Future Work
While CLIMB-3D achieves strong performance across CI-3DIS and CI-3DSS settings, sev-
eral limitations remain and warrant discussion. The experiments are currently limited to
indoor datasets (ScanNet200 and ScanNetV2), therefore evaluating performance on outdoor
scenes would help assess generalisability across diverse environments. The current setup
also considers only three incremental phases, whereas real-world applications often involve
longer sequences; incorporating such extended setups would better reflect practical chal-
lenges. Moreover, the model operates in a uni-modal setting. Integrating multi-modal cues,
such as vision-language models, could further enhance performance, as suggested by recent
2D and 3D studies [25, 65, 66]. 5 Conclusion
This work addresses the challenge of catastrophic forgetting in class-incremental 3D in-
stance segmentation by introducing a modular framework that integrates exemplar replay
(ER), a pseudo-label generator (PLG), and class-balanced re-weighting (CBR). The pro-
posed method incrementally adapts to new classes while preserving prior knowledge, all
without requiring access to the full dataset. Extensive experiments on a three-phase bench-
mark validate the individual and combined effectiveness of each component: ER enables
efficient memory-based retention; PLG facilitates knowledge preservation through pseudo-
supervision; and CBR mitigates class imbalance to enhance learning stability. Together,
these components significantly reduce forgetting and improve segmentation performance
across all phases. By achieving a strong balance between stability and plasticity, our ap-
proach advances continual 3D scene understanding and sets a new baseline for future re-
search in this area. THENGANE ET AL.: CLIMB-3D11
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne
Tuytelaars. Memory aware synapses: Learning what (not) to forget. InECCV, pages
139â€“154, 2018. [2] Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long-tailed
recognition via weight balancing. InCVPR, pages 6897â€“6907, 2022. [3] Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying
imbalanced data. InJoint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 770â€“785."
What is the objective of the task in 3DIS as defined in the document?,"3DIS. The objective of this task is to detect and segment each object instance within a point cloud. Formally, the training dataset is denoted as D={(P i,Yi)}N i=1, where N is the number of training samples. Each input P i âˆˆR MÃ—6 is a coloured point cloud consisting of M points, where each point is represented by its 3D coordinates and RGB values. The corresponding annotation Y i ={(m i,j ,ci,j )}J i j=1 contains J i object instances, where m i,j âˆˆ {0,1}M is a binary mask indicating which points belong to the j-th instance, and c i,j âˆˆ C={1,...,C} is the semantic class label of that instance.","Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

3DIS. The objective of this task is to detect and segment each object instance within a point cloud. Formally, the training dataset is denoted as D={(P i,Yi)}N i=1, where N is the number of training samples. Each input P i âˆˆR MÃ—6 is a coloured point cloud consisting of M points, where each point is represented by its 3D coordinates and RGB values. The corresponding annotation Y i ={(m i,j ,ci,j )}J i j=1 contains J i object instances, where m i,j âˆˆ {0,1}M is a binary mask indicating which points belong to the j-th instance, and c i,j âˆˆ C={1,...,C} is the semantic class label of that instance.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020."
What are the three incremental learning scenarios proposed for CI-3DIS in the document?,"To address these challenges, we propose three incremental learning scenarios, each designed to capture a different facet of real-world complexity. The design of these scenarios is detailed below; for further information and illustrations, refer to Appendix E of the supplementary material. Split-A: Frequency Scenarios. This scenario acknowledges that datasets are often labelled based on the frequency of categories. To accommodate this, we propose a split where the model learns from the most frequent categories and subsequently incorporates the less frequent ones in later stages. By prioritising the training of frequently occurring categories, the model can establish a strong foundation before expanding to handle rare categories. Split-B: Semantic Scenarios. Beyond frequency, semantic similarity is crucial in real-world deployments. While objects often share visual or functional traits, models may encounter semantically different categories in new environments. To simulate this challenge, we introduce the Split-B scenario. Here, categories are grouped based on their semantic relationships, and the model is incrementally trained on one semantic group at a time. This setup encourages the model to generalise across semantically similar categories and adapt more effectively when exposed to new ones. Unlike the Split-A scenario, which organises learning based on category frequency, the Split-B scenario may contain both frequent and infrequent categories within a single task, focusing on semantic continuity and transfer. Split-C: Random Scenarios. In some cases, data labelling is driven by the availability of objects rather than predefined criteria. To capture this, we introduce the Split-C scenario, which represents a fully random setting where any class can appear in any task, resulting in varying levels of class imbalance. By exposing the model to such diverse and unpredictable distributions, we aim to improve its robustness in real-world situations where labelled data availability is inconsistent.","To address these challenges, we propose three incremental learning scenarios, each designed to capture a different facet of real-world complexity. The design of these scenarios is detailed below; for further information and illustrations, refer to Appendix E of the supplementary material. Split-A: Frequency Scenarios. This scenario acknowledges that datasets are often labelled based on the frequency of categories. To accommodate this, we propose a split where the model learns from the most frequent categories and subsequently incorporates the less frequent ones in later stages. By prioritising the training of frequently occurring categories, the model can establish a strong foundation before expanding to handle rare categories. Split-B: Semantic Scenarios. Beyond frequency, semantic similarity is crucial in real-world deployments. While objects often share visual or functional traits, models may encounter semantically different categories in new environments. To simulate this challenge, we introduce the Split-B scenario. Here, categories are grouped based on their semantic relationships, and the model is incrementally trained on one semantic group at a time. This setup encourages the model to generalise across semantically similar categories and adapt more effectively when exposed to new ones. Unlike the Split-A scenario, which organises learning based on category frequency, the Split-B scenario may contain both frequent and infrequent categories within a single task, focusing on semantic continuity and transfer. Split-C: Random Scenarios. In some cases, data labelling is driven by the availability of objects rather than predefined criteria. To capture this, we introduce the Split-C scenario, which represents a fully random setting where any class can appear in any task, resulting in varying levels of class imbalance. By exposing the model to such diverse and unpredictable distributions, we aim to improve its robustness in real-world situations where labelled data availability is inconsistent.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

[78] Biao Zhang and Peter Wonka. Point cloud instance segmentation using probabilistic
embeddings. InCVPR, pages 8883â€“8892, 2021. [79] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsuper-
vised lifelong learning.Neural Networks, 149:95â€“106, 2022. [80] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang,
and Jiaya Jia. Understanding imbalanced semantic segmentation through neural col-
lapse. InCVPR, pages 19550â€“19560, 2023. [81] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch
network with cumulative learning for long-tailed visual recognition. InCVPR, pages
9719â€“9728, 2020. [82] Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang
Jiang. Balanced contrastive learning for long-tailed visual recognition. InCVPR, pages
6908â€“6917, 2022. THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced 3D Instance
Segmentation
Supplementary Material
Appendix A Illustrating Incremental scenarios
Frequent Scenario Semantic Scenario
background
frequent common
rare semantic 
cluster 1
semantic 
cluster 2
semantic 
cluster 3
background
random 
cluster 1
random 
cluster 2
random 
cluster 3
background
Random Scenario
Figure 3: Tasks are grouped into incremental scenarios based on object frequency, semantic
similarity, and random assignment. , , and denote different tasks; shapes indicate
object categories; marks the background.Left:Grouped by category frequency.Middle:
Grouped by semantic similarity (e.g., similar shapes).Right:Randomly grouped, mixing
semantic and frequency variations. Appendix B Per Phase Analysis
We extend the analysis from Tab."
What is the final training objective of CLIMB-3D as formulated in the document?,"The final training objective of CLIMB-3D, incorporating ER, PLG, and CBR, is formulated in Eq. (1) as: L(Dt ER;Î¦t ) = 1 |Dt ER| âˆ‘ (P,Yt )âˆˆDt ER 1 |Yt | âˆ‘ (mt j,ct j)âˆˆYt   Lmask(mt j,Ë†mt j)+w â€²â€² c Â·Lcls(ct j,Ë†ct j)  , where w â€²â€² c =w â€² c Â·Î» cls denotes the scaled weight vector, and Î» cls is a hyperparameter for balancing the loss terms.","The final training objective of CLIMB-3D, incorporating ER, PLG, and CBR, is formulated in Eq. (1) as: L(Dt ER;Î¦t ) = 1 |Dt ER| âˆ‘ (P,Yt )âˆˆDt ER 1 |Yt | âˆ‘ (mt j,ct j)âˆˆYt   Lmask(mt j,Ë†mt j)+w â€²â€² c Â·Lcls(ct j,Ë†ct j)  , where w â€²â€² c =w â€² c Â·Î» cls denotes the scaled weight vector, and Î» cls is a hyperparameter for balancing the loss terms.

arXiv:2502.17429v3  [cs.CV]  21 Nov 2025
THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced
3D Instance Segmentation
Vishal Thengane1
v.thengane@surrey.ac.uk
Jean Lahoud2
jean.lahoud@mbzuai.ac.ae
Hisham Cholakkal2
hisham.cholakkal@mbzuai.ac.ae
Rao Muhammad Anwer2
rao.anwer@mbzuai.ac.ae
Lu Yin1
l.yin@surrey.ac.uk
Xiatian Zhu1
xiatian.zhu@surrey.ac.uk
Salman Khan2, 3
salman.khan@mbzuai.ac.ae
1 University of Surrey,
Guildford, UK
2 Mohamed bin Zayed University of
Artificial Intelligence,
Abu Dhabi, UAE
3 Australian National University,
Canberra, Australia
Abstract
While 3D instance segmentation (3DIS) has advanced significantly, most existing
methods assume that all object classes are known in advance and uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new
classes emerge gradually and exhibit natural imbalance. Although some approaches ad-
dress the emergence of new classes, they often overlook class imbalance, which leads
to suboptimal performance, particularly on rare categories. To tackle this, we propose
CLIMB-3D, a unified framework forCLass-incrementalImbalance-aware3DIS. Build-
ing upon established exemplar replay (ER) strategies, we show that ER alone is insuf-
ficient to achieve robust performance under memory constraints. To mitigate this, we
introduce a novel pseudo-label generator (PLG) that extends supervision to previously
learned categories by leveraging predictions from a frozen model trained on prior tasks. Despite its promise, PLG tends to be biased towards frequent classes. Therefore, we
propose a class-balanced re-weighting (CBR) scheme that estimates object frequencies
from pseudo-labels and dynamically adjusts training bias, without requiring access to
past data. We design and evaluate three incremental scenarios for 3DIS on the challeng-
ing ScanNet200 dataset and additionally validate our method for semantic segmentation
on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by
up to 16.76% mAP for instance segmentation and approximately 30% mIoU for semantic
segmentation, demonstrating strong generalisation across both frequent and rare classes. Code is available at:https://github.com/vgthengane/CLIMB3D. Â© 2025.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

PMLR, 2019. 14THENGANE ET AL.: CLIMB-3D
[43] Zhizhong Li and Derek Hoiem. Learning without forgetting.IEEE PAMI, 40(12):
2935â€“2947, 2017. [44] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmenta-
tion in 3d scenes using semantic superpoint tree networks. InICCV, pages 2783â€“2792,
2021. [45] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss
for dense object detection. InICCV, pages 2980â€“2988, 2017."
What are the evaluation metrics used to assess the model's performance in the 3D Instance Segmentation (3DIS) setting as mentioned in the document?,"We evaluate our method using Mean Average Precision (mAP), a standard metric for 3DIS which provides a comprehensive measure of segmentation quality by accounting for both precision and recall. For comparison with existing 3DSS methods, we report the mean Intersection over Union (mIoU), which quantifies the overlap between predicted and ground truth segments. To assess the modelâ€™s ability to mitigate catastrophic forgetting in incremental settings, we use the Forgetting Percentage Points (FPP) metric, as defined in [47].","1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

We evaluate our method using Mean Average Precision (mAP), a standard metric for 3DIS which provides a comprehensive measure of segmentation quality by accounting for both precision and recall. For comparison with existing 3DSS methods, we report the mean Intersection over Union (mIoU), which quantifies the overlap between predicted and ground truth segments. To assess the modelâ€™s ability to mitigate catastrophic forgetting in incremental settings, we use the Forgetting Percentage Points (FPP) metric, as defined in [47].

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig."
What is the objective of the task in CI-3DIS as described in the document?,"CI-3DIS. Unlike conventional 3DIS, CI-3DIS involves sequential learning, where object categories are introduced incrementally over T training tasks. Each task t introduces a disjoint set of classes C t , with C= ST t=1 Ct and C t âˆ©C tâ€² =/ 0 for tâ‰ t â€². During each task t, the model receives a dataset D t ={(P i,Yt i)}N i=1, where each coloured point cloud P i âˆˆR MÃ—6 contains M points, and the corresponding annotation Y t i ={(m t i,j ,c t i,j )} J t i j=1 includes instance masks m t i,j âˆˆ {0,1} M and semantic class labels c t i,j âˆˆ C t . The model Î¦ t , initialized from Î¦tâˆ’1 , is trained on D t to predict instance-level outputs Ë†Yt i ={(Ë†m t i,j ,Ë†c t i,j )} Ë†Jt i j=1, where each predicted semantic label Ë†c t i,j belongs to the cumulative set of all classes observed so far: St k=1 Ck. The key challenge in this setup is to learn new classes without forgetting those encountered in previous tasks, despite limited supervision and the absence of past labels.","arXiv:2502.17429v3  [cs.CV]  21 Nov 2025
THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced
3D Instance Segmentation
Vishal Thengane1
v.thengane@surrey.ac.uk
Jean Lahoud2
jean.lahoud@mbzuai.ac.ae
Hisham Cholakkal2
hisham.cholakkal@mbzuai.ac.ae
Rao Muhammad Anwer2
rao.anwer@mbzuai.ac.ae
Lu Yin1
l.yin@surrey.ac.uk
Xiatian Zhu1
xiatian.zhu@surrey.ac.uk
Salman Khan2, 3
salman.khan@mbzuai.ac.ae
1 University of Surrey,
Guildford, UK
2 Mohamed bin Zayed University of
Artificial Intelligence,
Abu Dhabi, UAE
3 Australian National University,
Canberra, Australia
Abstract
While 3D instance segmentation (3DIS) has advanced significantly, most existing
methods assume that all object classes are known in advance and uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new
classes emerge gradually and exhibit natural imbalance. Although some approaches ad-
dress the emergence of new classes, they often overlook class imbalance, which leads
to suboptimal performance, particularly on rare categories. To tackle this, we propose
CLIMB-3D, a unified framework forCLass-incrementalImbalance-aware3DIS. Build-
ing upon established exemplar replay (ER) strategies, we show that ER alone is insuf-
ficient to achieve robust performance under memory constraints. To mitigate this, we
introduce a novel pseudo-label generator (PLG) that extends supervision to previously
learned categories by leveraging predictions from a frozen model trained on prior tasks. Despite its promise, PLG tends to be biased towards frequent classes. Therefore, we
propose a class-balanced re-weighting (CBR) scheme that estimates object frequencies
from pseudo-labels and dynamically adjusts training bias, without requiring access to
past data. We design and evaluate three incremental scenarios for 3DIS on the challeng-
ing ScanNet200 dataset and additionally validate our method for semantic segmentation
on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by
up to 16.76% mAP for instance segmentation and approximately 30% mIoU for semantic
segmentation, demonstrating strong generalisation across both frequent and rare classes. Code is available at:https://github.com/vgthengane/CLIMB3D. Â© 2025.

CI-3DIS. Unlike conventional 3DIS, CI-3DIS involves sequential learning, where object categories are introduced incrementally over T training tasks. Each task t introduces a disjoint set of classes C t , with C= ST t=1 Ct and C t âˆ©C tâ€² =/ 0 for tâ‰ t â€². During each task t, the model receives a dataset D t ={(P i,Yt i)}N i=1, where each coloured point cloud P i âˆˆR MÃ—6 contains M points, and the corresponding annotation Y t i ={(m t i,j ,c t i,j )} J t i j=1 includes instance masks m t i,j âˆˆ {0,1} M and semantic class labels c t i,j âˆˆ C t . The model Î¦ t , initialized from Î¦tâˆ’1 , is trained on D t to predict instance-level outputs Ë†Yt i ={(Ë†m t i,j ,Ë†c t i,j )} Ë†Jt i j=1, where each predicted semantic label Ë†c t i,j belongs to the cumulative set of all classes observed so far: St k=1 Ck. The key challenge in this setup is to learn new classes without forgetting those encountered in previous tasks, despite limited supervision and the absence of past labels.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

[78] Biao Zhang and Peter Wonka. Point cloud instance segmentation using probabilistic
embeddings. InCVPR, pages 8883â€“8892, 2021. [79] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsuper-
vised lifelong learning.Neural Networks, 149:95â€“106, 2022. [80] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang,
and Jiaya Jia. Understanding imbalanced semantic segmentation through neural col-
lapse. InCVPR, pages 19550â€“19560, 2023. [81] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch
network with cumulative learning for long-tailed visual recognition. InCVPR, pages
9719â€“9728, 2020. [82] Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang
Jiang. Balanced contrastive learning for long-tailed visual recognition. InCVPR, pages
6908â€“6917, 2022. THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced 3D Instance
Segmentation
Supplementary Material
Appendix A Illustrating Incremental scenarios
Frequent Scenario Semantic Scenario
background
frequent common
rare semantic 
cluster 1
semantic 
cluster 2
semantic 
cluster 3
background
random 
cluster 1
random 
cluster 2
random 
cluster 3
background
Random Scenario
Figure 3: Tasks are grouped into incremental scenarios based on object frequency, semantic
similarity, and random assignment. , , and denote different tasks; shapes indicate
object categories; marks the background.Left:Grouped by category frequency.Middle:
Grouped by semantic similarity (e.g., similar shapes).Right:Randomly grouped, mixing
semantic and frequency variations. Appendix B Per Phase Analysis
We extend the analysis from Tab."
When was the document chunk published?,"Springer, 2017.","Elsevier, 1989.

Springer,
2016.

Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710.

Springer, 2017."
Who are the authors of the paper on 3D indoor instance segmentation in an open-world?,"Mohamed El Amine Boudjoghra, Salwa Al Khatib, Jean Lahoud, Hisham Cholakkal, Rao Anwer, Salman H Khan, and Fahad Shahbaz Khan.","Mohamed El Amine Boudjoghra, Salwa Al Khatib, Jean Lahoud, Hisham Cholakkal, Rao Anwer, Salman H Khan, and Fahad Shahbaz Khan.

Springer,
2016.

[4] Mohamed El Amine Boudjoghra, Salwa Al Khatib, Jean Lahoud, Hisham Cholakkal,
Rao Anwer, Salman H Khan, and Fahad Shahbaz Khan. 3d indoor instance segmenta-
tion in an open-world.NeurIPS, 36, 2024. [5] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calder-
ara. Dark experience for general continual learning: a strong, simple baseline.NeurIPS,
33:15920â€“15930, 2020. [6] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning
imbalanced datasets with label-distribution-aware margin loss.NeurIPS, 32, 2019. [7] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrastive continual learning. In
ICCV, pages 9516â€“9525, 2021. [8] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique.Journal of artificial intelligence
research, 16:321â€“357, 2002. [9] Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and Xinggang Wang. Hierarchical
aggregation for 3d instance segmentation. InICCV, pages 15467â€“15476, 2021. [10] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification is not
all you need for semantic segmentation.NeurIPS, 34:17864â€“17875, 2021. [11] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Gird-
har. Masked-attention mask transformer for universal image segmentation. InCVPR,
pages 1290â€“1299, 2022. [12] Julian Chibane, Francis Engelmann, Tuan Anh Tran, and Gerard Pons-Moll. Box2mask: Weakly supervised 3d semantic instance segmentation using bounding
boxes. InECCV, pages 681â€“699.

The copyright of this document resides with its authors."
What is the title of the paper by Pietro Buzzega et al. related to general continual learning?,"Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara.","PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017.

Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara.

The copyright of this document resides with its authors."
Who authored the paper on learning imbalanced datasets with label-distribution-aware margin loss?,"Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma.","PMLR, 2019. 14THENGANE ET AL.: CLIMB-3D
[43] Zhizhong Li and Derek Hoiem. Learning without forgetting.IEEE PAMI, 40(12):
2935â€“2947, 2017. [44] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmenta-
tion in 3d scenes using semantic superpoint tree networks. InICCV, pages 2783â€“2792,
2021. [45] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss
for dense object detection. InICCV, pages 2980â€“2988, 2017.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020."
In which year was the paper 'Co2l: Contrastive continual learning' presented?,"Hyuntak Cha, Jaeho Lee, and Jinwoo Shin.","PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

Hyuntak Cha, Jaeho Lee, and Jinwoo Shin.

Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710.

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591."
What technique is referred to as 'Smote' in the paper by Nitesh V Chawla et al.?,"Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer.","Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

[4] Mohamed El Amine Boudjoghra, Salwa Al Khatib, Jean Lahoud, Hisham Cholakkal,
Rao Anwer, Salman H Khan, and Fahad Shahbaz Khan. 3d indoor instance segmenta-
tion in an open-world.NeurIPS, 36, 2024. [5] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calder-
ara. Dark experience for general continual learning: a strong, simple baseline.NeurIPS,
33:15920â€“15930, 2020. [6] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning
imbalanced datasets with label-distribution-aware margin loss.NeurIPS, 32, 2019. [7] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrastive continual learning. In
ICCV, pages 9516â€“9525, 2021. [8] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique.Journal of artificial intelligence
research, 16:321â€“357, 2002. [9] Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and Xinggang Wang. Hierarchical
aggregation for 3d instance segmentation. InICCV, pages 15467â€“15476, 2021. [10] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification is not
all you need for semantic segmentation.NeurIPS, 34:17864â€“17875, 2021. [11] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Gird-
har. Masked-attention mask transformer for universal image segmentation. InCVPR,
pages 1290â€“1299, 2022. [12] Julian Chibane, Francis Engelmann, Tuan Anh Tran, and Gerard Pons-Moll. Box2mask: Weakly supervised 3d semantic instance segmentation using bounding
boxes. InECCV, pages 681â€“699."
Who are the authors of the paper on hierarchical aggregation for 3D instance segmentation?,"Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and Xinggang Wang.","PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective
learning of deep convolutional neural networks. InECCV, pages 467â€“482.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and Xinggang Wang."
What is emphasized as not being sufficient for semantic segmentation in the paper by Bowen Cheng et al.?,"Bowen Cheng, Alex Schwing, and Alexander Kirillov.","Springer,
2016.

[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017.

Bowen Cheng, Alex Schwing, and Alexander Kirillov.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020."
"What type of transformer is proposed in the paper by Bowen Cheng, Ishan Misra, et al. for universal image segmentation?","Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar.","Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650."
Who are the authors of the paper on weakly supervised 3D semantic instance segmentation using bounding boxes?,"Julian Chibane, Francis Engelmann, Tuan Anh Tran, and Gerard Pons-Moll.","Julian Chibane, Francis Engelmann, Tuan Anh Tran, and Gerard Pons-Moll.

[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017.

[4] Mohamed El Amine Boudjoghra, Salwa Al Khatib, Jean Lahoud, Hisham Cholakkal,
Rao Anwer, Salman H Khan, and Fahad Shahbaz Khan. 3d indoor instance segmenta-
tion in an open-world.NeurIPS, 36, 2024. [5] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calder-
ara. Dark experience for general continual learning: a strong, simple baseline.NeurIPS,
33:15920â€“15930, 2020. [6] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning
imbalanced datasets with label-distribution-aware margin loss.NeurIPS, 32, 2019. [7] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrastive continual learning. In
ICCV, pages 9516â€“9525, 2021. [8] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique.Journal of artificial intelligence
research, 16:321â€“357, 2002. [9] Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and Xinggang Wang. Hierarchical
aggregation for 3d instance segmentation. InICCV, pages 15467â€“15476, 2021. [10] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification is not
all you need for semantic segmentation.NeurIPS, 34:17864â€“17875, 2021. [11] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Gird-
har. Masked-attention mask transformer for universal image segmentation. InCVPR,
pages 1290â€“1299, 2022. [12] Julian Chibane, Francis Engelmann, Tuan Anh Tran, and Gerard Pons-Moll. Box2mask: Weakly supervised 3d semantic instance segmentation using bounding
boxes. InECCV, pages 681â€“699.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020."
In which conference was the paper 'Box2mask: Weakly supervised 3D semantic instance segmentation using bounding boxes' presented?,"In ECCV, pages 681â€“699.","IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710.

In ECCV, pages 681â€“699.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019."
Who are the authors of the paper 'Learning without forgetting for 3d point cloud objects'?,"Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman.","[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman.

The copyright of this document resides with its authors.

Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710."
"What is the title of the paper by Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling?",Feature space augmentation for long-tailed data.,"It may be distributed unchanged freely in print or electronic forms. 2THENGANE ET AL.: CLIMB-3D
1 Introduction
Table Chair Couch Pillow Coffee Table Sofa Chair Recycling BinCabinet Unknown
Task-1 Task-2 Task-3
Figure 1:Overview of the CI-3DIS setting.New object categories are introduced incre-
mentally with each task. After every phase, the model must recognise both newly added
and previously learned classes. For instance, in Task 2, categories such asPillow,Coffee
Table, andSofa Chairare introduced, while the model is expected to retain recognition
of earlier classes likeTable,Chair, andCouch. 3D instance segmentation (3DIS) is a fundamental task in computer vision that involves
identifying precise object boundaries and class labels in 3D space, with broad applications in
graphics, robotics, and augmented reality [4, 39]. Traditional 3DIS methods, including top-
down [28, 70, 78], bottom-up [26, 73], and transformer-based approaches [59], perform well
under the assumption that all classes are available and balanced during training. However,
this assumption does not hold in real-world environments, where new object classes emerge
over time and exhibit natural imbalance. This gap motivates the need for Class-Incremental Learning (CIL) [17, 54], which sup-
ports learning new categories while retaining knowledge of previous ones [51]. While CIL
has seen success in 2D image tasks [1, 43, 54, 60], extensions to 3D point clouds remain
limited, often focusing on object-level classification [13, 18, 48]. Recent works on scene-
level class-incremental 3DIS (CI-3DIS) [4] and class-incremental semantic segmentation
(CI-3DSS) [74] show promise, but depend heavily on large exemplar [56] and overlook class
imbalance, limiting their practicality. To address this, we proposeCLIMB-3D, a unified framework forCLass-incremental
IMBalance-aware3DIS that jointly tackles catastrophic forgetting and class imbalance. The
framework begins with Exemplar Replay (ER), storing a small number of samples from
past classes for later replay. However, this alone does not yield promising results under the
strict memory constraints required in CI-3DIS. To address this, we introduce a Pseudo-Label
Generator (PLG), which uses a frozen model from previous task to generate supervision for
earlier classes. However, we observed that PLG tends to favour frequent classes while ig-
noring under-represented ones. To mitigate this, we propose a Class-Balanced Re-weighting
(CBR), which estimates class frequencies from the pseudo-labels to enhance the learning of
rare classes without accessing past data. Together, these components form a practical and
effective solution for real-world 3DIS. To evaluate CLIMB-3D, we designed three benchmark scenarios for CI-3DIS based on
the ScanNet200 dataset [57]. These scenarios simulate incremental learning setup under nat-
ural class imbalance, where new classes emerge based on (A) object frequency, (B) semantic
similarity, or (C) random grouping. Additionally, for comparison with existing methods,
we evaluate our approach on CI-3DSS using the ScanNetV2 dataset [16]. Experimental re-
sults show that our method significantly reduces forgetting and improves performance across
both frequent and rare categories, outperforming previous methods in both settings. Fig. 1
illustrates the CI-3DIS setup. THENGANE ET AL.: CLIMB-3D3
In summary, our contributions are: (i) a novel problem setting of imbalanced class-
incremental 3DIS with an effective method to balance learning and mitigate forgetting; (ii)
three benchmarks modelling continual object emergence with natural imbalance; and (iii)
strong experimental results, achieving up to 16.76% mAP improvement over baselines. 2 Related Work
3D Instance Segmentation.Various methods have been proposed for 3DIS. Grouping-
based approaches adopt a bottom-up pipeline that learns latent embeddings for point cluster-
ing [9, 21, 22, 29, 37, 44, 70, 78]. In contrast, proposal-based methods follow a top-down
strategy by detecting 3D boxes and segmenting objects within them [19, 26, 46, 73, 76]. Recently, transformer-based models [68] have also been applied to 3DIS and 3DSS [59, 64],
inspired by advances in 2D vision [10, 11]. However, these approaches require full anno-
tations for all classes and are not designed for progressive learning, where only new class
annotations are available and previous data is inaccessible. Another line of work aims to re-
duce annotation costs by proposing weakly supervised 3DIS methods based on sparse cues
[12, 27, 72]. While effective with limited annotations, these methods assume a fixed set of
classes and are susceptible to catastrophic forgetting in incremental settings. Incremental Learning.Continual learning involves training models sequentially to miti-
gate catastrophic forgetting. One common strategy is model regularisation, which constrains
parameter updates using techniques such as elastic weight consolidation or knowledge dis-
tillation [1, 24, 35, 43, 60]. Another approach is exemplar replay, where past data is stored
or generated to rehearse older tasks while learning new ones [5, 7, 30, 54]. A third direc-
tion involves dynamically expanding the model architecture or using modular sub-networks
to accommodate new tasks without interfering with old ones [31, 42, 53, 58, 71, 79]. Re-
cent efforts in class-incremental 3D segmentation [63, 74] have shown early promise but
often rely on basic architectures and focus primarily on semantic segmentation. Other works
[4, 36] explore continual learning in 3D settings, though they often depend on large memory
buffers. In contrast, our work introduces a tailored framework for 3D instance segmentation
that effectively transfers knowledge across tasks, even under class imbalance. Long-tailed Recognition.Imbalanced class distributions lead to poor recognition of rare
(long-tail) classes. To address this, re-sampling methods balance the data via input [8, 20, 50,
52, 61] or feature space [3, 14, 40], avoiding naÃ¯ve under-/over-sampling. Loss re-weighting
offers another direction, using class-based [6, 15, 23, 32, 33, 69] or per-example [45, 55, 62]
adjustments to ensure fair contribution. Parameter regularisation improves generalisation
through weight constraints [2], albeit requiring careful tuning. Other approaches leverage
transfer learning [77, 81], self-supervision [41, 75], or contrastive learning [34, 41, 82] to
improve rare-class representations. Long-tailed recognition is well-studied in 2D with large-
scale datasets [15, 49, 67]; in 3D, ScanNet200 [16, 57] enables similar efforts. Prior 3D
work focused on re-weighting, re-sampling, and transfer learning [57], while regularisation
remains underexplored. CeCo [80] balances class centres via auxiliary loss but overlooks
sampling and augmentation. Lahoud et al. [38] propose adaptive classifier regularisation
for 3D segmentation, outperforming prior approaches without threshold tuning. However,
neither method considers incremental learning. In contrast, we jointly address long-tail and
continual 3D semantic segmentation. 4THENGANE ET AL.: CLIMB-3D
Feature 
Backbone
Transformer 
Decoder
Feature 
Backbone
Transformer 
Decoder
(ð‘·,ð’€ð‘¡)
Current model (ð›·
ð‘¡
)
Pseudo Label Generator (PLG) 
Selection
à·¨ð‘Œð¾
ð‘¡
â„’CIâˆ’3DIS
à·¨ð‘Œð¾
ð‘¡,ð‘¤
à´¤ð‘Œð¾
ð‘¡
à· ð‘Œð‘¡
(ð·ð‘¡)
Current Dataset 
Exemplar 
Replay (ER)
+
~
Class-Balanced  
Re-weighting 
(CBR)
1
ð‘“(ð‘)
Figure 2: Overview ofCLIMB-3Dfor CI-3DIS. The model incrementally learns new classes
across sequential phases. During taskt, point cloudsPand their corresponding labelsY t are
sampled from a combination of the current training datasetD t and Exemplar Replay (ER),
which maintains a small memory of past examples. These are then passed to the current
modelÎ¦ t to produce predicted labelsY t . The Pseudo-Label Generator (PLG) selects the
top-Kpredictions from the previous modelÎ¦ tâˆ’1 . These pseudo-labels are then weighted
based on class frequencyf(c)using Class-Balanced Re-weighting (CBR), and the top-Kre-
weighted labels are selected to form the balanced pseudo-label set Â¯Yt . This pseudo-label set
is then concatenated with the ground-truth labels to form a final augmented supervision set
Yt for taskt, which is used to optimise the modelÎ¦ t using Eq. (3). 3 Methodology
3.1 Problem Formulation
3DIS.The objective of this task is to detect and segment each object instance within a point
cloud. Formally, the training dataset is denoted asD={(P i,Yi)}N
i=1, whereNis the number
of training samples. Each inputP i âˆˆR MÃ—6 is a coloured point cloud consisting ofMpoints,
where each point is represented by its 3D coordinates and RGB values. The corresponding
annotationY i ={(m i,j ,ci,j )}Ji
j=1 containsJ i object instances, wheremi,j âˆˆ {0,1}M is a binary
mask indicating which points belong to thej-th instance, andc i,j âˆˆ C={1,...,C}is the
semantic class label of that instance. GivenP i, the modelÎ¦predicts instance-level outputs
Ë†Yi ={(Ë†mi,j ,Ë†ci,j )}
Ë†Ji
j=1, where Ë†mi,j and Ë†ci,j denote the predicted mask and category label for
thej-th instance, respectively. The number of predicted instances Ë†Ji varies depending on the
modelâ€™s inference. The model is optimised using the following objective:
L3DIS(D;Î¦) = 1
|D| âˆ‘
(P,Y)âˆˆD
1
|Y| âˆ‘
(mj,cj)âˆˆY
(Lmask(mj,Ë†mj)+Î»Â·L cls(cj,Ë†cj)),(1)
whereL cls andL mask are the average classification and mask losses over all instances, andÎ»
controls their trade-off. CI-3DIS.Unlike conventional 3DIS, CI-3DIS involves sequential learning, where object
categories are introduced incrementally overTtraining tasks. Each tasktintroduces a dis-
joint set of classesC t , withC= ST
t=1 Ct andC t âˆ©C tâ€²
=/ 0 fortÌ¸=t â€². During each taskt, the THENGANE ET AL.: CLIMB-3D5
model receives a datasetD t ={(P i,Yt
i)}N
i=1, where each coloured point cloudP i âˆˆR MÃ—6
containsMpoints, and the corresponding annotationY t
i ={(m t
i,j ,ct
i,j )}
Jt
i
j=1 includes instance
masksm t
i,j âˆˆ {0,1}M and semantic class labelsc t
i,j âˆˆ Ct . The modelÎ¦ t , initialized from
Î¦tâˆ’1 , is trained onD t to predict instance-level outputs Ë†Yt
i ={(Ë†mt
i,j ,Ë†ct
i,j )}
Ë†Jt
i
j=1, where each
predicted semantic label Ë†ct
i,j belongs to the cumulative set of all classes observed so far:St
k=1 Ck. The key challenge in this setup is to learn new classes without forgetting those
encountered in previous tasks, despite limited supervision and the absence of past labels. 3.2 Method: CLIMB-3D
Overview.As shown in Fig. 2 and formulated in Sec. 3.1, the proposed framework for
CI-3DIS follows aphase-wisetraining strategy. In each phase, the model is exposed to a
carefully curated subset of the dataset, simulating real-world scenarios discussed in Sec. 3.3. Training naÃ¯vely on such phased data leads tocatastrophic forgetting[51], where the model
forgets prior knowledge when learning new tasks. To address this, we first incorporateEx-
emplar Replay(ER) [5], a strategy inspired by 2D incremental methods [1, 43, 54] and
recently extended to 3D settings [4], to mitigate forgetting by storing a small set of repre-
sentative samples from earlier phases. However, ER alone is insufficient to achieve robust
performance. To address this, two additional components are introduced: aPseudo-Label
Generator(PLG), which leverages a frozen model to generate supervision signals for previ-
ously seen classes, and aClass-Balanced Re-weighting(CBR) module that compensates for
class imbalance across phases. Each of these components is described in detail below 1. Exemplar Replay (ER).Inspired by Buzzega et al. [5], ER addresses the issue of catas-
trophic forgetting by allowing the model to retain a subset of data from earlier phases. During phaset, the model is trained not only on the current task dataD t , but also on a
set of exemplarsE 1:tâˆ’1, collected from previous phases. These exemplars are accumulated
incrementally:E 1:tâˆ’1 =E 1 âˆªÂ·Â·Â·âˆªE tâˆ’1 .Here,E t denotes the exemplar subset stored after
phaset, and|E t |denotes the exemplar replay size. The combined replay dataset is denoted
as:D t
ER =D t âˆªE 1:tâˆ’1.Epoch training consists of two stages: first, the model is trained on
the current data,D t ; then it is trained using exemplars fromE 1:tâˆ’1. While previous CI-3DIS approaches rely on large exemplar sets to mitigate forgetting [4],
such strategies become impractical under memory constraints. Consequently, although ER
aids knowledge retention, it is insufficient on its own for robust performance with a limited
exemplar budget. Therefore, PLG and CBR are introduced to better preserve past represen-
tations and enhance generalisation across phases. Pseudo-Label Generator (PLG).In the CI-3DIS setting, during phaset(wheret>1),
although ground-truth labels for previously seen classes are unavailable, the model from the
previous phaseÎ¦ tâˆ’1 , which preserves knowledge of past tasks, is retained. We use this
model to generate pseudo-labels for previously seen classes, thereby providing approximate
supervision during training. This allows the current modelÎ¦t , to retain prior knowledge and
reduce forgetting, even without access to ground-truth annotations. Given a point cloud and label pair(P,Y t ), fromD t , the previous modelÎ¦ tâˆ’1 , generates
pseudo-labels for the previously seen classes as Ë†Y1:tâˆ’1 =Î¦ tâˆ’1 (P), which we denote as ËœYt
1For clarity, modifications introduced by each component (ER, PLG, and CBR) are highlighted in green, blue,
and red, respectively. 6THENGANE ET AL.: CLIMB-3D
for brevity. However, we observe that utilising all predictions fromÎ¦tâˆ’1 introduces noisy or
incorrect labels, which degrade overall performance. To mitigate this, we select the top-K
most confident instance predictions, denoted as ËœYt
K, and concatenate them with the ground-
truth labels of the current task to obtain the supervision set: Â¯Yt =Y t âˆ¥ ËœYt
K. Following this,
the loss is computed over both the current taskâ€™s ground-truth labels and the top-Kconfi-
dent pseudo-labels from earlier phases, enabling the model to retain prior knowledge while
adapting to new classes. Class-Balanced Re-weighting (CBR).While ER and PLG contribute to preserving prior
knowledge, we observe that the modelâ€™s predictions are biased towards frequent classes,
resulting in the forgetting of rare categories. This issue is further amplified by the top-K
pseudo-labels selected by PLG, which predominantly represent dominant classes in the data. To address this, we propose aClass-Balanced Re-weighting(CBR) scheme that compensates
for class imbalance using object frequency statistics. At taskt, only the current datasetD t
and its label distributionp t (c)over classescâˆˆ Ct are available. Datasets and class distribu-
tions from earlier tasks{D i,p i(c)}tâˆ’1
i=1 are no longer accessible, which makes it challenging
to directly account for class imbalance across all previously seen tasks. Therefore, we pro-
pose to leverage the pseudo-label predictions of the frozen modelÎ¦ tâˆ’1 as a proxy for the
class distribution across previously learned categories. During each training iteration, the previous modelÎ¦ tâˆ’1 is applied to the current inputP
to generate pseudo-labels ËœYt , from which we accumulate class-wise frequency statistics for
previously learned classes. LetC1:t = St
i=1 Ci represent the union of all classes seen up to task
t. At the end of each epoch, we compute the overall class frequencyfâˆˆR |C1:t | by combining
the pseudo-label distribution Ëœpt (c)derived from ËœYt with the ground-truth label distribution
pt (c)from the current task. The resulting frequency vectorf= [f(c)] câˆˆC1:t is then used to
compute class-wise weightsw= [w(c)] câˆˆC1:t , which guide balanced pseudo-label selection
and help the model focus on rare classes. In the next epoch, the modelÎ¦ tâˆ’1 produces soft pseudo-label predictions ËœYt (as de-
scribed in PLG) , where each component ËœYt [c]denotes the confidence score for classcâˆˆ
C1:tâˆ’1. To promote balanced pseudo-label selection, class-wise re-weighting is applied using
w(c) = 1
f(c)+Îµ , whereÎµis a small constant for numerical stability. The adjusted predictions
are computed by applying the class-wise weights as: ËœYt,w [c] =w(c)Â· ËœYt [c],âˆ€câˆˆ C 1:tâˆ’1. Top-Kselection is then applied to both the original and the re-weighted scores, yielding
two pseudo-label sets: ËœYt
K and ËœYt,w
K , respectively. The final augmented target set, Yt , is
constructed by concatenating the ground-truth labelsY t with both sets of pseudo-labels:
Yt =Y t âˆ¥ ËœYt
K âˆ¥ ËœYt,w
K .(2)
Although the above re-weighting mitigates bias from prior tasks, it does not address class
imbalance within the current taskt. To resolve this, the class-balanced weightsw c are ex-
tended to incorporate statistics from both previously seen and current classes, i.e., overC 1:t
rather than onlyC 1:tâˆ’1. The updated weights, denoted asw â€²
c, are used to re-weights both
pseudo-label selection and classification loss, ensuring balanced supervision across all seen
categories, including rare ones. This unified strategy reduces bias and enhances performance
across phases. The pseudo-code for constructing the supervision set in Eq. (2), which incor-
porates both PLG and CBR components, is provided in Appendix A. Final Objective.The final training objective ofCLIMB-3D, incorporating ER, PLG, and THENGANE ET AL.: CLIMB-3D7
CBR, is formulated in Eq. (1) as:
L(Dt
ER;Î¦t ) = 1
|Dt
ER| âˆ‘
(P,Yt )âˆˆDt
ER
1
|Yt | âˆ‘
(mt
j,ct
j)âˆˆYt
 
Lmask(mt
j,Ë†mt
j)+w â€²â€²
c Â·Lcls(ct
j,Ë†ct
j)

,(3)
wherew â€²â€²
c =w â€²
c Â·Î» cls denotes the scaled weight vector, andÎ» cls is a hyperparameter for bal-
ancing the loss terms. 3.3 Benchmarking Incremental Scenarios
While CI-3DIS methods offer a wide range of practical applications, they frequently rely
on the assumption of uniform sample distribution, which rarely holds in real-world settings. In practice, the number of object categories, denoted byC, is often large and characterised
by substantial variability in category frequency, shape, structure, and size. To address these
challenges, we propose three incremental learning scenarios, each designed to capture a
different facet of real-world complexity. The design of these scenarios is detailed below; for
further information and illustrations, refer to Appendix E of the supplementary material. Split-A: Frequency Scenarios.This scenario acknowledges that datasets are often la-
belled based on the frequency of categories. To accommodate this, we propose a split where
the model learns from the most frequent categories and subsequently incorporates the less
frequent ones in later stages. By prioritising the training of frequently occurring categories,
the model can establish a strong foundation before expanding to handle rare categories. Split-B: Semantic Scenarios.Beyond frequency, semantic similarity is crucial in real-
world deployments. While objects often share visual or functional traits, models may en-
counter semantically different categories in new environments. To simulate this challenge,
we introduce theSplit-Bscenario. Here, categories are grouped based on their semantic
relationships, and the model is incrementally trained on one semantic group at a time. This
setup encourages the model to generalise across semantically similar categories and adapt
more effectively when exposed to new ones. Unlike theSplit-Ascenario, which organises
learning based on category frequency, theSplit-Bscenario may contain both frequent and
infrequent categories within a single task, focusing on semantic continuity and transfer. Split-C: Random Scenarios.In some cases, data labelling is driven by the availability of
objects rather than predefined criteria. To capture this, we introduce theSplit-Cscenario,
which represents a fully random setting where any class can appear in any task, resulting in
varying levels of class imbalance. By exposing the model to such diverse and unpredictable
distributions, we aim to improve its robustness in real-world situations where labelled data
availability is inconsistent. These incremental scenarios are designed to provide a more realistic representation of
object distributions, frequencies, and dynamics encountered in the real world. 4 Experiments
4.1 Setup
Datasets.We evaluateCLIMB-3Don ScanNet200 [57], which comprises 200 object cat-
egories and exhibits significant class imbalance, which makes it ideal for simulating and 8THENGANE ET AL.: CLIMB-3D
assessing real-world scenarios. In addition, we compare our approach against existing in-
cremental learning methods using ScanNetV2 [16] in the 3DSS setting. For this evaluation,
we follow the standard training and validation splits defined in prior works [74] to ensure
consistency with existing methods. Evaluation Metrics.We evaluate our method usingMean Average Precision(mAP), a
standard metric for 3DIS which provides a comprehensive measure of segmentation quality
by accounting for both precision and recall. For comparison with existing 3DSS methods,
we report themean Intersection over Union(mIoU), which quantifies the overlap between
predicted and ground truth segments. To assess the modelâ€™s ability to mitigate catastrophic
forgetting in incremental settings, we use theForgetting Percentage Points(FPP) metric, as
defined in [47]. FPP captures performance degradation by measuring the accuracy drop on
the initially seen categories between the first and final training phases. Detailed descriptions of the incremental scenarios (Split-A,Split-B,Split-C) and im-
plementation are provided in Appendix B. 4.2 Results and Discussion
Table 1: Comparison between the proposed method and the baseline in the 3DIS setting,
evaluated using mAP25, mAP50, mAP, and FPP after training across all phase. Scenarios Methods Average Precisionâ†‘FPPâ†“
mAP25 mAP50 mAP mAP 25 mAP50
Baseline 16.46 14.29 10.44 51.30 46.82Split-A CLIMB-3D(Ours) 35.69 31.05 22.72 3.44 2.63
Baseline 17.22 15.07 10.93 46.27 42.1Split-B CLIMB-3D(Ours) 35.48 31.56 23.69 8.00 5.51
Baseline 25.65 21.08 14.85 31.68 28.84Split-C CLIMB-3D(Ours) 31.59 26.78 18.93 9.10 7.89
To evaluate our proposed approach, we construct a baseline using ER [5] for the 3DIS
setting, as no existing incremental baselines are available for this task. In contrast, for the
3DSS setting, where incremental baselines do exist, we compare our method against three
prior approaches [43, 60, 74]. Results on CI-3DIS.Tab. 1 compares CLIMB-3D against the ER baseline across the three
CI-3DIS scenarios, evaluated after all phases using mAP 25, mAP50, overall mAP, and FPP. In theSplit-Ascenario, characterised by significant distribution shifts, our method achieves
gains of +19.23%, +16.76%, and +12.28% in mAP 25, mAP 50, and overall mAP, respec-
tively, while drastically reducing forgetting with FPP scores of 3.44% (mAP 25) and 2.63%
(mAP50), both over 45 points lower than the baseline. Under theSplit-Bscenario, with
semantically related new categories, improvements of +18.26%, +16.49%, and +12.76%
are observed alongside a reduction in forgetting to 8.00% and 5.51% from baseline levels
above 42%. For theSplit-Cscenario, featuring increasing geometric complexity, CLIMB-
3D maintains solid gains of +5.94%, +5.70%, and +4.08% across mAP metrics, with forget-
ting reduced by over 22 points. Overall, these results demonstrate robust forward transfer,
minimal forgetting, and stable learning, highlighting the methodâ€™s effectiveness for scalable
continual 3D semantic understanding. THENGANE ET AL.: CLIMB-3D9
Table 2: Comparison between the proposed method and existing baselines in the 3DSS set-
ting on ScanNetV2 [16], evaluated using mIoU. Methods Phase=1 Phase=2 All
EWC [60] 17.75 13.22 16.62
LwF [43] 30.38 13.37 26.13
Yanget al. [74] 34.16 13.43 28.98
CLIMB-3D(Ours) 69.39 32.56 59.38
Results on CI-3DSS.Although our primary focus is on CI-3DIS, we also evaluate our
method under the CI-3DSS setting to enable comparisons with existing approaches. To do
so, we adapt our predictions to assign each point the label corresponding to the highest-
confidence mask and exclude background classes (e.g., floor and wall), as these are not part
of the semantic segmentation targets. Tab. 2 presents a comparison between our method and
existing baselines on the ScanNet V2 dataset [16], evaluated using mIoU across two training
phases and overall. Although originally designed for instance segmentation, our method
generalises effectively to semantic segmentation, achieving substantial gains of +35.23%
mIoU in Phase 1 and +19.1% in Phase 2. Overall, it achieves 59.38% mIoU, significantly
higher than theâˆ¼30% mIoU of prior methods, highlighting its robustness and transferability. Refer to Appendix C, D, and F of the supplementary material for detailed analyses of per-
phase performance, rare-class evaluation, and qualitative result comparisons, respectively. 4.3 Ablation
Table 3: Ablation study illustrating the impact of each component in a three-phase setup. Each split (s) and corresponding number indicates data introduced at that phase. Best results
are highlighted inbold. Row Modules p=1â†‘p=2â†‘p=3â†‘ FPPâ†“
s1 s1 s2 All s1 s2 s3 All
1. Oracle - - - - 55.14 30.77 25.30 37.68 -
2. NaÃ¯ve 56.82 0.00 28.09 14.15 0.00 0.00 19.67 5.80 56.82
3. + ER 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28 46.44
4. + PLG 56.82 50.0034.3942.13 49.78 11.41 26.47 29.28 7.04
5. + CBR 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05 2.63
We conduct an ablation study to evaluate the contribution of each component in our
framework. As an upper bound, we report the performance of anOraclemodel, which is
trained jointly on the full dataset. For the incremental setting, we follow the previously
defined splits and first train the model naively across phases. We then incrementally add
each module to isolate its impact. Tab. 3 presents results for theSplit-Ascenario using both
mAP50 and FPP metrics. NaÃ¯ve Training.When trained naÃ¯vely without any dedicated modules, the model suffers
from severe catastrophic forgetting, as evident in row 2. The model entirely forgets pre-
viously learned classes upon entering new phases, with performance dropping to zero for
earlier splits and FPP rising to 56.82. 10THENGANE ET AL.: CLIMB-3D
Effect of ER.Adding exemplar replay (row 3) partially alleviates forgetting by maintaining
a buffer of past examples. This yields notable gains fors1in Phase 2 (+18.51%) and Phase 3
(+10.38%), and also improvess2in Phase 3 (+9.43%). However, the overall forgetting
remains substantial (FPP: 46.44), showing ER alone is insufficient. Effect of PLG.The addition of the pseudo-label generator (PLG), which generates labels for
previous classes by retaining a copy of the model from an earlier phase, facilitates knowledge
retention and forward transfer. As shown in row 4, PLG significantly reduces forgetting and
enhances performance on current tasks. Fors1, it improves mAP 50 by 31.49% in Phase 2
and 39.40% in Phase 3 over exemplar replay. Overall, PLG yields a 15.00% increase in
performance and reduces forgetting by 39.40%. Effect of CBR.Finally, class-balanced re-weighting (CBR) mitigates class imbalance dur-
ing both pseudo-labelling and current task learning by adjusting each classâ€™s contribution
based on its frequency. As shown in row 5, CBR enhancess1retention over PLG in Phases 2
and 3 (+4.67% and +4.41%), and further improves performance ons2ands3. It also achieves
the lowest FPP of 2.63, reflecting strong forgetting mitigation. Overall, CBR offers the best
trade-off between retaining past knowledge and acquiring new tasks, outperforming PLG
and ER by 4.41% and 43.81%, respectively. 4.4 Limitations & Future Work
While CLIMB-3D achieves strong performance across CI-3DIS and CI-3DSS settings, sev-
eral limitations remain and warrant discussion. The experiments are currently limited to
indoor datasets (ScanNet200 and ScanNetV2), therefore evaluating performance on outdoor
scenes would help assess generalisability across diverse environments. The current setup
also considers only three incremental phases, whereas real-world applications often involve
longer sequences; incorporating such extended setups would better reflect practical chal-
lenges. Moreover, the model operates in a uni-modal setting. Integrating multi-modal cues,
such as vision-language models, could further enhance performance, as suggested by recent
2D and 3D studies [25, 65, 66]. 5 Conclusion
This work addresses the challenge of catastrophic forgetting in class-incremental 3D in-
stance segmentation by introducing a modular framework that integrates exemplar replay
(ER), a pseudo-label generator (PLG), and class-balanced re-weighting (CBR). The pro-
posed method incrementally adapts to new classes while preserving prior knowledge, all
without requiring access to the full dataset. Extensive experiments on a three-phase bench-
mark validate the individual and combined effectiveness of each component: ER enables
efficient memory-based retention; PLG facilitates knowledge preservation through pseudo-
supervision; and CBR mitigates class imbalance to enhance learning stability. Together,
these components significantly reduce forgetting and improve segmentation performance
across all phases. By achieving a strong balance between stability and plasticity, our ap-
proach advances continual 3D scene understanding and sets a new baseline for future re-
search in this area. THENGANE ET AL.: CLIMB-3D11
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne
Tuytelaars. Memory aware synapses: Learning what (not) to forget. InECCV, pages
139â€“154, 2018. [2] Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long-tailed
recognition via weight balancing. InCVPR, pages 6897â€“6907, 2022. [3] Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying
imbalanced data. InJoint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 770â€“785.

[30] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for
continual learning.arXiv preprint arXiv:1710.10368, 2017. [31] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence
of similar and dissimilar tasks.NeurIPS, 33:18493â€“18504, 2020. [32] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019. [33] Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and
Roberto Togneri. Cost-sensitive learning of deep feature representations from im-
balanced data.IEEE transactions on neural networks and learning systems, 29(8):
3573â€“3587, 2017. [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 33:18661â€“18673, 2020. [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-
jardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Pro-
ceedings of the national academy of sciences, 114(13):3521â€“3526, 2017. [36] Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual
learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024. [37] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3d instance
segmentation via multi-task metric learning. InICCV, pages 9256â€“9266, 2019. [38] Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and
Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint
and sampling. InInternational Conference on Robotics and Automation (ICRA), pages
5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029. [39] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,
and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022. [40] Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing
Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In
CVPR, pages 5212â€“5221, 2021. [41] Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-
tailed visual recognition. InICCV, pages 630â€“639, 2021. [42] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In
ICML, pages 3925â€“3934.

Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710.

Feature space augmentation for long-tailed data."
In which event was the paper 'Learning without forgetting for 3d point cloud objects' presented?,"16th International Work-Conference on Artificial Neural Networks, IWANN 2021, Virtual Event, June 16â€“18, 2021.","[60] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming
catastrophic forgetting with hard attention to the task. InICML, pages 4548â€“4557.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

16th International Work-Conference on Artificial Neural Networks, IWANN 2021, Virtual Event, June 16â€“18, 2021.

Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710."
"What is the page range for the paper by Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling?",pages 694â€“710.,"Springer,
2016.

Elsevier, 1989.

pages 694â€“710.

Springer, 2017."
How many pages is the paper 'Learning without forgetting for 3d point cloud objects'?,pages 484â€“497.,"Elsevier, 1989.

Springer, 2017.

pages 484â€“497.

Springer,
2016."
"What is the publication year of the paper by Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling?","Springer, 2022.","Springer, 2022.

Springer,
2016.

Springer, 2017.

Elsevier, 1989."
Who are the authors of the paper on 'Feature space augmentation for long-tailed data'?,"Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling.","Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling.

Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020."
"What is the title of the event where the paper by Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling was presented?",ECCV.,"ECCV.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective
learning of deep convolutional neural networks. InECCV, pages 467â€“482.

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591."
"When was the 16th International Work-Conference on Artificial Neural Networks, IWANN 2021 held?","June 16â€“18, 2021.","Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710.

Springer,
2016.

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

June 16â€“18, 2021."
"What is the topic of the paper by Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman?",Learning without forgetting for 3d point cloud objects.,"Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710.

PMLR, 2019. 14THENGANE ET AL.: CLIMB-3D
[43] Zhizhong Li and Derek Hoiem. Learning without forgetting.IEEE PAMI, 40(12):
2935â€“2947, 2017. [44] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmenta-
tion in 3d scenes using semantic superpoint tree networks. InICCV, pages 2783â€“2792,
2021. [45] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss
for dense object detection. InICCV, pages 2980â€“2988, 2017.

Learning without forgetting for 3d point cloud objects.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916."
"What is the title of the paper by Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie?",Class-balanced loss based on effective number of samples.,"[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021.

Class-balanced loss based on effective number of samples.

It may be distributed unchanged freely in print or electronic forms. 2THENGANE ET AL.: CLIMB-3D
1 Introduction
Table Chair Couch Pillow Coffee Table Sofa Chair Recycling BinCabinet Unknown
Task-1 Task-2 Task-3
Figure 1:Overview of the CI-3DIS setting.New object categories are introduced incre-
mentally with each task. After every phase, the model must recognise both newly added
and previously learned classes. For instance, in Task 2, categories such asPillow,Coffee
Table, andSofa Chairare introduced, while the model is expected to retain recognition
of earlier classes likeTable,Chair, andCouch. 3D instance segmentation (3DIS) is a fundamental task in computer vision that involves
identifying precise object boundaries and class labels in 3D space, with broad applications in
graphics, robotics, and augmented reality [4, 39]. Traditional 3DIS methods, including top-
down [28, 70, 78], bottom-up [26, 73], and transformer-based approaches [59], perform well
under the assumption that all classes are available and balanced during training. However,
this assumption does not hold in real-world environments, where new object classes emerge
over time and exhibit natural imbalance. This gap motivates the need for Class-Incremental Learning (CIL) [17, 54], which sup-
ports learning new categories while retaining knowledge of previous ones [51]. While CIL
has seen success in 2D image tasks [1, 43, 54, 60], extensions to 3D point clouds remain
limited, often focusing on object-level classification [13, 18, 48]. Recent works on scene-
level class-incremental 3DIS (CI-3DIS) [4] and class-incremental semantic segmentation
(CI-3DSS) [74] show promise, but depend heavily on large exemplar [56] and overlook class
imbalance, limiting their practicality. To address this, we proposeCLIMB-3D, a unified framework forCLass-incremental
IMBalance-aware3DIS that jointly tackles catastrophic forgetting and class imbalance. The
framework begins with Exemplar Replay (ER), storing a small number of samples from
past classes for later replay. However, this alone does not yield promising results under the
strict memory constraints required in CI-3DIS. To address this, we introduce a Pseudo-Label
Generator (PLG), which uses a frozen model from previous task to generate supervision for
earlier classes. However, we observed that PLG tends to favour frequent classes while ig-
noring under-represented ones. To mitigate this, we propose a Class-Balanced Re-weighting
(CBR), which estimates class frequencies from the pseudo-labels to enhance the learning of
rare classes without accessing past data. Together, these components form a practical and
effective solution for real-world 3DIS. To evaluate CLIMB-3D, we designed three benchmark scenarios for CI-3DIS based on
the ScanNet200 dataset [57]. These scenarios simulate incremental learning setup under nat-
ural class imbalance, where new classes emerge based on (A) object frequency, (B) semantic
similarity, or (C) random grouping. Additionally, for comparison with existing methods,
we evaluate our approach on CI-3DSS using the ScanNetV2 dataset [16]. Experimental re-
sults show that our method significantly reduces forgetting and improves performance across
both frequent and rare categories, outperforming previous methods in both settings. Fig. 1
illustrates the CI-3DIS setup. THENGANE ET AL.: CLIMB-3D3
In summary, our contributions are: (i) a novel problem setting of imbalanced class-
incremental 3DIS with an effective method to balance learning and mitigate forgetting; (ii)
three benchmarks modelling continual object emergence with natural imbalance; and (iii)
strong experimental results, achieving up to 16.76% mAP improvement over baselines. 2 Related Work
3D Instance Segmentation.Various methods have been proposed for 3DIS. Grouping-
based approaches adopt a bottom-up pipeline that learns latent embeddings for point cluster-
ing [9, 21, 22, 29, 37, 44, 70, 78]. In contrast, proposal-based methods follow a top-down
strategy by detecting 3D boxes and segmenting objects within them [19, 26, 46, 73, 76]. Recently, transformer-based models [68] have also been applied to 3DIS and 3DSS [59, 64],
inspired by advances in 2D vision [10, 11]. However, these approaches require full anno-
tations for all classes and are not designed for progressive learning, where only new class
annotations are available and previous data is inaccessible. Another line of work aims to re-
duce annotation costs by proposing weakly supervised 3DIS methods based on sparse cues
[12, 27, 72]. While effective with limited annotations, these methods assume a fixed set of
classes and are susceptible to catastrophic forgetting in incremental settings. Incremental Learning.Continual learning involves training models sequentially to miti-
gate catastrophic forgetting. One common strategy is model regularisation, which constrains
parameter updates using techniques such as elastic weight consolidation or knowledge dis-
tillation [1, 24, 35, 43, 60]. Another approach is exemplar replay, where past data is stored
or generated to rehearse older tasks while learning new ones [5, 7, 30, 54]. A third direc-
tion involves dynamically expanding the model architecture or using modular sub-networks
to accommodate new tasks without interfering with old ones [31, 42, 53, 58, 71, 79]. Re-
cent efforts in class-incremental 3D segmentation [63, 74] have shown early promise but
often rely on basic architectures and focus primarily on semantic segmentation. Other works
[4, 36] explore continual learning in 3D settings, though they often depend on large memory
buffers. In contrast, our work introduces a tailored framework for 3D instance segmentation
that effectively transfers knowledge across tasks, even under class imbalance. Long-tailed Recognition.Imbalanced class distributions lead to poor recognition of rare
(long-tail) classes. To address this, re-sampling methods balance the data via input [8, 20, 50,
52, 61] or feature space [3, 14, 40], avoiding naÃ¯ve under-/over-sampling. Loss re-weighting
offers another direction, using class-based [6, 15, 23, 32, 33, 69] or per-example [45, 55, 62]
adjustments to ensure fair contribution. Parameter regularisation improves generalisation
through weight constraints [2], albeit requiring careful tuning. Other approaches leverage
transfer learning [77, 81], self-supervision [41, 75], or contrastive learning [34, 41, 82] to
improve rare-class representations. Long-tailed recognition is well-studied in 2D with large-
scale datasets [15, 49, 67]; in 3D, ScanNet200 [16, 57] enables similar efforts. Prior 3D
work focused on re-weighting, re-sampling, and transfer learning [57], while regularisation
remains underexplored. CeCo [80] balances class centres via auxiliary loss but overlooks
sampling and augmentation. Lahoud et al. [38] propose adaptive classifier regularisation
for 3D segmentation, outperforming prior approaches without threshold tuning. However,
neither method considers incremental learning. In contrast, we jointly address long-tail and
continual 3D semantic segmentation. 4THENGANE ET AL.: CLIMB-3D
Feature 
Backbone
Transformer 
Decoder
Feature 
Backbone
Transformer 
Decoder
(ð‘·,ð’€ð‘¡)
Current model (ð›·
ð‘¡
)
Pseudo Label Generator (PLG) 
Selection
à·¨ð‘Œð¾
ð‘¡
â„’CIâˆ’3DIS
à·¨ð‘Œð¾
ð‘¡,ð‘¤
à´¤ð‘Œð¾
ð‘¡
à· ð‘Œð‘¡
(ð·ð‘¡)
Current Dataset 
Exemplar 
Replay (ER)
+
~
Class-Balanced  
Re-weighting 
(CBR)
1
ð‘“(ð‘)
Figure 2: Overview ofCLIMB-3Dfor CI-3DIS. The model incrementally learns new classes
across sequential phases. During taskt, point cloudsPand their corresponding labelsY t are
sampled from a combination of the current training datasetD t and Exemplar Replay (ER),
which maintains a small memory of past examples. These are then passed to the current
modelÎ¦ t to produce predicted labelsY t . The Pseudo-Label Generator (PLG) selects the
top-Kpredictions from the previous modelÎ¦ tâˆ’1 . These pseudo-labels are then weighted
based on class frequencyf(c)using Class-Balanced Re-weighting (CBR), and the top-Kre-
weighted labels are selected to form the balanced pseudo-label set Â¯Yt . This pseudo-label set
is then concatenated with the ground-truth labels to form a final augmented supervision set
Yt for taskt, which is used to optimise the modelÎ¦ t using Eq. (3). 3 Methodology
3.1 Problem Formulation
3DIS.The objective of this task is to detect and segment each object instance within a point
cloud. Formally, the training dataset is denoted asD={(P i,Yi)}N
i=1, whereNis the number
of training samples. Each inputP i âˆˆR MÃ—6 is a coloured point cloud consisting ofMpoints,
where each point is represented by its 3D coordinates and RGB values. The corresponding
annotationY i ={(m i,j ,ci,j )}Ji
j=1 containsJ i object instances, wheremi,j âˆˆ {0,1}M is a binary
mask indicating which points belong to thej-th instance, andc i,j âˆˆ C={1,...,C}is the
semantic class label of that instance. GivenP i, the modelÎ¦predicts instance-level outputs
Ë†Yi ={(Ë†mi,j ,Ë†ci,j )}
Ë†Ji
j=1, where Ë†mi,j and Ë†ci,j denote the predicted mask and category label for
thej-th instance, respectively. The number of predicted instances Ë†Ji varies depending on the
modelâ€™s inference. The model is optimised using the following objective:
L3DIS(D;Î¦) = 1
|D| âˆ‘
(P,Y)âˆˆD
1
|Y| âˆ‘
(mj,cj)âˆˆY
(Lmask(mj,Ë†mj)+Î»Â·L cls(cj,Ë†cj)),(1)
whereL cls andL mask are the average classification and mask losses over all instances, andÎ»
controls their trade-off. CI-3DIS.Unlike conventional 3DIS, CI-3DIS involves sequential learning, where object
categories are introduced incrementally overTtraining tasks. Each tasktintroduces a dis-
joint set of classesC t , withC= ST
t=1 Ct andC t âˆ©C tâ€²
=/ 0 fortÌ¸=t â€². During each taskt, the THENGANE ET AL.: CLIMB-3D5
model receives a datasetD t ={(P i,Yt
i)}N
i=1, where each coloured point cloudP i âˆˆR MÃ—6
containsMpoints, and the corresponding annotationY t
i ={(m t
i,j ,ct
i,j )}
Jt
i
j=1 includes instance
masksm t
i,j âˆˆ {0,1}M and semantic class labelsc t
i,j âˆˆ Ct . The modelÎ¦ t , initialized from
Î¦tâˆ’1 , is trained onD t to predict instance-level outputs Ë†Yt
i ={(Ë†mt
i,j ,Ë†ct
i,j )}
Ë†Jt
i
j=1, where each
predicted semantic label Ë†ct
i,j belongs to the cumulative set of all classes observed so far:St
k=1 Ck. The key challenge in this setup is to learn new classes without forgetting those
encountered in previous tasks, despite limited supervision and the absence of past labels. 3.2 Method: CLIMB-3D
Overview.As shown in Fig. 2 and formulated in Sec. 3.1, the proposed framework for
CI-3DIS follows aphase-wisetraining strategy. In each phase, the model is exposed to a
carefully curated subset of the dataset, simulating real-world scenarios discussed in Sec. 3.3. Training naÃ¯vely on such phased data leads tocatastrophic forgetting[51], where the model
forgets prior knowledge when learning new tasks. To address this, we first incorporateEx-
emplar Replay(ER) [5], a strategy inspired by 2D incremental methods [1, 43, 54] and
recently extended to 3D settings [4], to mitigate forgetting by storing a small set of repre-
sentative samples from earlier phases. However, ER alone is insufficient to achieve robust
performance. To address this, two additional components are introduced: aPseudo-Label
Generator(PLG), which leverages a frozen model to generate supervision signals for previ-
ously seen classes, and aClass-Balanced Re-weighting(CBR) module that compensates for
class imbalance across phases. Each of these components is described in detail below 1. Exemplar Replay (ER).Inspired by Buzzega et al. [5], ER addresses the issue of catas-
trophic forgetting by allowing the model to retain a subset of data from earlier phases. During phaset, the model is trained not only on the current task dataD t , but also on a
set of exemplarsE 1:tâˆ’1, collected from previous phases. These exemplars are accumulated
incrementally:E 1:tâˆ’1 =E 1 âˆªÂ·Â·Â·âˆªE tâˆ’1 .Here,E t denotes the exemplar subset stored after
phaset, and|E t |denotes the exemplar replay size. The combined replay dataset is denoted
as:D t
ER =D t âˆªE 1:tâˆ’1.Epoch training consists of two stages: first, the model is trained on
the current data,D t ; then it is trained using exemplars fromE 1:tâˆ’1. While previous CI-3DIS approaches rely on large exemplar sets to mitigate forgetting [4],
such strategies become impractical under memory constraints. Consequently, although ER
aids knowledge retention, it is insufficient on its own for robust performance with a limited
exemplar budget. Therefore, PLG and CBR are introduced to better preserve past represen-
tations and enhance generalisation across phases. Pseudo-Label Generator (PLG).In the CI-3DIS setting, during phaset(wheret>1),
although ground-truth labels for previously seen classes are unavailable, the model from the
previous phaseÎ¦ tâˆ’1 , which preserves knowledge of past tasks, is retained. We use this
model to generate pseudo-labels for previously seen classes, thereby providing approximate
supervision during training. This allows the current modelÎ¦t , to retain prior knowledge and
reduce forgetting, even without access to ground-truth annotations. Given a point cloud and label pair(P,Y t ), fromD t , the previous modelÎ¦ tâˆ’1 , generates
pseudo-labels for the previously seen classes as Ë†Y1:tâˆ’1 =Î¦ tâˆ’1 (P), which we denote as ËœYt
1For clarity, modifications introduced by each component (ER, PLG, and CBR) are highlighted in green, blue,
and red, respectively. 6THENGANE ET AL.: CLIMB-3D
for brevity. However, we observe that utilising all predictions fromÎ¦tâˆ’1 introduces noisy or
incorrect labels, which degrade overall performance. To mitigate this, we select the top-K
most confident instance predictions, denoted as ËœYt
K, and concatenate them with the ground-
truth labels of the current task to obtain the supervision set: Â¯Yt =Y t âˆ¥ ËœYt
K. Following this,
the loss is computed over both the current taskâ€™s ground-truth labels and the top-Kconfi-
dent pseudo-labels from earlier phases, enabling the model to retain prior knowledge while
adapting to new classes. Class-Balanced Re-weighting (CBR).While ER and PLG contribute to preserving prior
knowledge, we observe that the modelâ€™s predictions are biased towards frequent classes,
resulting in the forgetting of rare categories. This issue is further amplified by the top-K
pseudo-labels selected by PLG, which predominantly represent dominant classes in the data. To address this, we propose aClass-Balanced Re-weighting(CBR) scheme that compensates
for class imbalance using object frequency statistics. At taskt, only the current datasetD t
and its label distributionp t (c)over classescâˆˆ Ct are available. Datasets and class distribu-
tions from earlier tasks{D i,p i(c)}tâˆ’1
i=1 are no longer accessible, which makes it challenging
to directly account for class imbalance across all previously seen tasks. Therefore, we pro-
pose to leverage the pseudo-label predictions of the frozen modelÎ¦ tâˆ’1 as a proxy for the
class distribution across previously learned categories. During each training iteration, the previous modelÎ¦ tâˆ’1 is applied to the current inputP
to generate pseudo-labels ËœYt , from which we accumulate class-wise frequency statistics for
previously learned classes. LetC1:t = St
i=1 Ci represent the union of all classes seen up to task
t. At the end of each epoch, we compute the overall class frequencyfâˆˆR |C1:t | by combining
the pseudo-label distribution Ëœpt (c)derived from ËœYt with the ground-truth label distribution
pt (c)from the current task. The resulting frequency vectorf= [f(c)] câˆˆC1:t is then used to
compute class-wise weightsw= [w(c)] câˆˆC1:t , which guide balanced pseudo-label selection
and help the model focus on rare classes. In the next epoch, the modelÎ¦ tâˆ’1 produces soft pseudo-label predictions ËœYt (as de-
scribed in PLG) , where each component ËœYt [c]denotes the confidence score for classcâˆˆ
C1:tâˆ’1. To promote balanced pseudo-label selection, class-wise re-weighting is applied using
w(c) = 1
f(c)+Îµ , whereÎµis a small constant for numerical stability. The adjusted predictions
are computed by applying the class-wise weights as: ËœYt,w [c] =w(c)Â· ËœYt [c],âˆ€câˆˆ C 1:tâˆ’1. Top-Kselection is then applied to both the original and the re-weighted scores, yielding
two pseudo-label sets: ËœYt
K and ËœYt,w
K , respectively. The final augmented target set, Yt , is
constructed by concatenating the ground-truth labelsY t with both sets of pseudo-labels:
Yt =Y t âˆ¥ ËœYt
K âˆ¥ ËœYt,w
K .(2)
Although the above re-weighting mitigates bias from prior tasks, it does not address class
imbalance within the current taskt. To resolve this, the class-balanced weightsw c are ex-
tended to incorporate statistics from both previously seen and current classes, i.e., overC 1:t
rather than onlyC 1:tâˆ’1. The updated weights, denoted asw â€²
c, are used to re-weights both
pseudo-label selection and classification loss, ensuring balanced supervision across all seen
categories, including rare ones. This unified strategy reduces bias and enhances performance
across phases. The pseudo-code for constructing the supervision set in Eq. (2), which incor-
porates both PLG and CBR components, is provided in Appendix A. Final Objective.The final training objective ofCLIMB-3D, incorporating ER, PLG, and THENGANE ET AL.: CLIMB-3D7
CBR, is formulated in Eq. (1) as:
L(Dt
ER;Î¦t ) = 1
|Dt
ER| âˆ‘
(P,Yt )âˆˆDt
ER
1
|Yt | âˆ‘
(mt
j,ct
j)âˆˆYt
 
Lmask(mt
j,Ë†mt
j)+w â€²â€²
c Â·Lcls(ct
j,Ë†ct
j)

,(3)
wherew â€²â€²
c =w â€²
c Â·Î» cls denotes the scaled weight vector, andÎ» cls is a hyperparameter for bal-
ancing the loss terms. 3.3 Benchmarking Incremental Scenarios
While CI-3DIS methods offer a wide range of practical applications, they frequently rely
on the assumption of uniform sample distribution, which rarely holds in real-world settings. In practice, the number of object categories, denoted byC, is often large and characterised
by substantial variability in category frequency, shape, structure, and size. To address these
challenges, we propose three incremental learning scenarios, each designed to capture a
different facet of real-world complexity. The design of these scenarios is detailed below; for
further information and illustrations, refer to Appendix E of the supplementary material. Split-A: Frequency Scenarios.This scenario acknowledges that datasets are often la-
belled based on the frequency of categories. To accommodate this, we propose a split where
the model learns from the most frequent categories and subsequently incorporates the less
frequent ones in later stages. By prioritising the training of frequently occurring categories,
the model can establish a strong foundation before expanding to handle rare categories. Split-B: Semantic Scenarios.Beyond frequency, semantic similarity is crucial in real-
world deployments. While objects often share visual or functional traits, models may en-
counter semantically different categories in new environments. To simulate this challenge,
we introduce theSplit-Bscenario. Here, categories are grouped based on their semantic
relationships, and the model is incrementally trained on one semantic group at a time. This
setup encourages the model to generalise across semantically similar categories and adapt
more effectively when exposed to new ones. Unlike theSplit-Ascenario, which organises
learning based on category frequency, theSplit-Bscenario may contain both frequent and
infrequent categories within a single task, focusing on semantic continuity and transfer. Split-C: Random Scenarios.In some cases, data labelling is driven by the availability of
objects rather than predefined criteria. To capture this, we introduce theSplit-Cscenario,
which represents a fully random setting where any class can appear in any task, resulting in
varying levels of class imbalance. By exposing the model to such diverse and unpredictable
distributions, we aim to improve its robustness in real-world situations where labelled data
availability is inconsistent. These incremental scenarios are designed to provide a more realistic representation of
object distributions, frequencies, and dynamics encountered in the real world. 4 Experiments
4.1 Setup
Datasets.We evaluateCLIMB-3Don ScanNet200 [57], which comprises 200 object cat-
egories and exhibits significant class imbalance, which makes it ideal for simulating and 8THENGANE ET AL.: CLIMB-3D
assessing real-world scenarios. In addition, we compare our approach against existing in-
cremental learning methods using ScanNetV2 [16] in the 3DSS setting. For this evaluation,
we follow the standard training and validation splits defined in prior works [74] to ensure
consistency with existing methods. Evaluation Metrics.We evaluate our method usingMean Average Precision(mAP), a
standard metric for 3DIS which provides a comprehensive measure of segmentation quality
by accounting for both precision and recall. For comparison with existing 3DSS methods,
we report themean Intersection over Union(mIoU), which quantifies the overlap between
predicted and ground truth segments. To assess the modelâ€™s ability to mitigate catastrophic
forgetting in incremental settings, we use theForgetting Percentage Points(FPP) metric, as
defined in [47]. FPP captures performance degradation by measuring the accuracy drop on
the initially seen categories between the first and final training phases. Detailed descriptions of the incremental scenarios (Split-A,Split-B,Split-C) and im-
plementation are provided in Appendix B. 4.2 Results and Discussion
Table 1: Comparison between the proposed method and the baseline in the 3DIS setting,
evaluated using mAP25, mAP50, mAP, and FPP after training across all phase. Scenarios Methods Average Precisionâ†‘FPPâ†“
mAP25 mAP50 mAP mAP 25 mAP50
Baseline 16.46 14.29 10.44 51.30 46.82Split-A CLIMB-3D(Ours) 35.69 31.05 22.72 3.44 2.63
Baseline 17.22 15.07 10.93 46.27 42.1Split-B CLIMB-3D(Ours) 35.48 31.56 23.69 8.00 5.51
Baseline 25.65 21.08 14.85 31.68 28.84Split-C CLIMB-3D(Ours) 31.59 26.78 18.93 9.10 7.89
To evaluate our proposed approach, we construct a baseline using ER [5] for the 3DIS
setting, as no existing incremental baselines are available for this task. In contrast, for the
3DSS setting, where incremental baselines do exist, we compare our method against three
prior approaches [43, 60, 74]. Results on CI-3DIS.Tab. 1 compares CLIMB-3D against the ER baseline across the three
CI-3DIS scenarios, evaluated after all phases using mAP 25, mAP50, overall mAP, and FPP. In theSplit-Ascenario, characterised by significant distribution shifts, our method achieves
gains of +19.23%, +16.76%, and +12.28% in mAP 25, mAP 50, and overall mAP, respec-
tively, while drastically reducing forgetting with FPP scores of 3.44% (mAP 25) and 2.63%
(mAP50), both over 45 points lower than the baseline. Under theSplit-Bscenario, with
semantically related new categories, improvements of +18.26%, +16.49%, and +12.76%
are observed alongside a reduction in forgetting to 8.00% and 5.51% from baseline levels
above 42%. For theSplit-Cscenario, featuring increasing geometric complexity, CLIMB-
3D maintains solid gains of +5.94%, +5.70%, and +4.08% across mAP metrics, with forget-
ting reduced by over 22 points. Overall, these results demonstrate robust forward transfer,
minimal forgetting, and stable learning, highlighting the methodâ€™s effectiveness for scalable
continual 3D semantic understanding. THENGANE ET AL.: CLIMB-3D9
Table 2: Comparison between the proposed method and existing baselines in the 3DSS set-
ting on ScanNetV2 [16], evaluated using mIoU. Methods Phase=1 Phase=2 All
EWC [60] 17.75 13.22 16.62
LwF [43] 30.38 13.37 26.13
Yanget al. [74] 34.16 13.43 28.98
CLIMB-3D(Ours) 69.39 32.56 59.38
Results on CI-3DSS.Although our primary focus is on CI-3DIS, we also evaluate our
method under the CI-3DSS setting to enable comparisons with existing approaches. To do
so, we adapt our predictions to assign each point the label corresponding to the highest-
confidence mask and exclude background classes (e.g., floor and wall), as these are not part
of the semantic segmentation targets. Tab. 2 presents a comparison between our method and
existing baselines on the ScanNet V2 dataset [16], evaluated using mIoU across two training
phases and overall. Although originally designed for instance segmentation, our method
generalises effectively to semantic segmentation, achieving substantial gains of +35.23%
mIoU in Phase 1 and +19.1% in Phase 2. Overall, it achieves 59.38% mIoU, significantly
higher than theâˆ¼30% mIoU of prior methods, highlighting its robustness and transferability. Refer to Appendix C, D, and F of the supplementary material for detailed analyses of per-
phase performance, rare-class evaluation, and qualitative result comparisons, respectively. 4.3 Ablation
Table 3: Ablation study illustrating the impact of each component in a three-phase setup. Each split (s) and corresponding number indicates data introduced at that phase. Best results
are highlighted inbold. Row Modules p=1â†‘p=2â†‘p=3â†‘ FPPâ†“
s1 s1 s2 All s1 s2 s3 All
1. Oracle - - - - 55.14 30.77 25.30 37.68 -
2. NaÃ¯ve 56.82 0.00 28.09 14.15 0.00 0.00 19.67 5.80 56.82
3. + ER 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28 46.44
4. + PLG 56.82 50.0034.3942.13 49.78 11.41 26.47 29.28 7.04
5. + CBR 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05 2.63
We conduct an ablation study to evaluate the contribution of each component in our
framework. As an upper bound, we report the performance of anOraclemodel, which is
trained jointly on the full dataset. For the incremental setting, we follow the previously
defined splits and first train the model naively across phases. We then incrementally add
each module to isolate its impact. Tab. 3 presents results for theSplit-Ascenario using both
mAP50 and FPP metrics. NaÃ¯ve Training.When trained naÃ¯vely without any dedicated modules, the model suffers
from severe catastrophic forgetting, as evident in row 2. The model entirely forgets pre-
viously learned classes upon entering new phases, with performance dropping to zero for
earlier splits and FPP rising to 56.82. 10THENGANE ET AL.: CLIMB-3D
Effect of ER.Adding exemplar replay (row 3) partially alleviates forgetting by maintaining
a buffer of past examples. This yields notable gains fors1in Phase 2 (+18.51%) and Phase 3
(+10.38%), and also improvess2in Phase 3 (+9.43%). However, the overall forgetting
remains substantial (FPP: 46.44), showing ER alone is insufficient. Effect of PLG.The addition of the pseudo-label generator (PLG), which generates labels for
previous classes by retaining a copy of the model from an earlier phase, facilitates knowledge
retention and forward transfer. As shown in row 4, PLG significantly reduces forgetting and
enhances performance on current tasks. Fors1, it improves mAP 50 by 31.49% in Phase 2
and 39.40% in Phase 3 over exemplar replay. Overall, PLG yields a 15.00% increase in
performance and reduces forgetting by 39.40%. Effect of CBR.Finally, class-balanced re-weighting (CBR) mitigates class imbalance dur-
ing both pseudo-labelling and current task learning by adjusting each classâ€™s contribution
based on its frequency. As shown in row 5, CBR enhancess1retention over PLG in Phases 2
and 3 (+4.67% and +4.41%), and further improves performance ons2ands3. It also achieves
the lowest FPP of 2.63, reflecting strong forgetting mitigation. Overall, CBR offers the best
trade-off between retaining past knowledge and acquiring new tasks, outperforming PLG
and ER by 4.41% and 43.81%, respectively. 4.4 Limitations & Future Work
While CLIMB-3D achieves strong performance across CI-3DIS and CI-3DSS settings, sev-
eral limitations remain and warrant discussion. The experiments are currently limited to
indoor datasets (ScanNet200 and ScanNetV2), therefore evaluating performance on outdoor
scenes would help assess generalisability across diverse environments. The current setup
also considers only three incremental phases, whereas real-world applications often involve
longer sequences; incorporating such extended setups would better reflect practical chal-
lenges. Moreover, the model operates in a uni-modal setting. Integrating multi-modal cues,
such as vision-language models, could further enhance performance, as suggested by recent
2D and 3D studies [25, 65, 66]. 5 Conclusion
This work addresses the challenge of catastrophic forgetting in class-incremental 3D in-
stance segmentation by introducing a modular framework that integrates exemplar replay
(ER), a pseudo-label generator (PLG), and class-balanced re-weighting (CBR). The pro-
posed method incrementally adapts to new classes while preserving prior knowledge, all
without requiring access to the full dataset. Extensive experiments on a three-phase bench-
mark validate the individual and combined effectiveness of each component: ER enables
efficient memory-based retention; PLG facilitates knowledge preservation through pseudo-
supervision; and CBR mitigates class imbalance to enhance learning stability. Together,
these components significantly reduce forgetting and improve segmentation performance
across all phases. By achieving a strong balance between stability and plasticity, our ap-
proach advances continual 3D scene understanding and sets a new baseline for future re-
search in this area. THENGANE ET AL.: CLIMB-3D11
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne
Tuytelaars. Memory aware synapses: Learning what (not) to forget. InECCV, pages
139â€“154, 2018. [2] Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long-tailed
recognition via weight balancing. InCVPR, pages 6897â€“6907, 2022. [3] Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying
imbalanced data. InJoint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 770â€“785.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019."
"In which year was the paper by Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie published?","InCVPR, pages 9268â€“9277, 2019.","[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

InCVPR, pages 9268â€“9277, 2019.

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591."
"How many pages does the paper by Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie span?","InCVPR, pages 9268â€“9277, 2019.","InCVPR, pages 9268â€“9277, 2019.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019."
Who are the authors of the paper 'Class-balanced loss based on effective number of samples'?,"Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie.","Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie.

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019."
What conference is mentioned in the oracle_context?,"InCVPR, pages 9268â€“9277, 2019.","IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

InCVPR, pages 9268â€“9277, 2019."
"What type of loss is discussed in the paper by Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie?",Class-balanced loss based on effective number of samples.,"Class-balanced loss based on effective number of samples.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

It may be distributed unchanged freely in print or electronic forms. 2THENGANE ET AL.: CLIMB-3D
1 Introduction
Table Chair Couch Pillow Coffee Table Sofa Chair Recycling BinCabinet Unknown
Task-1 Task-2 Task-3
Figure 1:Overview of the CI-3DIS setting.New object categories are introduced incre-
mentally with each task. After every phase, the model must recognise both newly added
and previously learned classes. For instance, in Task 2, categories such asPillow,Coffee
Table, andSofa Chairare introduced, while the model is expected to retain recognition
of earlier classes likeTable,Chair, andCouch. 3D instance segmentation (3DIS) is a fundamental task in computer vision that involves
identifying precise object boundaries and class labels in 3D space, with broad applications in
graphics, robotics, and augmented reality [4, 39]. Traditional 3DIS methods, including top-
down [28, 70, 78], bottom-up [26, 73], and transformer-based approaches [59], perform well
under the assumption that all classes are available and balanced during training. However,
this assumption does not hold in real-world environments, where new object classes emerge
over time and exhibit natural imbalance. This gap motivates the need for Class-Incremental Learning (CIL) [17, 54], which sup-
ports learning new categories while retaining knowledge of previous ones [51]. While CIL
has seen success in 2D image tasks [1, 43, 54, 60], extensions to 3D point clouds remain
limited, often focusing on object-level classification [13, 18, 48]. Recent works on scene-
level class-incremental 3DIS (CI-3DIS) [4] and class-incremental semantic segmentation
(CI-3DSS) [74] show promise, but depend heavily on large exemplar [56] and overlook class
imbalance, limiting their practicality. To address this, we proposeCLIMB-3D, a unified framework forCLass-incremental
IMBalance-aware3DIS that jointly tackles catastrophic forgetting and class imbalance. The
framework begins with Exemplar Replay (ER), storing a small number of samples from
past classes for later replay. However, this alone does not yield promising results under the
strict memory constraints required in CI-3DIS. To address this, we introduce a Pseudo-Label
Generator (PLG), which uses a frozen model from previous task to generate supervision for
earlier classes. However, we observed that PLG tends to favour frequent classes while ig-
noring under-represented ones. To mitigate this, we propose a Class-Balanced Re-weighting
(CBR), which estimates class frequencies from the pseudo-labels to enhance the learning of
rare classes without accessing past data. Together, these components form a practical and
effective solution for real-world 3DIS. To evaluate CLIMB-3D, we designed three benchmark scenarios for CI-3DIS based on
the ScanNet200 dataset [57]. These scenarios simulate incremental learning setup under nat-
ural class imbalance, where new classes emerge based on (A) object frequency, (B) semantic
similarity, or (C) random grouping. Additionally, for comparison with existing methods,
we evaluate our approach on CI-3DSS using the ScanNetV2 dataset [16]. Experimental re-
sults show that our method significantly reduces forgetting and improves performance across
both frequent and rare categories, outperforming previous methods in both settings. Fig. 1
illustrates the CI-3DIS setup. THENGANE ET AL.: CLIMB-3D3
In summary, our contributions are: (i) a novel problem setting of imbalanced class-
incremental 3DIS with an effective method to balance learning and mitigate forgetting; (ii)
three benchmarks modelling continual object emergence with natural imbalance; and (iii)
strong experimental results, achieving up to 16.76% mAP improvement over baselines. 2 Related Work
3D Instance Segmentation.Various methods have been proposed for 3DIS. Grouping-
based approaches adopt a bottom-up pipeline that learns latent embeddings for point cluster-
ing [9, 21, 22, 29, 37, 44, 70, 78]. In contrast, proposal-based methods follow a top-down
strategy by detecting 3D boxes and segmenting objects within them [19, 26, 46, 73, 76]. Recently, transformer-based models [68] have also been applied to 3DIS and 3DSS [59, 64],
inspired by advances in 2D vision [10, 11]. However, these approaches require full anno-
tations for all classes and are not designed for progressive learning, where only new class
annotations are available and previous data is inaccessible. Another line of work aims to re-
duce annotation costs by proposing weakly supervised 3DIS methods based on sparse cues
[12, 27, 72]. While effective with limited annotations, these methods assume a fixed set of
classes and are susceptible to catastrophic forgetting in incremental settings. Incremental Learning.Continual learning involves training models sequentially to miti-
gate catastrophic forgetting. One common strategy is model regularisation, which constrains
parameter updates using techniques such as elastic weight consolidation or knowledge dis-
tillation [1, 24, 35, 43, 60]. Another approach is exemplar replay, where past data is stored
or generated to rehearse older tasks while learning new ones [5, 7, 30, 54]. A third direc-
tion involves dynamically expanding the model architecture or using modular sub-networks
to accommodate new tasks without interfering with old ones [31, 42, 53, 58, 71, 79]. Re-
cent efforts in class-incremental 3D segmentation [63, 74] have shown early promise but
often rely on basic architectures and focus primarily on semantic segmentation. Other works
[4, 36] explore continual learning in 3D settings, though they often depend on large memory
buffers. In contrast, our work introduces a tailored framework for 3D instance segmentation
that effectively transfers knowledge across tasks, even under class imbalance. Long-tailed Recognition.Imbalanced class distributions lead to poor recognition of rare
(long-tail) classes. To address this, re-sampling methods balance the data via input [8, 20, 50,
52, 61] or feature space [3, 14, 40], avoiding naÃ¯ve under-/over-sampling. Loss re-weighting
offers another direction, using class-based [6, 15, 23, 32, 33, 69] or per-example [45, 55, 62]
adjustments to ensure fair contribution. Parameter regularisation improves generalisation
through weight constraints [2], albeit requiring careful tuning. Other approaches leverage
transfer learning [77, 81], self-supervision [41, 75], or contrastive learning [34, 41, 82] to
improve rare-class representations. Long-tailed recognition is well-studied in 2D with large-
scale datasets [15, 49, 67]; in 3D, ScanNet200 [16, 57] enables similar efforts. Prior 3D
work focused on re-weighting, re-sampling, and transfer learning [57], while regularisation
remains underexplored. CeCo [80] balances class centres via auxiliary loss but overlooks
sampling and augmentation. Lahoud et al. [38] propose adaptive classifier regularisation
for 3D segmentation, outperforming prior approaches without threshold tuning. However,
neither method considers incremental learning. In contrast, we jointly address long-tail and
continual 3D semantic segmentation. 4THENGANE ET AL.: CLIMB-3D
Feature 
Backbone
Transformer 
Decoder
Feature 
Backbone
Transformer 
Decoder
(ð‘·,ð’€ð‘¡)
Current model (ð›·
ð‘¡
)
Pseudo Label Generator (PLG) 
Selection
à·¨ð‘Œð¾
ð‘¡
â„’CIâˆ’3DIS
à·¨ð‘Œð¾
ð‘¡,ð‘¤
à´¤ð‘Œð¾
ð‘¡
à· ð‘Œð‘¡
(ð·ð‘¡)
Current Dataset 
Exemplar 
Replay (ER)
+
~
Class-Balanced  
Re-weighting 
(CBR)
1
ð‘“(ð‘)
Figure 2: Overview ofCLIMB-3Dfor CI-3DIS. The model incrementally learns new classes
across sequential phases. During taskt, point cloudsPand their corresponding labelsY t are
sampled from a combination of the current training datasetD t and Exemplar Replay (ER),
which maintains a small memory of past examples. These are then passed to the current
modelÎ¦ t to produce predicted labelsY t . The Pseudo-Label Generator (PLG) selects the
top-Kpredictions from the previous modelÎ¦ tâˆ’1 . These pseudo-labels are then weighted
based on class frequencyf(c)using Class-Balanced Re-weighting (CBR), and the top-Kre-
weighted labels are selected to form the balanced pseudo-label set Â¯Yt . This pseudo-label set
is then concatenated with the ground-truth labels to form a final augmented supervision set
Yt for taskt, which is used to optimise the modelÎ¦ t using Eq. (3). 3 Methodology
3.1 Problem Formulation
3DIS.The objective of this task is to detect and segment each object instance within a point
cloud. Formally, the training dataset is denoted asD={(P i,Yi)}N
i=1, whereNis the number
of training samples. Each inputP i âˆˆR MÃ—6 is a coloured point cloud consisting ofMpoints,
where each point is represented by its 3D coordinates and RGB values. The corresponding
annotationY i ={(m i,j ,ci,j )}Ji
j=1 containsJ i object instances, wheremi,j âˆˆ {0,1}M is a binary
mask indicating which points belong to thej-th instance, andc i,j âˆˆ C={1,...,C}is the
semantic class label of that instance. GivenP i, the modelÎ¦predicts instance-level outputs
Ë†Yi ={(Ë†mi,j ,Ë†ci,j )}
Ë†Ji
j=1, where Ë†mi,j and Ë†ci,j denote the predicted mask and category label for
thej-th instance, respectively. The number of predicted instances Ë†Ji varies depending on the
modelâ€™s inference. The model is optimised using the following objective:
L3DIS(D;Î¦) = 1
|D| âˆ‘
(P,Y)âˆˆD
1
|Y| âˆ‘
(mj,cj)âˆˆY
(Lmask(mj,Ë†mj)+Î»Â·L cls(cj,Ë†cj)),(1)
whereL cls andL mask are the average classification and mask losses over all instances, andÎ»
controls their trade-off. CI-3DIS.Unlike conventional 3DIS, CI-3DIS involves sequential learning, where object
categories are introduced incrementally overTtraining tasks. Each tasktintroduces a dis-
joint set of classesC t , withC= ST
t=1 Ct andC t âˆ©C tâ€²
=/ 0 fortÌ¸=t â€². During each taskt, the THENGANE ET AL.: CLIMB-3D5
model receives a datasetD t ={(P i,Yt
i)}N
i=1, where each coloured point cloudP i âˆˆR MÃ—6
containsMpoints, and the corresponding annotationY t
i ={(m t
i,j ,ct
i,j )}
Jt
i
j=1 includes instance
masksm t
i,j âˆˆ {0,1}M and semantic class labelsc t
i,j âˆˆ Ct . The modelÎ¦ t , initialized from
Î¦tâˆ’1 , is trained onD t to predict instance-level outputs Ë†Yt
i ={(Ë†mt
i,j ,Ë†ct
i,j )}
Ë†Jt
i
j=1, where each
predicted semantic label Ë†ct
i,j belongs to the cumulative set of all classes observed so far:St
k=1 Ck. The key challenge in this setup is to learn new classes without forgetting those
encountered in previous tasks, despite limited supervision and the absence of past labels. 3.2 Method: CLIMB-3D
Overview.As shown in Fig. 2 and formulated in Sec. 3.1, the proposed framework for
CI-3DIS follows aphase-wisetraining strategy. In each phase, the model is exposed to a
carefully curated subset of the dataset, simulating real-world scenarios discussed in Sec. 3.3. Training naÃ¯vely on such phased data leads tocatastrophic forgetting[51], where the model
forgets prior knowledge when learning new tasks. To address this, we first incorporateEx-
emplar Replay(ER) [5], a strategy inspired by 2D incremental methods [1, 43, 54] and
recently extended to 3D settings [4], to mitigate forgetting by storing a small set of repre-
sentative samples from earlier phases. However, ER alone is insufficient to achieve robust
performance. To address this, two additional components are introduced: aPseudo-Label
Generator(PLG), which leverages a frozen model to generate supervision signals for previ-
ously seen classes, and aClass-Balanced Re-weighting(CBR) module that compensates for
class imbalance across phases. Each of these components is described in detail below 1. Exemplar Replay (ER).Inspired by Buzzega et al. [5], ER addresses the issue of catas-
trophic forgetting by allowing the model to retain a subset of data from earlier phases. During phaset, the model is trained not only on the current task dataD t , but also on a
set of exemplarsE 1:tâˆ’1, collected from previous phases. These exemplars are accumulated
incrementally:E 1:tâˆ’1 =E 1 âˆªÂ·Â·Â·âˆªE tâˆ’1 .Here,E t denotes the exemplar subset stored after
phaset, and|E t |denotes the exemplar replay size. The combined replay dataset is denoted
as:D t
ER =D t âˆªE 1:tâˆ’1.Epoch training consists of two stages: first, the model is trained on
the current data,D t ; then it is trained using exemplars fromE 1:tâˆ’1. While previous CI-3DIS approaches rely on large exemplar sets to mitigate forgetting [4],
such strategies become impractical under memory constraints. Consequently, although ER
aids knowledge retention, it is insufficient on its own for robust performance with a limited
exemplar budget. Therefore, PLG and CBR are introduced to better preserve past represen-
tations and enhance generalisation across phases. Pseudo-Label Generator (PLG).In the CI-3DIS setting, during phaset(wheret>1),
although ground-truth labels for previously seen classes are unavailable, the model from the
previous phaseÎ¦ tâˆ’1 , which preserves knowledge of past tasks, is retained. We use this
model to generate pseudo-labels for previously seen classes, thereby providing approximate
supervision during training. This allows the current modelÎ¦t , to retain prior knowledge and
reduce forgetting, even without access to ground-truth annotations. Given a point cloud and label pair(P,Y t ), fromD t , the previous modelÎ¦ tâˆ’1 , generates
pseudo-labels for the previously seen classes as Ë†Y1:tâˆ’1 =Î¦ tâˆ’1 (P), which we denote as ËœYt
1For clarity, modifications introduced by each component (ER, PLG, and CBR) are highlighted in green, blue,
and red, respectively. 6THENGANE ET AL.: CLIMB-3D
for brevity. However, we observe that utilising all predictions fromÎ¦tâˆ’1 introduces noisy or
incorrect labels, which degrade overall performance. To mitigate this, we select the top-K
most confident instance predictions, denoted as ËœYt
K, and concatenate them with the ground-
truth labels of the current task to obtain the supervision set: Â¯Yt =Y t âˆ¥ ËœYt
K. Following this,
the loss is computed over both the current taskâ€™s ground-truth labels and the top-Kconfi-
dent pseudo-labels from earlier phases, enabling the model to retain prior knowledge while
adapting to new classes. Class-Balanced Re-weighting (CBR).While ER and PLG contribute to preserving prior
knowledge, we observe that the modelâ€™s predictions are biased towards frequent classes,
resulting in the forgetting of rare categories. This issue is further amplified by the top-K
pseudo-labels selected by PLG, which predominantly represent dominant classes in the data. To address this, we propose aClass-Balanced Re-weighting(CBR) scheme that compensates
for class imbalance using object frequency statistics. At taskt, only the current datasetD t
and its label distributionp t (c)over classescâˆˆ Ct are available. Datasets and class distribu-
tions from earlier tasks{D i,p i(c)}tâˆ’1
i=1 are no longer accessible, which makes it challenging
to directly account for class imbalance across all previously seen tasks. Therefore, we pro-
pose to leverage the pseudo-label predictions of the frozen modelÎ¦ tâˆ’1 as a proxy for the
class distribution across previously learned categories. During each training iteration, the previous modelÎ¦ tâˆ’1 is applied to the current inputP
to generate pseudo-labels ËœYt , from which we accumulate class-wise frequency statistics for
previously learned classes. LetC1:t = St
i=1 Ci represent the union of all classes seen up to task
t. At the end of each epoch, we compute the overall class frequencyfâˆˆR |C1:t | by combining
the pseudo-label distribution Ëœpt (c)derived from ËœYt with the ground-truth label distribution
pt (c)from the current task. The resulting frequency vectorf= [f(c)] câˆˆC1:t is then used to
compute class-wise weightsw= [w(c)] câˆˆC1:t , which guide balanced pseudo-label selection
and help the model focus on rare classes. In the next epoch, the modelÎ¦ tâˆ’1 produces soft pseudo-label predictions ËœYt (as de-
scribed in PLG) , where each component ËœYt [c]denotes the confidence score for classcâˆˆ
C1:tâˆ’1. To promote balanced pseudo-label selection, class-wise re-weighting is applied using
w(c) = 1
f(c)+Îµ , whereÎµis a small constant for numerical stability. The adjusted predictions
are computed by applying the class-wise weights as: ËœYt,w [c] =w(c)Â· ËœYt [c],âˆ€câˆˆ C 1:tâˆ’1. Top-Kselection is then applied to both the original and the re-weighted scores, yielding
two pseudo-label sets: ËœYt
K and ËœYt,w
K , respectively. The final augmented target set, Yt , is
constructed by concatenating the ground-truth labelsY t with both sets of pseudo-labels:
Yt =Y t âˆ¥ ËœYt
K âˆ¥ ËœYt,w
K .(2)
Although the above re-weighting mitigates bias from prior tasks, it does not address class
imbalance within the current taskt. To resolve this, the class-balanced weightsw c are ex-
tended to incorporate statistics from both previously seen and current classes, i.e., overC 1:t
rather than onlyC 1:tâˆ’1. The updated weights, denoted asw â€²
c, are used to re-weights both
pseudo-label selection and classification loss, ensuring balanced supervision across all seen
categories, including rare ones. This unified strategy reduces bias and enhances performance
across phases. The pseudo-code for constructing the supervision set in Eq. (2), which incor-
porates both PLG and CBR components, is provided in Appendix A. Final Objective.The final training objective ofCLIMB-3D, incorporating ER, PLG, and THENGANE ET AL.: CLIMB-3D7
CBR, is formulated in Eq. (1) as:
L(Dt
ER;Î¦t ) = 1
|Dt
ER| âˆ‘
(P,Yt )âˆˆDt
ER
1
|Yt | âˆ‘
(mt
j,ct
j)âˆˆYt
 
Lmask(mt
j,Ë†mt
j)+w â€²â€²
c Â·Lcls(ct
j,Ë†ct
j)

,(3)
wherew â€²â€²
c =w â€²
c Â·Î» cls denotes the scaled weight vector, andÎ» cls is a hyperparameter for bal-
ancing the loss terms. 3.3 Benchmarking Incremental Scenarios
While CI-3DIS methods offer a wide range of practical applications, they frequently rely
on the assumption of uniform sample distribution, which rarely holds in real-world settings. In practice, the number of object categories, denoted byC, is often large and characterised
by substantial variability in category frequency, shape, structure, and size. To address these
challenges, we propose three incremental learning scenarios, each designed to capture a
different facet of real-world complexity. The design of these scenarios is detailed below; for
further information and illustrations, refer to Appendix E of the supplementary material. Split-A: Frequency Scenarios.This scenario acknowledges that datasets are often la-
belled based on the frequency of categories. To accommodate this, we propose a split where
the model learns from the most frequent categories and subsequently incorporates the less
frequent ones in later stages. By prioritising the training of frequently occurring categories,
the model can establish a strong foundation before expanding to handle rare categories. Split-B: Semantic Scenarios.Beyond frequency, semantic similarity is crucial in real-
world deployments. While objects often share visual or functional traits, models may en-
counter semantically different categories in new environments. To simulate this challenge,
we introduce theSplit-Bscenario. Here, categories are grouped based on their semantic
relationships, and the model is incrementally trained on one semantic group at a time. This
setup encourages the model to generalise across semantically similar categories and adapt
more effectively when exposed to new ones. Unlike theSplit-Ascenario, which organises
learning based on category frequency, theSplit-Bscenario may contain both frequent and
infrequent categories within a single task, focusing on semantic continuity and transfer. Split-C: Random Scenarios.In some cases, data labelling is driven by the availability of
objects rather than predefined criteria. To capture this, we introduce theSplit-Cscenario,
which represents a fully random setting where any class can appear in any task, resulting in
varying levels of class imbalance. By exposing the model to such diverse and unpredictable
distributions, we aim to improve its robustness in real-world situations where labelled data
availability is inconsistent. These incremental scenarios are designed to provide a more realistic representation of
object distributions, frequencies, and dynamics encountered in the real world. 4 Experiments
4.1 Setup
Datasets.We evaluateCLIMB-3Don ScanNet200 [57], which comprises 200 object cat-
egories and exhibits significant class imbalance, which makes it ideal for simulating and 8THENGANE ET AL.: CLIMB-3D
assessing real-world scenarios. In addition, we compare our approach against existing in-
cremental learning methods using ScanNetV2 [16] in the 3DSS setting. For this evaluation,
we follow the standard training and validation splits defined in prior works [74] to ensure
consistency with existing methods. Evaluation Metrics.We evaluate our method usingMean Average Precision(mAP), a
standard metric for 3DIS which provides a comprehensive measure of segmentation quality
by accounting for both precision and recall. For comparison with existing 3DSS methods,
we report themean Intersection over Union(mIoU), which quantifies the overlap between
predicted and ground truth segments. To assess the modelâ€™s ability to mitigate catastrophic
forgetting in incremental settings, we use theForgetting Percentage Points(FPP) metric, as
defined in [47]. FPP captures performance degradation by measuring the accuracy drop on
the initially seen categories between the first and final training phases. Detailed descriptions of the incremental scenarios (Split-A,Split-B,Split-C) and im-
plementation are provided in Appendix B. 4.2 Results and Discussion
Table 1: Comparison between the proposed method and the baseline in the 3DIS setting,
evaluated using mAP25, mAP50, mAP, and FPP after training across all phase. Scenarios Methods Average Precisionâ†‘FPPâ†“
mAP25 mAP50 mAP mAP 25 mAP50
Baseline 16.46 14.29 10.44 51.30 46.82Split-A CLIMB-3D(Ours) 35.69 31.05 22.72 3.44 2.63
Baseline 17.22 15.07 10.93 46.27 42.1Split-B CLIMB-3D(Ours) 35.48 31.56 23.69 8.00 5.51
Baseline 25.65 21.08 14.85 31.68 28.84Split-C CLIMB-3D(Ours) 31.59 26.78 18.93 9.10 7.89
To evaluate our proposed approach, we construct a baseline using ER [5] for the 3DIS
setting, as no existing incremental baselines are available for this task. In contrast, for the
3DSS setting, where incremental baselines do exist, we compare our method against three
prior approaches [43, 60, 74]. Results on CI-3DIS.Tab. 1 compares CLIMB-3D against the ER baseline across the three
CI-3DIS scenarios, evaluated after all phases using mAP 25, mAP50, overall mAP, and FPP. In theSplit-Ascenario, characterised by significant distribution shifts, our method achieves
gains of +19.23%, +16.76%, and +12.28% in mAP 25, mAP 50, and overall mAP, respec-
tively, while drastically reducing forgetting with FPP scores of 3.44% (mAP 25) and 2.63%
(mAP50), both over 45 points lower than the baseline. Under theSplit-Bscenario, with
semantically related new categories, improvements of +18.26%, +16.49%, and +12.76%
are observed alongside a reduction in forgetting to 8.00% and 5.51% from baseline levels
above 42%. For theSplit-Cscenario, featuring increasing geometric complexity, CLIMB-
3D maintains solid gains of +5.94%, +5.70%, and +4.08% across mAP metrics, with forget-
ting reduced by over 22 points. Overall, these results demonstrate robust forward transfer,
minimal forgetting, and stable learning, highlighting the methodâ€™s effectiveness for scalable
continual 3D semantic understanding. THENGANE ET AL.: CLIMB-3D9
Table 2: Comparison between the proposed method and existing baselines in the 3DSS set-
ting on ScanNetV2 [16], evaluated using mIoU. Methods Phase=1 Phase=2 All
EWC [60] 17.75 13.22 16.62
LwF [43] 30.38 13.37 26.13
Yanget al. [74] 34.16 13.43 28.98
CLIMB-3D(Ours) 69.39 32.56 59.38
Results on CI-3DSS.Although our primary focus is on CI-3DIS, we also evaluate our
method under the CI-3DSS setting to enable comparisons with existing approaches. To do
so, we adapt our predictions to assign each point the label corresponding to the highest-
confidence mask and exclude background classes (e.g., floor and wall), as these are not part
of the semantic segmentation targets. Tab. 2 presents a comparison between our method and
existing baselines on the ScanNet V2 dataset [16], evaluated using mIoU across two training
phases and overall. Although originally designed for instance segmentation, our method
generalises effectively to semantic segmentation, achieving substantial gains of +35.23%
mIoU in Phase 1 and +19.1% in Phase 2. Overall, it achieves 59.38% mIoU, significantly
higher than theâˆ¼30% mIoU of prior methods, highlighting its robustness and transferability. Refer to Appendix C, D, and F of the supplementary material for detailed analyses of per-
phase performance, rare-class evaluation, and qualitative result comparisons, respectively. 4.3 Ablation
Table 3: Ablation study illustrating the impact of each component in a three-phase setup. Each split (s) and corresponding number indicates data introduced at that phase. Best results
are highlighted inbold. Row Modules p=1â†‘p=2â†‘p=3â†‘ FPPâ†“
s1 s1 s2 All s1 s2 s3 All
1. Oracle - - - - 55.14 30.77 25.30 37.68 -
2. NaÃ¯ve 56.82 0.00 28.09 14.15 0.00 0.00 19.67 5.80 56.82
3. + ER 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28 46.44
4. + PLG 56.82 50.0034.3942.13 49.78 11.41 26.47 29.28 7.04
5. + CBR 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05 2.63
We conduct an ablation study to evaluate the contribution of each component in our
framework. As an upper bound, we report the performance of anOraclemodel, which is
trained jointly on the full dataset. For the incremental setting, we follow the previously
defined splits and first train the model naively across phases. We then incrementally add
each module to isolate its impact. Tab. 3 presents results for theSplit-Ascenario using both
mAP50 and FPP metrics. NaÃ¯ve Training.When trained naÃ¯vely without any dedicated modules, the model suffers
from severe catastrophic forgetting, as evident in row 2. The model entirely forgets pre-
viously learned classes upon entering new phases, with performance dropping to zero for
earlier splits and FPP rising to 56.82. 10THENGANE ET AL.: CLIMB-3D
Effect of ER.Adding exemplar replay (row 3) partially alleviates forgetting by maintaining
a buffer of past examples. This yields notable gains fors1in Phase 2 (+18.51%) and Phase 3
(+10.38%), and also improvess2in Phase 3 (+9.43%). However, the overall forgetting
remains substantial (FPP: 46.44), showing ER alone is insufficient. Effect of PLG.The addition of the pseudo-label generator (PLG), which generates labels for
previous classes by retaining a copy of the model from an earlier phase, facilitates knowledge
retention and forward transfer. As shown in row 4, PLG significantly reduces forgetting and
enhances performance on current tasks. Fors1, it improves mAP 50 by 31.49% in Phase 2
and 39.40% in Phase 3 over exemplar replay. Overall, PLG yields a 15.00% increase in
performance and reduces forgetting by 39.40%. Effect of CBR.Finally, class-balanced re-weighting (CBR) mitigates class imbalance dur-
ing both pseudo-labelling and current task learning by adjusting each classâ€™s contribution
based on its frequency. As shown in row 5, CBR enhancess1retention over PLG in Phases 2
and 3 (+4.67% and +4.41%), and further improves performance ons2ands3. It also achieves
the lowest FPP of 2.63, reflecting strong forgetting mitigation. Overall, CBR offers the best
trade-off between retaining past knowledge and acquiring new tasks, outperforming PLG
and ER by 4.41% and 43.81%, respectively. 4.4 Limitations & Future Work
While CLIMB-3D achieves strong performance across CI-3DIS and CI-3DSS settings, sev-
eral limitations remain and warrant discussion. The experiments are currently limited to
indoor datasets (ScanNet200 and ScanNetV2), therefore evaluating performance on outdoor
scenes would help assess generalisability across diverse environments. The current setup
also considers only three incremental phases, whereas real-world applications often involve
longer sequences; incorporating such extended setups would better reflect practical chal-
lenges. Moreover, the model operates in a uni-modal setting. Integrating multi-modal cues,
such as vision-language models, could further enhance performance, as suggested by recent
2D and 3D studies [25, 65, 66]. 5 Conclusion
This work addresses the challenge of catastrophic forgetting in class-incremental 3D in-
stance segmentation by introducing a modular framework that integrates exemplar replay
(ER), a pseudo-label generator (PLG), and class-balanced re-weighting (CBR). The pro-
posed method incrementally adapts to new classes while preserving prior knowledge, all
without requiring access to the full dataset. Extensive experiments on a three-phase bench-
mark validate the individual and combined effectiveness of each component: ER enables
efficient memory-based retention; PLG facilitates knowledge preservation through pseudo-
supervision; and CBR mitigates class imbalance to enhance learning stability. Together,
these components significantly reduce forgetting and improve segmentation performance
across all phases. By achieving a strong balance between stability and plasticity, our ap-
proach advances continual 3D scene understanding and sets a new baseline for future re-
search in this area. THENGANE ET AL.: CLIMB-3D11
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne
Tuytelaars. Memory aware synapses: Learning what (not) to forget. InECCV, pages
139â€“154, 2018. [2] Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long-tailed
recognition via weight balancing. InCVPR, pages 6897â€“6907, 2022. [3] Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying
imbalanced data. InJoint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 770â€“785.

[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021."
"What is the main focus of the paper by Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie?",Class-balanced loss based on effective number of samples.,"Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

Class-balanced loss based on effective number of samples.

[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021.

It may be distributed unchanged freely in print or electronic forms. 2THENGANE ET AL.: CLIMB-3D
1 Introduction
Table Chair Couch Pillow Coffee Table Sofa Chair Recycling BinCabinet Unknown
Task-1 Task-2 Task-3
Figure 1:Overview of the CI-3DIS setting.New object categories are introduced incre-
mentally with each task. After every phase, the model must recognise both newly added
and previously learned classes. For instance, in Task 2, categories such asPillow,Coffee
Table, andSofa Chairare introduced, while the model is expected to retain recognition
of earlier classes likeTable,Chair, andCouch. 3D instance segmentation (3DIS) is a fundamental task in computer vision that involves
identifying precise object boundaries and class labels in 3D space, with broad applications in
graphics, robotics, and augmented reality [4, 39]. Traditional 3DIS methods, including top-
down [28, 70, 78], bottom-up [26, 73], and transformer-based approaches [59], perform well
under the assumption that all classes are available and balanced during training. However,
this assumption does not hold in real-world environments, where new object classes emerge
over time and exhibit natural imbalance. This gap motivates the need for Class-Incremental Learning (CIL) [17, 54], which sup-
ports learning new categories while retaining knowledge of previous ones [51]. While CIL
has seen success in 2D image tasks [1, 43, 54, 60], extensions to 3D point clouds remain
limited, often focusing on object-level classification [13, 18, 48]. Recent works on scene-
level class-incremental 3DIS (CI-3DIS) [4] and class-incremental semantic segmentation
(CI-3DSS) [74] show promise, but depend heavily on large exemplar [56] and overlook class
imbalance, limiting their practicality. To address this, we proposeCLIMB-3D, a unified framework forCLass-incremental
IMBalance-aware3DIS that jointly tackles catastrophic forgetting and class imbalance. The
framework begins with Exemplar Replay (ER), storing a small number of samples from
past classes for later replay. However, this alone does not yield promising results under the
strict memory constraints required in CI-3DIS. To address this, we introduce a Pseudo-Label
Generator (PLG), which uses a frozen model from previous task to generate supervision for
earlier classes. However, we observed that PLG tends to favour frequent classes while ig-
noring under-represented ones. To mitigate this, we propose a Class-Balanced Re-weighting
(CBR), which estimates class frequencies from the pseudo-labels to enhance the learning of
rare classes without accessing past data. Together, these components form a practical and
effective solution for real-world 3DIS. To evaluate CLIMB-3D, we designed three benchmark scenarios for CI-3DIS based on
the ScanNet200 dataset [57]. These scenarios simulate incremental learning setup under nat-
ural class imbalance, where new classes emerge based on (A) object frequency, (B) semantic
similarity, or (C) random grouping. Additionally, for comparison with existing methods,
we evaluate our approach on CI-3DSS using the ScanNetV2 dataset [16]. Experimental re-
sults show that our method significantly reduces forgetting and improves performance across
both frequent and rare categories, outperforming previous methods in both settings. Fig. 1
illustrates the CI-3DIS setup. THENGANE ET AL.: CLIMB-3D3
In summary, our contributions are: (i) a novel problem setting of imbalanced class-
incremental 3DIS with an effective method to balance learning and mitigate forgetting; (ii)
three benchmarks modelling continual object emergence with natural imbalance; and (iii)
strong experimental results, achieving up to 16.76% mAP improvement over baselines. 2 Related Work
3D Instance Segmentation.Various methods have been proposed for 3DIS. Grouping-
based approaches adopt a bottom-up pipeline that learns latent embeddings for point cluster-
ing [9, 21, 22, 29, 37, 44, 70, 78]. In contrast, proposal-based methods follow a top-down
strategy by detecting 3D boxes and segmenting objects within them [19, 26, 46, 73, 76]. Recently, transformer-based models [68] have also been applied to 3DIS and 3DSS [59, 64],
inspired by advances in 2D vision [10, 11]. However, these approaches require full anno-
tations for all classes and are not designed for progressive learning, where only new class
annotations are available and previous data is inaccessible. Another line of work aims to re-
duce annotation costs by proposing weakly supervised 3DIS methods based on sparse cues
[12, 27, 72]. While effective with limited annotations, these methods assume a fixed set of
classes and are susceptible to catastrophic forgetting in incremental settings. Incremental Learning.Continual learning involves training models sequentially to miti-
gate catastrophic forgetting. One common strategy is model regularisation, which constrains
parameter updates using techniques such as elastic weight consolidation or knowledge dis-
tillation [1, 24, 35, 43, 60]. Another approach is exemplar replay, where past data is stored
or generated to rehearse older tasks while learning new ones [5, 7, 30, 54]. A third direc-
tion involves dynamically expanding the model architecture or using modular sub-networks
to accommodate new tasks without interfering with old ones [31, 42, 53, 58, 71, 79]. Re-
cent efforts in class-incremental 3D segmentation [63, 74] have shown early promise but
often rely on basic architectures and focus primarily on semantic segmentation. Other works
[4, 36] explore continual learning in 3D settings, though they often depend on large memory
buffers. In contrast, our work introduces a tailored framework for 3D instance segmentation
that effectively transfers knowledge across tasks, even under class imbalance. Long-tailed Recognition.Imbalanced class distributions lead to poor recognition of rare
(long-tail) classes. To address this, re-sampling methods balance the data via input [8, 20, 50,
52, 61] or feature space [3, 14, 40], avoiding naÃ¯ve under-/over-sampling. Loss re-weighting
offers another direction, using class-based [6, 15, 23, 32, 33, 69] or per-example [45, 55, 62]
adjustments to ensure fair contribution. Parameter regularisation improves generalisation
through weight constraints [2], albeit requiring careful tuning. Other approaches leverage
transfer learning [77, 81], self-supervision [41, 75], or contrastive learning [34, 41, 82] to
improve rare-class representations. Long-tailed recognition is well-studied in 2D with large-
scale datasets [15, 49, 67]; in 3D, ScanNet200 [16, 57] enables similar efforts. Prior 3D
work focused on re-weighting, re-sampling, and transfer learning [57], while regularisation
remains underexplored. CeCo [80] balances class centres via auxiliary loss but overlooks
sampling and augmentation. Lahoud et al. [38] propose adaptive classifier regularisation
for 3D segmentation, outperforming prior approaches without threshold tuning. However,
neither method considers incremental learning. In contrast, we jointly address long-tail and
continual 3D semantic segmentation. 4THENGANE ET AL.: CLIMB-3D
Feature 
Backbone
Transformer 
Decoder
Feature 
Backbone
Transformer 
Decoder
(ð‘·,ð’€ð‘¡)
Current model (ð›·
ð‘¡
)
Pseudo Label Generator (PLG) 
Selection
à·¨ð‘Œð¾
ð‘¡
â„’CIâˆ’3DIS
à·¨ð‘Œð¾
ð‘¡,ð‘¤
à´¤ð‘Œð¾
ð‘¡
à· ð‘Œð‘¡
(ð·ð‘¡)
Current Dataset 
Exemplar 
Replay (ER)
+
~
Class-Balanced  
Re-weighting 
(CBR)
1
ð‘“(ð‘)
Figure 2: Overview ofCLIMB-3Dfor CI-3DIS. The model incrementally learns new classes
across sequential phases. During taskt, point cloudsPand their corresponding labelsY t are
sampled from a combination of the current training datasetD t and Exemplar Replay (ER),
which maintains a small memory of past examples. These are then passed to the current
modelÎ¦ t to produce predicted labelsY t . The Pseudo-Label Generator (PLG) selects the
top-Kpredictions from the previous modelÎ¦ tâˆ’1 . These pseudo-labels are then weighted
based on class frequencyf(c)using Class-Balanced Re-weighting (CBR), and the top-Kre-
weighted labels are selected to form the balanced pseudo-label set Â¯Yt . This pseudo-label set
is then concatenated with the ground-truth labels to form a final augmented supervision set
Yt for taskt, which is used to optimise the modelÎ¦ t using Eq. (3). 3 Methodology
3.1 Problem Formulation
3DIS.The objective of this task is to detect and segment each object instance within a point
cloud. Formally, the training dataset is denoted asD={(P i,Yi)}N
i=1, whereNis the number
of training samples. Each inputP i âˆˆR MÃ—6 is a coloured point cloud consisting ofMpoints,
where each point is represented by its 3D coordinates and RGB values. The corresponding
annotationY i ={(m i,j ,ci,j )}Ji
j=1 containsJ i object instances, wheremi,j âˆˆ {0,1}M is a binary
mask indicating which points belong to thej-th instance, andc i,j âˆˆ C={1,...,C}is the
semantic class label of that instance. GivenP i, the modelÎ¦predicts instance-level outputs
Ë†Yi ={(Ë†mi,j ,Ë†ci,j )}
Ë†Ji
j=1, where Ë†mi,j and Ë†ci,j denote the predicted mask and category label for
thej-th instance, respectively. The number of predicted instances Ë†Ji varies depending on the
modelâ€™s inference. The model is optimised using the following objective:
L3DIS(D;Î¦) = 1
|D| âˆ‘
(P,Y)âˆˆD
1
|Y| âˆ‘
(mj,cj)âˆˆY
(Lmask(mj,Ë†mj)+Î»Â·L cls(cj,Ë†cj)),(1)
whereL cls andL mask are the average classification and mask losses over all instances, andÎ»
controls their trade-off. CI-3DIS.Unlike conventional 3DIS, CI-3DIS involves sequential learning, where object
categories are introduced incrementally overTtraining tasks. Each tasktintroduces a dis-
joint set of classesC t , withC= ST
t=1 Ct andC t âˆ©C tâ€²
=/ 0 fortÌ¸=t â€². During each taskt, the THENGANE ET AL.: CLIMB-3D5
model receives a datasetD t ={(P i,Yt
i)}N
i=1, where each coloured point cloudP i âˆˆR MÃ—6
containsMpoints, and the corresponding annotationY t
i ={(m t
i,j ,ct
i,j )}
Jt
i
j=1 includes instance
masksm t
i,j âˆˆ {0,1}M and semantic class labelsc t
i,j âˆˆ Ct . The modelÎ¦ t , initialized from
Î¦tâˆ’1 , is trained onD t to predict instance-level outputs Ë†Yt
i ={(Ë†mt
i,j ,Ë†ct
i,j )}
Ë†Jt
i
j=1, where each
predicted semantic label Ë†ct
i,j belongs to the cumulative set of all classes observed so far:St
k=1 Ck. The key challenge in this setup is to learn new classes without forgetting those
encountered in previous tasks, despite limited supervision and the absence of past labels. 3.2 Method: CLIMB-3D
Overview.As shown in Fig. 2 and formulated in Sec. 3.1, the proposed framework for
CI-3DIS follows aphase-wisetraining strategy. In each phase, the model is exposed to a
carefully curated subset of the dataset, simulating real-world scenarios discussed in Sec. 3.3. Training naÃ¯vely on such phased data leads tocatastrophic forgetting[51], where the model
forgets prior knowledge when learning new tasks. To address this, we first incorporateEx-
emplar Replay(ER) [5], a strategy inspired by 2D incremental methods [1, 43, 54] and
recently extended to 3D settings [4], to mitigate forgetting by storing a small set of repre-
sentative samples from earlier phases. However, ER alone is insufficient to achieve robust
performance. To address this, two additional components are introduced: aPseudo-Label
Generator(PLG), which leverages a frozen model to generate supervision signals for previ-
ously seen classes, and aClass-Balanced Re-weighting(CBR) module that compensates for
class imbalance across phases. Each of these components is described in detail below 1. Exemplar Replay (ER).Inspired by Buzzega et al. [5], ER addresses the issue of catas-
trophic forgetting by allowing the model to retain a subset of data from earlier phases. During phaset, the model is trained not only on the current task dataD t , but also on a
set of exemplarsE 1:tâˆ’1, collected from previous phases. These exemplars are accumulated
incrementally:E 1:tâˆ’1 =E 1 âˆªÂ·Â·Â·âˆªE tâˆ’1 .Here,E t denotes the exemplar subset stored after
phaset, and|E t |denotes the exemplar replay size. The combined replay dataset is denoted
as:D t
ER =D t âˆªE 1:tâˆ’1.Epoch training consists of two stages: first, the model is trained on
the current data,D t ; then it is trained using exemplars fromE 1:tâˆ’1. While previous CI-3DIS approaches rely on large exemplar sets to mitigate forgetting [4],
such strategies become impractical under memory constraints. Consequently, although ER
aids knowledge retention, it is insufficient on its own for robust performance with a limited
exemplar budget. Therefore, PLG and CBR are introduced to better preserve past represen-
tations and enhance generalisation across phases. Pseudo-Label Generator (PLG).In the CI-3DIS setting, during phaset(wheret>1),
although ground-truth labels for previously seen classes are unavailable, the model from the
previous phaseÎ¦ tâˆ’1 , which preserves knowledge of past tasks, is retained. We use this
model to generate pseudo-labels for previously seen classes, thereby providing approximate
supervision during training. This allows the current modelÎ¦t , to retain prior knowledge and
reduce forgetting, even without access to ground-truth annotations. Given a point cloud and label pair(P,Y t ), fromD t , the previous modelÎ¦ tâˆ’1 , generates
pseudo-labels for the previously seen classes as Ë†Y1:tâˆ’1 =Î¦ tâˆ’1 (P), which we denote as ËœYt
1For clarity, modifications introduced by each component (ER, PLG, and CBR) are highlighted in green, blue,
and red, respectively. 6THENGANE ET AL.: CLIMB-3D
for brevity. However, we observe that utilising all predictions fromÎ¦tâˆ’1 introduces noisy or
incorrect labels, which degrade overall performance. To mitigate this, we select the top-K
most confident instance predictions, denoted as ËœYt
K, and concatenate them with the ground-
truth labels of the current task to obtain the supervision set: Â¯Yt =Y t âˆ¥ ËœYt
K. Following this,
the loss is computed over both the current taskâ€™s ground-truth labels and the top-Kconfi-
dent pseudo-labels from earlier phases, enabling the model to retain prior knowledge while
adapting to new classes. Class-Balanced Re-weighting (CBR).While ER and PLG contribute to preserving prior
knowledge, we observe that the modelâ€™s predictions are biased towards frequent classes,
resulting in the forgetting of rare categories. This issue is further amplified by the top-K
pseudo-labels selected by PLG, which predominantly represent dominant classes in the data. To address this, we propose aClass-Balanced Re-weighting(CBR) scheme that compensates
for class imbalance using object frequency statistics. At taskt, only the current datasetD t
and its label distributionp t (c)over classescâˆˆ Ct are available. Datasets and class distribu-
tions from earlier tasks{D i,p i(c)}tâˆ’1
i=1 are no longer accessible, which makes it challenging
to directly account for class imbalance across all previously seen tasks. Therefore, we pro-
pose to leverage the pseudo-label predictions of the frozen modelÎ¦ tâˆ’1 as a proxy for the
class distribution across previously learned categories. During each training iteration, the previous modelÎ¦ tâˆ’1 is applied to the current inputP
to generate pseudo-labels ËœYt , from which we accumulate class-wise frequency statistics for
previously learned classes. LetC1:t = St
i=1 Ci represent the union of all classes seen up to task
t. At the end of each epoch, we compute the overall class frequencyfâˆˆR |C1:t | by combining
the pseudo-label distribution Ëœpt (c)derived from ËœYt with the ground-truth label distribution
pt (c)from the current task. The resulting frequency vectorf= [f(c)] câˆˆC1:t is then used to
compute class-wise weightsw= [w(c)] câˆˆC1:t , which guide balanced pseudo-label selection
and help the model focus on rare classes. In the next epoch, the modelÎ¦ tâˆ’1 produces soft pseudo-label predictions ËœYt (as de-
scribed in PLG) , where each component ËœYt [c]denotes the confidence score for classcâˆˆ
C1:tâˆ’1. To promote balanced pseudo-label selection, class-wise re-weighting is applied using
w(c) = 1
f(c)+Îµ , whereÎµis a small constant for numerical stability. The adjusted predictions
are computed by applying the class-wise weights as: ËœYt,w [c] =w(c)Â· ËœYt [c],âˆ€câˆˆ C 1:tâˆ’1. Top-Kselection is then applied to both the original and the re-weighted scores, yielding
two pseudo-label sets: ËœYt
K and ËœYt,w
K , respectively. The final augmented target set, Yt , is
constructed by concatenating the ground-truth labelsY t with both sets of pseudo-labels:
Yt =Y t âˆ¥ ËœYt
K âˆ¥ ËœYt,w
K .(2)
Although the above re-weighting mitigates bias from prior tasks, it does not address class
imbalance within the current taskt. To resolve this, the class-balanced weightsw c are ex-
tended to incorporate statistics from both previously seen and current classes, i.e., overC 1:t
rather than onlyC 1:tâˆ’1. The updated weights, denoted asw â€²
c, are used to re-weights both
pseudo-label selection and classification loss, ensuring balanced supervision across all seen
categories, including rare ones. This unified strategy reduces bias and enhances performance
across phases. The pseudo-code for constructing the supervision set in Eq. (2), which incor-
porates both PLG and CBR components, is provided in Appendix A. Final Objective.The final training objective ofCLIMB-3D, incorporating ER, PLG, and THENGANE ET AL.: CLIMB-3D7
CBR, is formulated in Eq. (1) as:
L(Dt
ER;Î¦t ) = 1
|Dt
ER| âˆ‘
(P,Yt )âˆˆDt
ER
1
|Yt | âˆ‘
(mt
j,ct
j)âˆˆYt
 
Lmask(mt
j,Ë†mt
j)+w â€²â€²
c Â·Lcls(ct
j,Ë†ct
j)

,(3)
wherew â€²â€²
c =w â€²
c Â·Î» cls denotes the scaled weight vector, andÎ» cls is a hyperparameter for bal-
ancing the loss terms. 3.3 Benchmarking Incremental Scenarios
While CI-3DIS methods offer a wide range of practical applications, they frequently rely
on the assumption of uniform sample distribution, which rarely holds in real-world settings. In practice, the number of object categories, denoted byC, is often large and characterised
by substantial variability in category frequency, shape, structure, and size. To address these
challenges, we propose three incremental learning scenarios, each designed to capture a
different facet of real-world complexity. The design of these scenarios is detailed below; for
further information and illustrations, refer to Appendix E of the supplementary material. Split-A: Frequency Scenarios.This scenario acknowledges that datasets are often la-
belled based on the frequency of categories. To accommodate this, we propose a split where
the model learns from the most frequent categories and subsequently incorporates the less
frequent ones in later stages. By prioritising the training of frequently occurring categories,
the model can establish a strong foundation before expanding to handle rare categories. Split-B: Semantic Scenarios.Beyond frequency, semantic similarity is crucial in real-
world deployments. While objects often share visual or functional traits, models may en-
counter semantically different categories in new environments. To simulate this challenge,
we introduce theSplit-Bscenario. Here, categories are grouped based on their semantic
relationships, and the model is incrementally trained on one semantic group at a time. This
setup encourages the model to generalise across semantically similar categories and adapt
more effectively when exposed to new ones. Unlike theSplit-Ascenario, which organises
learning based on category frequency, theSplit-Bscenario may contain both frequent and
infrequent categories within a single task, focusing on semantic continuity and transfer. Split-C: Random Scenarios.In some cases, data labelling is driven by the availability of
objects rather than predefined criteria. To capture this, we introduce theSplit-Cscenario,
which represents a fully random setting where any class can appear in any task, resulting in
varying levels of class imbalance. By exposing the model to such diverse and unpredictable
distributions, we aim to improve its robustness in real-world situations where labelled data
availability is inconsistent. These incremental scenarios are designed to provide a more realistic representation of
object distributions, frequencies, and dynamics encountered in the real world. 4 Experiments
4.1 Setup
Datasets.We evaluateCLIMB-3Don ScanNet200 [57], which comprises 200 object cat-
egories and exhibits significant class imbalance, which makes it ideal for simulating and 8THENGANE ET AL.: CLIMB-3D
assessing real-world scenarios. In addition, we compare our approach against existing in-
cremental learning methods using ScanNetV2 [16] in the 3DSS setting. For this evaluation,
we follow the standard training and validation splits defined in prior works [74] to ensure
consistency with existing methods. Evaluation Metrics.We evaluate our method usingMean Average Precision(mAP), a
standard metric for 3DIS which provides a comprehensive measure of segmentation quality
by accounting for both precision and recall. For comparison with existing 3DSS methods,
we report themean Intersection over Union(mIoU), which quantifies the overlap between
predicted and ground truth segments. To assess the modelâ€™s ability to mitigate catastrophic
forgetting in incremental settings, we use theForgetting Percentage Points(FPP) metric, as
defined in [47]. FPP captures performance degradation by measuring the accuracy drop on
the initially seen categories between the first and final training phases. Detailed descriptions of the incremental scenarios (Split-A,Split-B,Split-C) and im-
plementation are provided in Appendix B. 4.2 Results and Discussion
Table 1: Comparison between the proposed method and the baseline in the 3DIS setting,
evaluated using mAP25, mAP50, mAP, and FPP after training across all phase. Scenarios Methods Average Precisionâ†‘FPPâ†“
mAP25 mAP50 mAP mAP 25 mAP50
Baseline 16.46 14.29 10.44 51.30 46.82Split-A CLIMB-3D(Ours) 35.69 31.05 22.72 3.44 2.63
Baseline 17.22 15.07 10.93 46.27 42.1Split-B CLIMB-3D(Ours) 35.48 31.56 23.69 8.00 5.51
Baseline 25.65 21.08 14.85 31.68 28.84Split-C CLIMB-3D(Ours) 31.59 26.78 18.93 9.10 7.89
To evaluate our proposed approach, we construct a baseline using ER [5] for the 3DIS
setting, as no existing incremental baselines are available for this task. In contrast, for the
3DSS setting, where incremental baselines do exist, we compare our method against three
prior approaches [43, 60, 74]. Results on CI-3DIS.Tab. 1 compares CLIMB-3D against the ER baseline across the three
CI-3DIS scenarios, evaluated after all phases using mAP 25, mAP50, overall mAP, and FPP. In theSplit-Ascenario, characterised by significant distribution shifts, our method achieves
gains of +19.23%, +16.76%, and +12.28% in mAP 25, mAP 50, and overall mAP, respec-
tively, while drastically reducing forgetting with FPP scores of 3.44% (mAP 25) and 2.63%
(mAP50), both over 45 points lower than the baseline. Under theSplit-Bscenario, with
semantically related new categories, improvements of +18.26%, +16.49%, and +12.76%
are observed alongside a reduction in forgetting to 8.00% and 5.51% from baseline levels
above 42%. For theSplit-Cscenario, featuring increasing geometric complexity, CLIMB-
3D maintains solid gains of +5.94%, +5.70%, and +4.08% across mAP metrics, with forget-
ting reduced by over 22 points. Overall, these results demonstrate robust forward transfer,
minimal forgetting, and stable learning, highlighting the methodâ€™s effectiveness for scalable
continual 3D semantic understanding. THENGANE ET AL.: CLIMB-3D9
Table 2: Comparison between the proposed method and existing baselines in the 3DSS set-
ting on ScanNetV2 [16], evaluated using mIoU. Methods Phase=1 Phase=2 All
EWC [60] 17.75 13.22 16.62
LwF [43] 30.38 13.37 26.13
Yanget al. [74] 34.16 13.43 28.98
CLIMB-3D(Ours) 69.39 32.56 59.38
Results on CI-3DSS.Although our primary focus is on CI-3DIS, we also evaluate our
method under the CI-3DSS setting to enable comparisons with existing approaches. To do
so, we adapt our predictions to assign each point the label corresponding to the highest-
confidence mask and exclude background classes (e.g., floor and wall), as these are not part
of the semantic segmentation targets. Tab. 2 presents a comparison between our method and
existing baselines on the ScanNet V2 dataset [16], evaluated using mIoU across two training
phases and overall. Although originally designed for instance segmentation, our method
generalises effectively to semantic segmentation, achieving substantial gains of +35.23%
mIoU in Phase 1 and +19.1% in Phase 2. Overall, it achieves 59.38% mIoU, significantly
higher than theâˆ¼30% mIoU of prior methods, highlighting its robustness and transferability. Refer to Appendix C, D, and F of the supplementary material for detailed analyses of per-
phase performance, rare-class evaluation, and qualitative result comparisons, respectively. 4.3 Ablation
Table 3: Ablation study illustrating the impact of each component in a three-phase setup. Each split (s) and corresponding number indicates data introduced at that phase. Best results
are highlighted inbold. Row Modules p=1â†‘p=2â†‘p=3â†‘ FPPâ†“
s1 s1 s2 All s1 s2 s3 All
1. Oracle - - - - 55.14 30.77 25.30 37.68 -
2. NaÃ¯ve 56.82 0.00 28.09 14.15 0.00 0.00 19.67 5.80 56.82
3. + ER 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28 46.44
4. + PLG 56.82 50.0034.3942.13 49.78 11.41 26.47 29.28 7.04
5. + CBR 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05 2.63
We conduct an ablation study to evaluate the contribution of each component in our
framework. As an upper bound, we report the performance of anOraclemodel, which is
trained jointly on the full dataset. For the incremental setting, we follow the previously
defined splits and first train the model naively across phases. We then incrementally add
each module to isolate its impact. Tab. 3 presents results for theSplit-Ascenario using both
mAP50 and FPP metrics. NaÃ¯ve Training.When trained naÃ¯vely without any dedicated modules, the model suffers
from severe catastrophic forgetting, as evident in row 2. The model entirely forgets pre-
viously learned classes upon entering new phases, with performance dropping to zero for
earlier splits and FPP rising to 56.82. 10THENGANE ET AL.: CLIMB-3D
Effect of ER.Adding exemplar replay (row 3) partially alleviates forgetting by maintaining
a buffer of past examples. This yields notable gains fors1in Phase 2 (+18.51%) and Phase 3
(+10.38%), and also improvess2in Phase 3 (+9.43%). However, the overall forgetting
remains substantial (FPP: 46.44), showing ER alone is insufficient. Effect of PLG.The addition of the pseudo-label generator (PLG), which generates labels for
previous classes by retaining a copy of the model from an earlier phase, facilitates knowledge
retention and forward transfer. As shown in row 4, PLG significantly reduces forgetting and
enhances performance on current tasks. Fors1, it improves mAP 50 by 31.49% in Phase 2
and 39.40% in Phase 3 over exemplar replay. Overall, PLG yields a 15.00% increase in
performance and reduces forgetting by 39.40%. Effect of CBR.Finally, class-balanced re-weighting (CBR) mitigates class imbalance dur-
ing both pseudo-labelling and current task learning by adjusting each classâ€™s contribution
based on its frequency. As shown in row 5, CBR enhancess1retention over PLG in Phases 2
and 3 (+4.67% and +4.41%), and further improves performance ons2ands3. It also achieves
the lowest FPP of 2.63, reflecting strong forgetting mitigation. Overall, CBR offers the best
trade-off between retaining past knowledge and acquiring new tasks, outperforming PLG
and ER by 4.41% and 43.81%, respectively. 4.4 Limitations & Future Work
While CLIMB-3D achieves strong performance across CI-3DIS and CI-3DSS settings, sev-
eral limitations remain and warrant discussion. The experiments are currently limited to
indoor datasets (ScanNet200 and ScanNetV2), therefore evaluating performance on outdoor
scenes would help assess generalisability across diverse environments. The current setup
also considers only three incremental phases, whereas real-world applications often involve
longer sequences; incorporating such extended setups would better reflect practical chal-
lenges. Moreover, the model operates in a uni-modal setting. Integrating multi-modal cues,
such as vision-language models, could further enhance performance, as suggested by recent
2D and 3D studies [25, 65, 66]. 5 Conclusion
This work addresses the challenge of catastrophic forgetting in class-incremental 3D in-
stance segmentation by introducing a modular framework that integrates exemplar replay
(ER), a pseudo-label generator (PLG), and class-balanced re-weighting (CBR). The pro-
posed method incrementally adapts to new classes while preserving prior knowledge, all
without requiring access to the full dataset. Extensive experiments on a three-phase bench-
mark validate the individual and combined effectiveness of each component: ER enables
efficient memory-based retention; PLG facilitates knowledge preservation through pseudo-
supervision; and CBR mitigates class imbalance to enhance learning stability. Together,
these components significantly reduce forgetting and improve segmentation performance
across all phases. By achieving a strong balance between stability and plasticity, our ap-
proach advances continual 3D scene understanding and sets a new baseline for future re-
search in this area. THENGANE ET AL.: CLIMB-3D11
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne
Tuytelaars. Memory aware synapses: Learning what (not) to forget. InECCV, pages
139â€“154, 2018. [2] Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long-tailed
recognition via weight balancing. InCVPR, pages 6897â€“6907, 2022. [3] Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying
imbalanced data. InJoint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 770â€“785."
"How is the number of samples characterized in the paper by Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie?",Class-balanced loss based on effective number of samples.,"Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

Class-balanced loss based on effective number of samples.

It may be distributed unchanged freely in print or electronic forms. 2THENGANE ET AL.: CLIMB-3D
1 Introduction
Table Chair Couch Pillow Coffee Table Sofa Chair Recycling BinCabinet Unknown
Task-1 Task-2 Task-3
Figure 1:Overview of the CI-3DIS setting.New object categories are introduced incre-
mentally with each task. After every phase, the model must recognise both newly added
and previously learned classes. For instance, in Task 2, categories such asPillow,Coffee
Table, andSofa Chairare introduced, while the model is expected to retain recognition
of earlier classes likeTable,Chair, andCouch. 3D instance segmentation (3DIS) is a fundamental task in computer vision that involves
identifying precise object boundaries and class labels in 3D space, with broad applications in
graphics, robotics, and augmented reality [4, 39]. Traditional 3DIS methods, including top-
down [28, 70, 78], bottom-up [26, 73], and transformer-based approaches [59], perform well
under the assumption that all classes are available and balanced during training. However,
this assumption does not hold in real-world environments, where new object classes emerge
over time and exhibit natural imbalance. This gap motivates the need for Class-Incremental Learning (CIL) [17, 54], which sup-
ports learning new categories while retaining knowledge of previous ones [51]. While CIL
has seen success in 2D image tasks [1, 43, 54, 60], extensions to 3D point clouds remain
limited, often focusing on object-level classification [13, 18, 48]. Recent works on scene-
level class-incremental 3DIS (CI-3DIS) [4] and class-incremental semantic segmentation
(CI-3DSS) [74] show promise, but depend heavily on large exemplar [56] and overlook class
imbalance, limiting their practicality. To address this, we proposeCLIMB-3D, a unified framework forCLass-incremental
IMBalance-aware3DIS that jointly tackles catastrophic forgetting and class imbalance. The
framework begins with Exemplar Replay (ER), storing a small number of samples from
past classes for later replay. However, this alone does not yield promising results under the
strict memory constraints required in CI-3DIS. To address this, we introduce a Pseudo-Label
Generator (PLG), which uses a frozen model from previous task to generate supervision for
earlier classes. However, we observed that PLG tends to favour frequent classes while ig-
noring under-represented ones. To mitigate this, we propose a Class-Balanced Re-weighting
(CBR), which estimates class frequencies from the pseudo-labels to enhance the learning of
rare classes without accessing past data. Together, these components form a practical and
effective solution for real-world 3DIS. To evaluate CLIMB-3D, we designed three benchmark scenarios for CI-3DIS based on
the ScanNet200 dataset [57]. These scenarios simulate incremental learning setup under nat-
ural class imbalance, where new classes emerge based on (A) object frequency, (B) semantic
similarity, or (C) random grouping. Additionally, for comparison with existing methods,
we evaluate our approach on CI-3DSS using the ScanNetV2 dataset [16]. Experimental re-
sults show that our method significantly reduces forgetting and improves performance across
both frequent and rare categories, outperforming previous methods in both settings. Fig. 1
illustrates the CI-3DIS setup. THENGANE ET AL.: CLIMB-3D3
In summary, our contributions are: (i) a novel problem setting of imbalanced class-
incremental 3DIS with an effective method to balance learning and mitigate forgetting; (ii)
three benchmarks modelling continual object emergence with natural imbalance; and (iii)
strong experimental results, achieving up to 16.76% mAP improvement over baselines. 2 Related Work
3D Instance Segmentation.Various methods have been proposed for 3DIS. Grouping-
based approaches adopt a bottom-up pipeline that learns latent embeddings for point cluster-
ing [9, 21, 22, 29, 37, 44, 70, 78]. In contrast, proposal-based methods follow a top-down
strategy by detecting 3D boxes and segmenting objects within them [19, 26, 46, 73, 76]. Recently, transformer-based models [68] have also been applied to 3DIS and 3DSS [59, 64],
inspired by advances in 2D vision [10, 11]. However, these approaches require full anno-
tations for all classes and are not designed for progressive learning, where only new class
annotations are available and previous data is inaccessible. Another line of work aims to re-
duce annotation costs by proposing weakly supervised 3DIS methods based on sparse cues
[12, 27, 72]. While effective with limited annotations, these methods assume a fixed set of
classes and are susceptible to catastrophic forgetting in incremental settings. Incremental Learning.Continual learning involves training models sequentially to miti-
gate catastrophic forgetting. One common strategy is model regularisation, which constrains
parameter updates using techniques such as elastic weight consolidation or knowledge dis-
tillation [1, 24, 35, 43, 60]. Another approach is exemplar replay, where past data is stored
or generated to rehearse older tasks while learning new ones [5, 7, 30, 54]. A third direc-
tion involves dynamically expanding the model architecture or using modular sub-networks
to accommodate new tasks without interfering with old ones [31, 42, 53, 58, 71, 79]. Re-
cent efforts in class-incremental 3D segmentation [63, 74] have shown early promise but
often rely on basic architectures and focus primarily on semantic segmentation. Other works
[4, 36] explore continual learning in 3D settings, though they often depend on large memory
buffers. In contrast, our work introduces a tailored framework for 3D instance segmentation
that effectively transfers knowledge across tasks, even under class imbalance. Long-tailed Recognition.Imbalanced class distributions lead to poor recognition of rare
(long-tail) classes. To address this, re-sampling methods balance the data via input [8, 20, 50,
52, 61] or feature space [3, 14, 40], avoiding naÃ¯ve under-/over-sampling. Loss re-weighting
offers another direction, using class-based [6, 15, 23, 32, 33, 69] or per-example [45, 55, 62]
adjustments to ensure fair contribution. Parameter regularisation improves generalisation
through weight constraints [2], albeit requiring careful tuning. Other approaches leverage
transfer learning [77, 81], self-supervision [41, 75], or contrastive learning [34, 41, 82] to
improve rare-class representations. Long-tailed recognition is well-studied in 2D with large-
scale datasets [15, 49, 67]; in 3D, ScanNet200 [16, 57] enables similar efforts. Prior 3D
work focused on re-weighting, re-sampling, and transfer learning [57], while regularisation
remains underexplored. CeCo [80] balances class centres via auxiliary loss but overlooks
sampling and augmentation. Lahoud et al. [38] propose adaptive classifier regularisation
for 3D segmentation, outperforming prior approaches without threshold tuning. However,
neither method considers incremental learning. In contrast, we jointly address long-tail and
continual 3D semantic segmentation. 4THENGANE ET AL.: CLIMB-3D
Feature 
Backbone
Transformer 
Decoder
Feature 
Backbone
Transformer 
Decoder
(ð‘·,ð’€ð‘¡)
Current model (ð›·
ð‘¡
)
Pseudo Label Generator (PLG) 
Selection
à·¨ð‘Œð¾
ð‘¡
â„’CIâˆ’3DIS
à·¨ð‘Œð¾
ð‘¡,ð‘¤
à´¤ð‘Œð¾
ð‘¡
à· ð‘Œð‘¡
(ð·ð‘¡)
Current Dataset 
Exemplar 
Replay (ER)
+
~
Class-Balanced  
Re-weighting 
(CBR)
1
ð‘“(ð‘)
Figure 2: Overview ofCLIMB-3Dfor CI-3DIS. The model incrementally learns new classes
across sequential phases. During taskt, point cloudsPand their corresponding labelsY t are
sampled from a combination of the current training datasetD t and Exemplar Replay (ER),
which maintains a small memory of past examples. These are then passed to the current
modelÎ¦ t to produce predicted labelsY t . The Pseudo-Label Generator (PLG) selects the
top-Kpredictions from the previous modelÎ¦ tâˆ’1 . These pseudo-labels are then weighted
based on class frequencyf(c)using Class-Balanced Re-weighting (CBR), and the top-Kre-
weighted labels are selected to form the balanced pseudo-label set Â¯Yt . This pseudo-label set
is then concatenated with the ground-truth labels to form a final augmented supervision set
Yt for taskt, which is used to optimise the modelÎ¦ t using Eq. (3). 3 Methodology
3.1 Problem Formulation
3DIS.The objective of this task is to detect and segment each object instance within a point
cloud. Formally, the training dataset is denoted asD={(P i,Yi)}N
i=1, whereNis the number
of training samples. Each inputP i âˆˆR MÃ—6 is a coloured point cloud consisting ofMpoints,
where each point is represented by its 3D coordinates and RGB values. The corresponding
annotationY i ={(m i,j ,ci,j )}Ji
j=1 containsJ i object instances, wheremi,j âˆˆ {0,1}M is a binary
mask indicating which points belong to thej-th instance, andc i,j âˆˆ C={1,...,C}is the
semantic class label of that instance. GivenP i, the modelÎ¦predicts instance-level outputs
Ë†Yi ={(Ë†mi,j ,Ë†ci,j )}
Ë†Ji
j=1, where Ë†mi,j and Ë†ci,j denote the predicted mask and category label for
thej-th instance, respectively. The number of predicted instances Ë†Ji varies depending on the
modelâ€™s inference. The model is optimised using the following objective:
L3DIS(D;Î¦) = 1
|D| âˆ‘
(P,Y)âˆˆD
1
|Y| âˆ‘
(mj,cj)âˆˆY
(Lmask(mj,Ë†mj)+Î»Â·L cls(cj,Ë†cj)),(1)
whereL cls andL mask are the average classification and mask losses over all instances, andÎ»
controls their trade-off. CI-3DIS.Unlike conventional 3DIS, CI-3DIS involves sequential learning, where object
categories are introduced incrementally overTtraining tasks. Each tasktintroduces a dis-
joint set of classesC t , withC= ST
t=1 Ct andC t âˆ©C tâ€²
=/ 0 fortÌ¸=t â€². During each taskt, the THENGANE ET AL.: CLIMB-3D5
model receives a datasetD t ={(P i,Yt
i)}N
i=1, where each coloured point cloudP i âˆˆR MÃ—6
containsMpoints, and the corresponding annotationY t
i ={(m t
i,j ,ct
i,j )}
Jt
i
j=1 includes instance
masksm t
i,j âˆˆ {0,1}M and semantic class labelsc t
i,j âˆˆ Ct . The modelÎ¦ t , initialized from
Î¦tâˆ’1 , is trained onD t to predict instance-level outputs Ë†Yt
i ={(Ë†mt
i,j ,Ë†ct
i,j )}
Ë†Jt
i
j=1, where each
predicted semantic label Ë†ct
i,j belongs to the cumulative set of all classes observed so far:St
k=1 Ck. The key challenge in this setup is to learn new classes without forgetting those
encountered in previous tasks, despite limited supervision and the absence of past labels. 3.2 Method: CLIMB-3D
Overview.As shown in Fig. 2 and formulated in Sec. 3.1, the proposed framework for
CI-3DIS follows aphase-wisetraining strategy. In each phase, the model is exposed to a
carefully curated subset of the dataset, simulating real-world scenarios discussed in Sec. 3.3. Training naÃ¯vely on such phased data leads tocatastrophic forgetting[51], where the model
forgets prior knowledge when learning new tasks. To address this, we first incorporateEx-
emplar Replay(ER) [5], a strategy inspired by 2D incremental methods [1, 43, 54] and
recently extended to 3D settings [4], to mitigate forgetting by storing a small set of repre-
sentative samples from earlier phases. However, ER alone is insufficient to achieve robust
performance. To address this, two additional components are introduced: aPseudo-Label
Generator(PLG), which leverages a frozen model to generate supervision signals for previ-
ously seen classes, and aClass-Balanced Re-weighting(CBR) module that compensates for
class imbalance across phases. Each of these components is described in detail below 1. Exemplar Replay (ER).Inspired by Buzzega et al. [5], ER addresses the issue of catas-
trophic forgetting by allowing the model to retain a subset of data from earlier phases. During phaset, the model is trained not only on the current task dataD t , but also on a
set of exemplarsE 1:tâˆ’1, collected from previous phases. These exemplars are accumulated
incrementally:E 1:tâˆ’1 =E 1 âˆªÂ·Â·Â·âˆªE tâˆ’1 .Here,E t denotes the exemplar subset stored after
phaset, and|E t |denotes the exemplar replay size. The combined replay dataset is denoted
as:D t
ER =D t âˆªE 1:tâˆ’1.Epoch training consists of two stages: first, the model is trained on
the current data,D t ; then it is trained using exemplars fromE 1:tâˆ’1. While previous CI-3DIS approaches rely on large exemplar sets to mitigate forgetting [4],
such strategies become impractical under memory constraints. Consequently, although ER
aids knowledge retention, it is insufficient on its own for robust performance with a limited
exemplar budget. Therefore, PLG and CBR are introduced to better preserve past represen-
tations and enhance generalisation across phases. Pseudo-Label Generator (PLG).In the CI-3DIS setting, during phaset(wheret>1),
although ground-truth labels for previously seen classes are unavailable, the model from the
previous phaseÎ¦ tâˆ’1 , which preserves knowledge of past tasks, is retained. We use this
model to generate pseudo-labels for previously seen classes, thereby providing approximate
supervision during training. This allows the current modelÎ¦t , to retain prior knowledge and
reduce forgetting, even without access to ground-truth annotations. Given a point cloud and label pair(P,Y t ), fromD t , the previous modelÎ¦ tâˆ’1 , generates
pseudo-labels for the previously seen classes as Ë†Y1:tâˆ’1 =Î¦ tâˆ’1 (P), which we denote as ËœYt
1For clarity, modifications introduced by each component (ER, PLG, and CBR) are highlighted in green, blue,
and red, respectively. 6THENGANE ET AL.: CLIMB-3D
for brevity. However, we observe that utilising all predictions fromÎ¦tâˆ’1 introduces noisy or
incorrect labels, which degrade overall performance. To mitigate this, we select the top-K
most confident instance predictions, denoted as ËœYt
K, and concatenate them with the ground-
truth labels of the current task to obtain the supervision set: Â¯Yt =Y t âˆ¥ ËœYt
K. Following this,
the loss is computed over both the current taskâ€™s ground-truth labels and the top-Kconfi-
dent pseudo-labels from earlier phases, enabling the model to retain prior knowledge while
adapting to new classes. Class-Balanced Re-weighting (CBR).While ER and PLG contribute to preserving prior
knowledge, we observe that the modelâ€™s predictions are biased towards frequent classes,
resulting in the forgetting of rare categories. This issue is further amplified by the top-K
pseudo-labels selected by PLG, which predominantly represent dominant classes in the data. To address this, we propose aClass-Balanced Re-weighting(CBR) scheme that compensates
for class imbalance using object frequency statistics. At taskt, only the current datasetD t
and its label distributionp t (c)over classescâˆˆ Ct are available. Datasets and class distribu-
tions from earlier tasks{D i,p i(c)}tâˆ’1
i=1 are no longer accessible, which makes it challenging
to directly account for class imbalance across all previously seen tasks. Therefore, we pro-
pose to leverage the pseudo-label predictions of the frozen modelÎ¦ tâˆ’1 as a proxy for the
class distribution across previously learned categories. During each training iteration, the previous modelÎ¦ tâˆ’1 is applied to the current inputP
to generate pseudo-labels ËœYt , from which we accumulate class-wise frequency statistics for
previously learned classes. LetC1:t = St
i=1 Ci represent the union of all classes seen up to task
t. At the end of each epoch, we compute the overall class frequencyfâˆˆR |C1:t | by combining
the pseudo-label distribution Ëœpt (c)derived from ËœYt with the ground-truth label distribution
pt (c)from the current task. The resulting frequency vectorf= [f(c)] câˆˆC1:t is then used to
compute class-wise weightsw= [w(c)] câˆˆC1:t , which guide balanced pseudo-label selection
and help the model focus on rare classes. In the next epoch, the modelÎ¦ tâˆ’1 produces soft pseudo-label predictions ËœYt (as de-
scribed in PLG) , where each component ËœYt [c]denotes the confidence score for classcâˆˆ
C1:tâˆ’1. To promote balanced pseudo-label selection, class-wise re-weighting is applied using
w(c) = 1
f(c)+Îµ , whereÎµis a small constant for numerical stability. The adjusted predictions
are computed by applying the class-wise weights as: ËœYt,w [c] =w(c)Â· ËœYt [c],âˆ€câˆˆ C 1:tâˆ’1. Top-Kselection is then applied to both the original and the re-weighted scores, yielding
two pseudo-label sets: ËœYt
K and ËœYt,w
K , respectively. The final augmented target set, Yt , is
constructed by concatenating the ground-truth labelsY t with both sets of pseudo-labels:
Yt =Y t âˆ¥ ËœYt
K âˆ¥ ËœYt,w
K .(2)
Although the above re-weighting mitigates bias from prior tasks, it does not address class
imbalance within the current taskt. To resolve this, the class-balanced weightsw c are ex-
tended to incorporate statistics from both previously seen and current classes, i.e., overC 1:t
rather than onlyC 1:tâˆ’1. The updated weights, denoted asw â€²
c, are used to re-weights both
pseudo-label selection and classification loss, ensuring balanced supervision across all seen
categories, including rare ones. This unified strategy reduces bias and enhances performance
across phases. The pseudo-code for constructing the supervision set in Eq. (2), which incor-
porates both PLG and CBR components, is provided in Appendix A. Final Objective.The final training objective ofCLIMB-3D, incorporating ER, PLG, and THENGANE ET AL.: CLIMB-3D7
CBR, is formulated in Eq. (1) as:
L(Dt
ER;Î¦t ) = 1
|Dt
ER| âˆ‘
(P,Yt )âˆˆDt
ER
1
|Yt | âˆ‘
(mt
j,ct
j)âˆˆYt
 
Lmask(mt
j,Ë†mt
j)+w â€²â€²
c Â·Lcls(ct
j,Ë†ct
j)

,(3)
wherew â€²â€²
c =w â€²
c Â·Î» cls denotes the scaled weight vector, andÎ» cls is a hyperparameter for bal-
ancing the loss terms. 3.3 Benchmarking Incremental Scenarios
While CI-3DIS methods offer a wide range of practical applications, they frequently rely
on the assumption of uniform sample distribution, which rarely holds in real-world settings. In practice, the number of object categories, denoted byC, is often large and characterised
by substantial variability in category frequency, shape, structure, and size. To address these
challenges, we propose three incremental learning scenarios, each designed to capture a
different facet of real-world complexity. The design of these scenarios is detailed below; for
further information and illustrations, refer to Appendix E of the supplementary material. Split-A: Frequency Scenarios.This scenario acknowledges that datasets are often la-
belled based on the frequency of categories. To accommodate this, we propose a split where
the model learns from the most frequent categories and subsequently incorporates the less
frequent ones in later stages. By prioritising the training of frequently occurring categories,
the model can establish a strong foundation before expanding to handle rare categories. Split-B: Semantic Scenarios.Beyond frequency, semantic similarity is crucial in real-
world deployments. While objects often share visual or functional traits, models may en-
counter semantically different categories in new environments. To simulate this challenge,
we introduce theSplit-Bscenario. Here, categories are grouped based on their semantic
relationships, and the model is incrementally trained on one semantic group at a time. This
setup encourages the model to generalise across semantically similar categories and adapt
more effectively when exposed to new ones. Unlike theSplit-Ascenario, which organises
learning based on category frequency, theSplit-Bscenario may contain both frequent and
infrequent categories within a single task, focusing on semantic continuity and transfer. Split-C: Random Scenarios.In some cases, data labelling is driven by the availability of
objects rather than predefined criteria. To capture this, we introduce theSplit-Cscenario,
which represents a fully random setting where any class can appear in any task, resulting in
varying levels of class imbalance. By exposing the model to such diverse and unpredictable
distributions, we aim to improve its robustness in real-world situations where labelled data
availability is inconsistent. These incremental scenarios are designed to provide a more realistic representation of
object distributions, frequencies, and dynamics encountered in the real world. 4 Experiments
4.1 Setup
Datasets.We evaluateCLIMB-3Don ScanNet200 [57], which comprises 200 object cat-
egories and exhibits significant class imbalance, which makes it ideal for simulating and 8THENGANE ET AL.: CLIMB-3D
assessing real-world scenarios. In addition, we compare our approach against existing in-
cremental learning methods using ScanNetV2 [16] in the 3DSS setting. For this evaluation,
we follow the standard training and validation splits defined in prior works [74] to ensure
consistency with existing methods. Evaluation Metrics.We evaluate our method usingMean Average Precision(mAP), a
standard metric for 3DIS which provides a comprehensive measure of segmentation quality
by accounting for both precision and recall. For comparison with existing 3DSS methods,
we report themean Intersection over Union(mIoU), which quantifies the overlap between
predicted and ground truth segments. To assess the modelâ€™s ability to mitigate catastrophic
forgetting in incremental settings, we use theForgetting Percentage Points(FPP) metric, as
defined in [47]. FPP captures performance degradation by measuring the accuracy drop on
the initially seen categories between the first and final training phases. Detailed descriptions of the incremental scenarios (Split-A,Split-B,Split-C) and im-
plementation are provided in Appendix B. 4.2 Results and Discussion
Table 1: Comparison between the proposed method and the baseline in the 3DIS setting,
evaluated using mAP25, mAP50, mAP, and FPP after training across all phase. Scenarios Methods Average Precisionâ†‘FPPâ†“
mAP25 mAP50 mAP mAP 25 mAP50
Baseline 16.46 14.29 10.44 51.30 46.82Split-A CLIMB-3D(Ours) 35.69 31.05 22.72 3.44 2.63
Baseline 17.22 15.07 10.93 46.27 42.1Split-B CLIMB-3D(Ours) 35.48 31.56 23.69 8.00 5.51
Baseline 25.65 21.08 14.85 31.68 28.84Split-C CLIMB-3D(Ours) 31.59 26.78 18.93 9.10 7.89
To evaluate our proposed approach, we construct a baseline using ER [5] for the 3DIS
setting, as no existing incremental baselines are available for this task. In contrast, for the
3DSS setting, where incremental baselines do exist, we compare our method against three
prior approaches [43, 60, 74]. Results on CI-3DIS.Tab. 1 compares CLIMB-3D against the ER baseline across the three
CI-3DIS scenarios, evaluated after all phases using mAP 25, mAP50, overall mAP, and FPP. In theSplit-Ascenario, characterised by significant distribution shifts, our method achieves
gains of +19.23%, +16.76%, and +12.28% in mAP 25, mAP 50, and overall mAP, respec-
tively, while drastically reducing forgetting with FPP scores of 3.44% (mAP 25) and 2.63%
(mAP50), both over 45 points lower than the baseline. Under theSplit-Bscenario, with
semantically related new categories, improvements of +18.26%, +16.49%, and +12.76%
are observed alongside a reduction in forgetting to 8.00% and 5.51% from baseline levels
above 42%. For theSplit-Cscenario, featuring increasing geometric complexity, CLIMB-
3D maintains solid gains of +5.94%, +5.70%, and +4.08% across mAP metrics, with forget-
ting reduced by over 22 points. Overall, these results demonstrate robust forward transfer,
minimal forgetting, and stable learning, highlighting the methodâ€™s effectiveness for scalable
continual 3D semantic understanding. THENGANE ET AL.: CLIMB-3D9
Table 2: Comparison between the proposed method and existing baselines in the 3DSS set-
ting on ScanNetV2 [16], evaluated using mIoU. Methods Phase=1 Phase=2 All
EWC [60] 17.75 13.22 16.62
LwF [43] 30.38 13.37 26.13
Yanget al. [74] 34.16 13.43 28.98
CLIMB-3D(Ours) 69.39 32.56 59.38
Results on CI-3DSS.Although our primary focus is on CI-3DIS, we also evaluate our
method under the CI-3DSS setting to enable comparisons with existing approaches. To do
so, we adapt our predictions to assign each point the label corresponding to the highest-
confidence mask and exclude background classes (e.g., floor and wall), as these are not part
of the semantic segmentation targets. Tab. 2 presents a comparison between our method and
existing baselines on the ScanNet V2 dataset [16], evaluated using mIoU across two training
phases and overall. Although originally designed for instance segmentation, our method
generalises effectively to semantic segmentation, achieving substantial gains of +35.23%
mIoU in Phase 1 and +19.1% in Phase 2. Overall, it achieves 59.38% mIoU, significantly
higher than theâˆ¼30% mIoU of prior methods, highlighting its robustness and transferability. Refer to Appendix C, D, and F of the supplementary material for detailed analyses of per-
phase performance, rare-class evaluation, and qualitative result comparisons, respectively. 4.3 Ablation
Table 3: Ablation study illustrating the impact of each component in a three-phase setup. Each split (s) and corresponding number indicates data introduced at that phase. Best results
are highlighted inbold. Row Modules p=1â†‘p=2â†‘p=3â†‘ FPPâ†“
s1 s1 s2 All s1 s2 s3 All
1. Oracle - - - - 55.14 30.77 25.30 37.68 -
2. NaÃ¯ve 56.82 0.00 28.09 14.15 0.00 0.00 19.67 5.80 56.82
3. + ER 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28 46.44
4. + PLG 56.82 50.0034.3942.13 49.78 11.41 26.47 29.28 7.04
5. + CBR 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05 2.63
We conduct an ablation study to evaluate the contribution of each component in our
framework. As an upper bound, we report the performance of anOraclemodel, which is
trained jointly on the full dataset. For the incremental setting, we follow the previously
defined splits and first train the model naively across phases. We then incrementally add
each module to isolate its impact. Tab. 3 presents results for theSplit-Ascenario using both
mAP50 and FPP metrics. NaÃ¯ve Training.When trained naÃ¯vely without any dedicated modules, the model suffers
from severe catastrophic forgetting, as evident in row 2. The model entirely forgets pre-
viously learned classes upon entering new phases, with performance dropping to zero for
earlier splits and FPP rising to 56.82. 10THENGANE ET AL.: CLIMB-3D
Effect of ER.Adding exemplar replay (row 3) partially alleviates forgetting by maintaining
a buffer of past examples. This yields notable gains fors1in Phase 2 (+18.51%) and Phase 3
(+10.38%), and also improvess2in Phase 3 (+9.43%). However, the overall forgetting
remains substantial (FPP: 46.44), showing ER alone is insufficient. Effect of PLG.The addition of the pseudo-label generator (PLG), which generates labels for
previous classes by retaining a copy of the model from an earlier phase, facilitates knowledge
retention and forward transfer. As shown in row 4, PLG significantly reduces forgetting and
enhances performance on current tasks. Fors1, it improves mAP 50 by 31.49% in Phase 2
and 39.40% in Phase 3 over exemplar replay. Overall, PLG yields a 15.00% increase in
performance and reduces forgetting by 39.40%. Effect of CBR.Finally, class-balanced re-weighting (CBR) mitigates class imbalance dur-
ing both pseudo-labelling and current task learning by adjusting each classâ€™s contribution
based on its frequency. As shown in row 5, CBR enhancess1retention over PLG in Phases 2
and 3 (+4.67% and +4.41%), and further improves performance ons2ands3. It also achieves
the lowest FPP of 2.63, reflecting strong forgetting mitigation. Overall, CBR offers the best
trade-off between retaining past knowledge and acquiring new tasks, outperforming PLG
and ER by 4.41% and 43.81%, respectively. 4.4 Limitations & Future Work
While CLIMB-3D achieves strong performance across CI-3DIS and CI-3DSS settings, sev-
eral limitations remain and warrant discussion. The experiments are currently limited to
indoor datasets (ScanNet200 and ScanNetV2), therefore evaluating performance on outdoor
scenes would help assess generalisability across diverse environments. The current setup
also considers only three incremental phases, whereas real-world applications often involve
longer sequences; incorporating such extended setups would better reflect practical chal-
lenges. Moreover, the model operates in a uni-modal setting. Integrating multi-modal cues,
such as vision-language models, could further enhance performance, as suggested by recent
2D and 3D studies [25, 65, 66]. 5 Conclusion
This work addresses the challenge of catastrophic forgetting in class-incremental 3D in-
stance segmentation by introducing a modular framework that integrates exemplar replay
(ER), a pseudo-label generator (PLG), and class-balanced re-weighting (CBR). The pro-
posed method incrementally adapts to new classes while preserving prior knowledge, all
without requiring access to the full dataset. Extensive experiments on a three-phase bench-
mark validate the individual and combined effectiveness of each component: ER enables
efficient memory-based retention; PLG facilitates knowledge preservation through pseudo-
supervision; and CBR mitigates class imbalance to enhance learning stability. Together,
these components significantly reduce forgetting and improve segmentation performance
across all phases. By achieving a strong balance between stability and plasticity, our ap-
proach advances continual 3D scene understanding and sets a new baseline for future re-
search in this area. THENGANE ET AL.: CLIMB-3D11
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne
Tuytelaars. Memory aware synapses: Learning what (not) to forget. InECCV, pages
139â€“154, 2018. [2] Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long-tailed
recognition via weight balancing. InCVPR, pages 6897â€“6907, 2022. [3] Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying
imbalanced data. InJoint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 770â€“785.

[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021."
"What is the significance of the effective number of samples in the context of the paper by Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie?",Class-balanced loss based on effective number of samples.,"It may be distributed unchanged freely in print or electronic forms. 2THENGANE ET AL.: CLIMB-3D
1 Introduction
Table Chair Couch Pillow Coffee Table Sofa Chair Recycling BinCabinet Unknown
Task-1 Task-2 Task-3
Figure 1:Overview of the CI-3DIS setting.New object categories are introduced incre-
mentally with each task. After every phase, the model must recognise both newly added
and previously learned classes. For instance, in Task 2, categories such asPillow,Coffee
Table, andSofa Chairare introduced, while the model is expected to retain recognition
of earlier classes likeTable,Chair, andCouch. 3D instance segmentation (3DIS) is a fundamental task in computer vision that involves
identifying precise object boundaries and class labels in 3D space, with broad applications in
graphics, robotics, and augmented reality [4, 39]. Traditional 3DIS methods, including top-
down [28, 70, 78], bottom-up [26, 73], and transformer-based approaches [59], perform well
under the assumption that all classes are available and balanced during training. However,
this assumption does not hold in real-world environments, where new object classes emerge
over time and exhibit natural imbalance. This gap motivates the need for Class-Incremental Learning (CIL) [17, 54], which sup-
ports learning new categories while retaining knowledge of previous ones [51]. While CIL
has seen success in 2D image tasks [1, 43, 54, 60], extensions to 3D point clouds remain
limited, often focusing on object-level classification [13, 18, 48]. Recent works on scene-
level class-incremental 3DIS (CI-3DIS) [4] and class-incremental semantic segmentation
(CI-3DSS) [74] show promise, but depend heavily on large exemplar [56] and overlook class
imbalance, limiting their practicality. To address this, we proposeCLIMB-3D, a unified framework forCLass-incremental
IMBalance-aware3DIS that jointly tackles catastrophic forgetting and class imbalance. The
framework begins with Exemplar Replay (ER), storing a small number of samples from
past classes for later replay. However, this alone does not yield promising results under the
strict memory constraints required in CI-3DIS. To address this, we introduce a Pseudo-Label
Generator (PLG), which uses a frozen model from previous task to generate supervision for
earlier classes. However, we observed that PLG tends to favour frequent classes while ig-
noring under-represented ones. To mitigate this, we propose a Class-Balanced Re-weighting
(CBR), which estimates class frequencies from the pseudo-labels to enhance the learning of
rare classes without accessing past data. Together, these components form a practical and
effective solution for real-world 3DIS. To evaluate CLIMB-3D, we designed three benchmark scenarios for CI-3DIS based on
the ScanNet200 dataset [57]. These scenarios simulate incremental learning setup under nat-
ural class imbalance, where new classes emerge based on (A) object frequency, (B) semantic
similarity, or (C) random grouping. Additionally, for comparison with existing methods,
we evaluate our approach on CI-3DSS using the ScanNetV2 dataset [16]. Experimental re-
sults show that our method significantly reduces forgetting and improves performance across
both frequent and rare categories, outperforming previous methods in both settings. Fig. 1
illustrates the CI-3DIS setup. THENGANE ET AL.: CLIMB-3D3
In summary, our contributions are: (i) a novel problem setting of imbalanced class-
incremental 3DIS with an effective method to balance learning and mitigate forgetting; (ii)
three benchmarks modelling continual object emergence with natural imbalance; and (iii)
strong experimental results, achieving up to 16.76% mAP improvement over baselines. 2 Related Work
3D Instance Segmentation.Various methods have been proposed for 3DIS. Grouping-
based approaches adopt a bottom-up pipeline that learns latent embeddings for point cluster-
ing [9, 21, 22, 29, 37, 44, 70, 78]. In contrast, proposal-based methods follow a top-down
strategy by detecting 3D boxes and segmenting objects within them [19, 26, 46, 73, 76]. Recently, transformer-based models [68] have also been applied to 3DIS and 3DSS [59, 64],
inspired by advances in 2D vision [10, 11]. However, these approaches require full anno-
tations for all classes and are not designed for progressive learning, where only new class
annotations are available and previous data is inaccessible. Another line of work aims to re-
duce annotation costs by proposing weakly supervised 3DIS methods based on sparse cues
[12, 27, 72]. While effective with limited annotations, these methods assume a fixed set of
classes and are susceptible to catastrophic forgetting in incremental settings. Incremental Learning.Continual learning involves training models sequentially to miti-
gate catastrophic forgetting. One common strategy is model regularisation, which constrains
parameter updates using techniques such as elastic weight consolidation or knowledge dis-
tillation [1, 24, 35, 43, 60]. Another approach is exemplar replay, where past data is stored
or generated to rehearse older tasks while learning new ones [5, 7, 30, 54]. A third direc-
tion involves dynamically expanding the model architecture or using modular sub-networks
to accommodate new tasks without interfering with old ones [31, 42, 53, 58, 71, 79]. Re-
cent efforts in class-incremental 3D segmentation [63, 74] have shown early promise but
often rely on basic architectures and focus primarily on semantic segmentation. Other works
[4, 36] explore continual learning in 3D settings, though they often depend on large memory
buffers. In contrast, our work introduces a tailored framework for 3D instance segmentation
that effectively transfers knowledge across tasks, even under class imbalance. Long-tailed Recognition.Imbalanced class distributions lead to poor recognition of rare
(long-tail) classes. To address this, re-sampling methods balance the data via input [8, 20, 50,
52, 61] or feature space [3, 14, 40], avoiding naÃ¯ve under-/over-sampling. Loss re-weighting
offers another direction, using class-based [6, 15, 23, 32, 33, 69] or per-example [45, 55, 62]
adjustments to ensure fair contribution. Parameter regularisation improves generalisation
through weight constraints [2], albeit requiring careful tuning. Other approaches leverage
transfer learning [77, 81], self-supervision [41, 75], or contrastive learning [34, 41, 82] to
improve rare-class representations. Long-tailed recognition is well-studied in 2D with large-
scale datasets [15, 49, 67]; in 3D, ScanNet200 [16, 57] enables similar efforts. Prior 3D
work focused on re-weighting, re-sampling, and transfer learning [57], while regularisation
remains underexplored. CeCo [80] balances class centres via auxiliary loss but overlooks
sampling and augmentation. Lahoud et al. [38] propose adaptive classifier regularisation
for 3D segmentation, outperforming prior approaches without threshold tuning. However,
neither method considers incremental learning. In contrast, we jointly address long-tail and
continual 3D semantic segmentation. 4THENGANE ET AL.: CLIMB-3D
Feature 
Backbone
Transformer 
Decoder
Feature 
Backbone
Transformer 
Decoder
(ð‘·,ð’€ð‘¡)
Current model (ð›·
ð‘¡
)
Pseudo Label Generator (PLG) 
Selection
à·¨ð‘Œð¾
ð‘¡
â„’CIâˆ’3DIS
à·¨ð‘Œð¾
ð‘¡,ð‘¤
à´¤ð‘Œð¾
ð‘¡
à· ð‘Œð‘¡
(ð·ð‘¡)
Current Dataset 
Exemplar 
Replay (ER)
+
~
Class-Balanced  
Re-weighting 
(CBR)
1
ð‘“(ð‘)
Figure 2: Overview ofCLIMB-3Dfor CI-3DIS. The model incrementally learns new classes
across sequential phases. During taskt, point cloudsPand their corresponding labelsY t are
sampled from a combination of the current training datasetD t and Exemplar Replay (ER),
which maintains a small memory of past examples. These are then passed to the current
modelÎ¦ t to produce predicted labelsY t . The Pseudo-Label Generator (PLG) selects the
top-Kpredictions from the previous modelÎ¦ tâˆ’1 . These pseudo-labels are then weighted
based on class frequencyf(c)using Class-Balanced Re-weighting (CBR), and the top-Kre-
weighted labels are selected to form the balanced pseudo-label set Â¯Yt . This pseudo-label set
is then concatenated with the ground-truth labels to form a final augmented supervision set
Yt for taskt, which is used to optimise the modelÎ¦ t using Eq. (3). 3 Methodology
3.1 Problem Formulation
3DIS.The objective of this task is to detect and segment each object instance within a point
cloud. Formally, the training dataset is denoted asD={(P i,Yi)}N
i=1, whereNis the number
of training samples. Each inputP i âˆˆR MÃ—6 is a coloured point cloud consisting ofMpoints,
where each point is represented by its 3D coordinates and RGB values. The corresponding
annotationY i ={(m i,j ,ci,j )}Ji
j=1 containsJ i object instances, wheremi,j âˆˆ {0,1}M is a binary
mask indicating which points belong to thej-th instance, andc i,j âˆˆ C={1,...,C}is the
semantic class label of that instance. GivenP i, the modelÎ¦predicts instance-level outputs
Ë†Yi ={(Ë†mi,j ,Ë†ci,j )}
Ë†Ji
j=1, where Ë†mi,j and Ë†ci,j denote the predicted mask and category label for
thej-th instance, respectively. The number of predicted instances Ë†Ji varies depending on the
modelâ€™s inference. The model is optimised using the following objective:
L3DIS(D;Î¦) = 1
|D| âˆ‘
(P,Y)âˆˆD
1
|Y| âˆ‘
(mj,cj)âˆˆY
(Lmask(mj,Ë†mj)+Î»Â·L cls(cj,Ë†cj)),(1)
whereL cls andL mask are the average classification and mask losses over all instances, andÎ»
controls their trade-off. CI-3DIS.Unlike conventional 3DIS, CI-3DIS involves sequential learning, where object
categories are introduced incrementally overTtraining tasks. Each tasktintroduces a dis-
joint set of classesC t , withC= ST
t=1 Ct andC t âˆ©C tâ€²
=/ 0 fortÌ¸=t â€². During each taskt, the THENGANE ET AL.: CLIMB-3D5
model receives a datasetD t ={(P i,Yt
i)}N
i=1, where each coloured point cloudP i âˆˆR MÃ—6
containsMpoints, and the corresponding annotationY t
i ={(m t
i,j ,ct
i,j )}
Jt
i
j=1 includes instance
masksm t
i,j âˆˆ {0,1}M and semantic class labelsc t
i,j âˆˆ Ct . The modelÎ¦ t , initialized from
Î¦tâˆ’1 , is trained onD t to predict instance-level outputs Ë†Yt
i ={(Ë†mt
i,j ,Ë†ct
i,j )}
Ë†Jt
i
j=1, where each
predicted semantic label Ë†ct
i,j belongs to the cumulative set of all classes observed so far:St
k=1 Ck. The key challenge in this setup is to learn new classes without forgetting those
encountered in previous tasks, despite limited supervision and the absence of past labels. 3.2 Method: CLIMB-3D
Overview.As shown in Fig. 2 and formulated in Sec. 3.1, the proposed framework for
CI-3DIS follows aphase-wisetraining strategy. In each phase, the model is exposed to a
carefully curated subset of the dataset, simulating real-world scenarios discussed in Sec. 3.3. Training naÃ¯vely on such phased data leads tocatastrophic forgetting[51], where the model
forgets prior knowledge when learning new tasks. To address this, we first incorporateEx-
emplar Replay(ER) [5], a strategy inspired by 2D incremental methods [1, 43, 54] and
recently extended to 3D settings [4], to mitigate forgetting by storing a small set of repre-
sentative samples from earlier phases. However, ER alone is insufficient to achieve robust
performance. To address this, two additional components are introduced: aPseudo-Label
Generator(PLG), which leverages a frozen model to generate supervision signals for previ-
ously seen classes, and aClass-Balanced Re-weighting(CBR) module that compensates for
class imbalance across phases. Each of these components is described in detail below 1. Exemplar Replay (ER).Inspired by Buzzega et al. [5], ER addresses the issue of catas-
trophic forgetting by allowing the model to retain a subset of data from earlier phases. During phaset, the model is trained not only on the current task dataD t , but also on a
set of exemplarsE 1:tâˆ’1, collected from previous phases. These exemplars are accumulated
incrementally:E 1:tâˆ’1 =E 1 âˆªÂ·Â·Â·âˆªE tâˆ’1 .Here,E t denotes the exemplar subset stored after
phaset, and|E t |denotes the exemplar replay size. The combined replay dataset is denoted
as:D t
ER =D t âˆªE 1:tâˆ’1.Epoch training consists of two stages: first, the model is trained on
the current data,D t ; then it is trained using exemplars fromE 1:tâˆ’1. While previous CI-3DIS approaches rely on large exemplar sets to mitigate forgetting [4],
such strategies become impractical under memory constraints. Consequently, although ER
aids knowledge retention, it is insufficient on its own for robust performance with a limited
exemplar budget. Therefore, PLG and CBR are introduced to better preserve past represen-
tations and enhance generalisation across phases. Pseudo-Label Generator (PLG).In the CI-3DIS setting, during phaset(wheret>1),
although ground-truth labels for previously seen classes are unavailable, the model from the
previous phaseÎ¦ tâˆ’1 , which preserves knowledge of past tasks, is retained. We use this
model to generate pseudo-labels for previously seen classes, thereby providing approximate
supervision during training. This allows the current modelÎ¦t , to retain prior knowledge and
reduce forgetting, even without access to ground-truth annotations. Given a point cloud and label pair(P,Y t ), fromD t , the previous modelÎ¦ tâˆ’1 , generates
pseudo-labels for the previously seen classes as Ë†Y1:tâˆ’1 =Î¦ tâˆ’1 (P), which we denote as ËœYt
1For clarity, modifications introduced by each component (ER, PLG, and CBR) are highlighted in green, blue,
and red, respectively. 6THENGANE ET AL.: CLIMB-3D
for brevity. However, we observe that utilising all predictions fromÎ¦tâˆ’1 introduces noisy or
incorrect labels, which degrade overall performance. To mitigate this, we select the top-K
most confident instance predictions, denoted as ËœYt
K, and concatenate them with the ground-
truth labels of the current task to obtain the supervision set: Â¯Yt =Y t âˆ¥ ËœYt
K. Following this,
the loss is computed over both the current taskâ€™s ground-truth labels and the top-Kconfi-
dent pseudo-labels from earlier phases, enabling the model to retain prior knowledge while
adapting to new classes. Class-Balanced Re-weighting (CBR).While ER and PLG contribute to preserving prior
knowledge, we observe that the modelâ€™s predictions are biased towards frequent classes,
resulting in the forgetting of rare categories. This issue is further amplified by the top-K
pseudo-labels selected by PLG, which predominantly represent dominant classes in the data. To address this, we propose aClass-Balanced Re-weighting(CBR) scheme that compensates
for class imbalance using object frequency statistics. At taskt, only the current datasetD t
and its label distributionp t (c)over classescâˆˆ Ct are available. Datasets and class distribu-
tions from earlier tasks{D i,p i(c)}tâˆ’1
i=1 are no longer accessible, which makes it challenging
to directly account for class imbalance across all previously seen tasks. Therefore, we pro-
pose to leverage the pseudo-label predictions of the frozen modelÎ¦ tâˆ’1 as a proxy for the
class distribution across previously learned categories. During each training iteration, the previous modelÎ¦ tâˆ’1 is applied to the current inputP
to generate pseudo-labels ËœYt , from which we accumulate class-wise frequency statistics for
previously learned classes. LetC1:t = St
i=1 Ci represent the union of all classes seen up to task
t. At the end of each epoch, we compute the overall class frequencyfâˆˆR |C1:t | by combining
the pseudo-label distribution Ëœpt (c)derived from ËœYt with the ground-truth label distribution
pt (c)from the current task. The resulting frequency vectorf= [f(c)] câˆˆC1:t is then used to
compute class-wise weightsw= [w(c)] câˆˆC1:t , which guide balanced pseudo-label selection
and help the model focus on rare classes. In the next epoch, the modelÎ¦ tâˆ’1 produces soft pseudo-label predictions ËœYt (as de-
scribed in PLG) , where each component ËœYt [c]denotes the confidence score for classcâˆˆ
C1:tâˆ’1. To promote balanced pseudo-label selection, class-wise re-weighting is applied using
w(c) = 1
f(c)+Îµ , whereÎµis a small constant for numerical stability. The adjusted predictions
are computed by applying the class-wise weights as: ËœYt,w [c] =w(c)Â· ËœYt [c],âˆ€câˆˆ C 1:tâˆ’1. Top-Kselection is then applied to both the original and the re-weighted scores, yielding
two pseudo-label sets: ËœYt
K and ËœYt,w
K , respectively. The final augmented target set, Yt , is
constructed by concatenating the ground-truth labelsY t with both sets of pseudo-labels:
Yt =Y t âˆ¥ ËœYt
K âˆ¥ ËœYt,w
K .(2)
Although the above re-weighting mitigates bias from prior tasks, it does not address class
imbalance within the current taskt. To resolve this, the class-balanced weightsw c are ex-
tended to incorporate statistics from both previously seen and current classes, i.e., overC 1:t
rather than onlyC 1:tâˆ’1. The updated weights, denoted asw â€²
c, are used to re-weights both
pseudo-label selection and classification loss, ensuring balanced supervision across all seen
categories, including rare ones. This unified strategy reduces bias and enhances performance
across phases. The pseudo-code for constructing the supervision set in Eq. (2), which incor-
porates both PLG and CBR components, is provided in Appendix A. Final Objective.The final training objective ofCLIMB-3D, incorporating ER, PLG, and THENGANE ET AL.: CLIMB-3D7
CBR, is formulated in Eq. (1) as:
L(Dt
ER;Î¦t ) = 1
|Dt
ER| âˆ‘
(P,Yt )âˆˆDt
ER
1
|Yt | âˆ‘
(mt
j,ct
j)âˆˆYt
 
Lmask(mt
j,Ë†mt
j)+w â€²â€²
c Â·Lcls(ct
j,Ë†ct
j)

,(3)
wherew â€²â€²
c =w â€²
c Â·Î» cls denotes the scaled weight vector, andÎ» cls is a hyperparameter for bal-
ancing the loss terms. 3.3 Benchmarking Incremental Scenarios
While CI-3DIS methods offer a wide range of practical applications, they frequently rely
on the assumption of uniform sample distribution, which rarely holds in real-world settings. In practice, the number of object categories, denoted byC, is often large and characterised
by substantial variability in category frequency, shape, structure, and size. To address these
challenges, we propose three incremental learning scenarios, each designed to capture a
different facet of real-world complexity. The design of these scenarios is detailed below; for
further information and illustrations, refer to Appendix E of the supplementary material. Split-A: Frequency Scenarios.This scenario acknowledges that datasets are often la-
belled based on the frequency of categories. To accommodate this, we propose a split where
the model learns from the most frequent categories and subsequently incorporates the less
frequent ones in later stages. By prioritising the training of frequently occurring categories,
the model can establish a strong foundation before expanding to handle rare categories. Split-B: Semantic Scenarios.Beyond frequency, semantic similarity is crucial in real-
world deployments. While objects often share visual or functional traits, models may en-
counter semantically different categories in new environments. To simulate this challenge,
we introduce theSplit-Bscenario. Here, categories are grouped based on their semantic
relationships, and the model is incrementally trained on one semantic group at a time. This
setup encourages the model to generalise across semantically similar categories and adapt
more effectively when exposed to new ones. Unlike theSplit-Ascenario, which organises
learning based on category frequency, theSplit-Bscenario may contain both frequent and
infrequent categories within a single task, focusing on semantic continuity and transfer. Split-C: Random Scenarios.In some cases, data labelling is driven by the availability of
objects rather than predefined criteria. To capture this, we introduce theSplit-Cscenario,
which represents a fully random setting where any class can appear in any task, resulting in
varying levels of class imbalance. By exposing the model to such diverse and unpredictable
distributions, we aim to improve its robustness in real-world situations where labelled data
availability is inconsistent. These incremental scenarios are designed to provide a more realistic representation of
object distributions, frequencies, and dynamics encountered in the real world. 4 Experiments
4.1 Setup
Datasets.We evaluateCLIMB-3Don ScanNet200 [57], which comprises 200 object cat-
egories and exhibits significant class imbalance, which makes it ideal for simulating and 8THENGANE ET AL.: CLIMB-3D
assessing real-world scenarios. In addition, we compare our approach against existing in-
cremental learning methods using ScanNetV2 [16] in the 3DSS setting. For this evaluation,
we follow the standard training and validation splits defined in prior works [74] to ensure
consistency with existing methods. Evaluation Metrics.We evaluate our method usingMean Average Precision(mAP), a
standard metric for 3DIS which provides a comprehensive measure of segmentation quality
by accounting for both precision and recall. For comparison with existing 3DSS methods,
we report themean Intersection over Union(mIoU), which quantifies the overlap between
predicted and ground truth segments. To assess the modelâ€™s ability to mitigate catastrophic
forgetting in incremental settings, we use theForgetting Percentage Points(FPP) metric, as
defined in [47]. FPP captures performance degradation by measuring the accuracy drop on
the initially seen categories between the first and final training phases. Detailed descriptions of the incremental scenarios (Split-A,Split-B,Split-C) and im-
plementation are provided in Appendix B. 4.2 Results and Discussion
Table 1: Comparison between the proposed method and the baseline in the 3DIS setting,
evaluated using mAP25, mAP50, mAP, and FPP after training across all phase. Scenarios Methods Average Precisionâ†‘FPPâ†“
mAP25 mAP50 mAP mAP 25 mAP50
Baseline 16.46 14.29 10.44 51.30 46.82Split-A CLIMB-3D(Ours) 35.69 31.05 22.72 3.44 2.63
Baseline 17.22 15.07 10.93 46.27 42.1Split-B CLIMB-3D(Ours) 35.48 31.56 23.69 8.00 5.51
Baseline 25.65 21.08 14.85 31.68 28.84Split-C CLIMB-3D(Ours) 31.59 26.78 18.93 9.10 7.89
To evaluate our proposed approach, we construct a baseline using ER [5] for the 3DIS
setting, as no existing incremental baselines are available for this task. In contrast, for the
3DSS setting, where incremental baselines do exist, we compare our method against three
prior approaches [43, 60, 74]. Results on CI-3DIS.Tab. 1 compares CLIMB-3D against the ER baseline across the three
CI-3DIS scenarios, evaluated after all phases using mAP 25, mAP50, overall mAP, and FPP. In theSplit-Ascenario, characterised by significant distribution shifts, our method achieves
gains of +19.23%, +16.76%, and +12.28% in mAP 25, mAP 50, and overall mAP, respec-
tively, while drastically reducing forgetting with FPP scores of 3.44% (mAP 25) and 2.63%
(mAP50), both over 45 points lower than the baseline. Under theSplit-Bscenario, with
semantically related new categories, improvements of +18.26%, +16.49%, and +12.76%
are observed alongside a reduction in forgetting to 8.00% and 5.51% from baseline levels
above 42%. For theSplit-Cscenario, featuring increasing geometric complexity, CLIMB-
3D maintains solid gains of +5.94%, +5.70%, and +4.08% across mAP metrics, with forget-
ting reduced by over 22 points. Overall, these results demonstrate robust forward transfer,
minimal forgetting, and stable learning, highlighting the methodâ€™s effectiveness for scalable
continual 3D semantic understanding. THENGANE ET AL.: CLIMB-3D9
Table 2: Comparison between the proposed method and existing baselines in the 3DSS set-
ting on ScanNetV2 [16], evaluated using mIoU. Methods Phase=1 Phase=2 All
EWC [60] 17.75 13.22 16.62
LwF [43] 30.38 13.37 26.13
Yanget al. [74] 34.16 13.43 28.98
CLIMB-3D(Ours) 69.39 32.56 59.38
Results on CI-3DSS.Although our primary focus is on CI-3DIS, we also evaluate our
method under the CI-3DSS setting to enable comparisons with existing approaches. To do
so, we adapt our predictions to assign each point the label corresponding to the highest-
confidence mask and exclude background classes (e.g., floor and wall), as these are not part
of the semantic segmentation targets. Tab. 2 presents a comparison between our method and
existing baselines on the ScanNet V2 dataset [16], evaluated using mIoU across two training
phases and overall. Although originally designed for instance segmentation, our method
generalises effectively to semantic segmentation, achieving substantial gains of +35.23%
mIoU in Phase 1 and +19.1% in Phase 2. Overall, it achieves 59.38% mIoU, significantly
higher than theâˆ¼30% mIoU of prior methods, highlighting its robustness and transferability. Refer to Appendix C, D, and F of the supplementary material for detailed analyses of per-
phase performance, rare-class evaluation, and qualitative result comparisons, respectively. 4.3 Ablation
Table 3: Ablation study illustrating the impact of each component in a three-phase setup. Each split (s) and corresponding number indicates data introduced at that phase. Best results
are highlighted inbold. Row Modules p=1â†‘p=2â†‘p=3â†‘ FPPâ†“
s1 s1 s2 All s1 s2 s3 All
1. Oracle - - - - 55.14 30.77 25.30 37.68 -
2. NaÃ¯ve 56.82 0.00 28.09 14.15 0.00 0.00 19.67 5.80 56.82
3. + ER 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28 46.44
4. + PLG 56.82 50.0034.3942.13 49.78 11.41 26.47 29.28 7.04
5. + CBR 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05 2.63
We conduct an ablation study to evaluate the contribution of each component in our
framework. As an upper bound, we report the performance of anOraclemodel, which is
trained jointly on the full dataset. For the incremental setting, we follow the previously
defined splits and first train the model naively across phases. We then incrementally add
each module to isolate its impact. Tab. 3 presents results for theSplit-Ascenario using both
mAP50 and FPP metrics. NaÃ¯ve Training.When trained naÃ¯vely without any dedicated modules, the model suffers
from severe catastrophic forgetting, as evident in row 2. The model entirely forgets pre-
viously learned classes upon entering new phases, with performance dropping to zero for
earlier splits and FPP rising to 56.82. 10THENGANE ET AL.: CLIMB-3D
Effect of ER.Adding exemplar replay (row 3) partially alleviates forgetting by maintaining
a buffer of past examples. This yields notable gains fors1in Phase 2 (+18.51%) and Phase 3
(+10.38%), and also improvess2in Phase 3 (+9.43%). However, the overall forgetting
remains substantial (FPP: 46.44), showing ER alone is insufficient. Effect of PLG.The addition of the pseudo-label generator (PLG), which generates labels for
previous classes by retaining a copy of the model from an earlier phase, facilitates knowledge
retention and forward transfer. As shown in row 4, PLG significantly reduces forgetting and
enhances performance on current tasks. Fors1, it improves mAP 50 by 31.49% in Phase 2
and 39.40% in Phase 3 over exemplar replay. Overall, PLG yields a 15.00% increase in
performance and reduces forgetting by 39.40%. Effect of CBR.Finally, class-balanced re-weighting (CBR) mitigates class imbalance dur-
ing both pseudo-labelling and current task learning by adjusting each classâ€™s contribution
based on its frequency. As shown in row 5, CBR enhancess1retention over PLG in Phases 2
and 3 (+4.67% and +4.41%), and further improves performance ons2ands3. It also achieves
the lowest FPP of 2.63, reflecting strong forgetting mitigation. Overall, CBR offers the best
trade-off between retaining past knowledge and acquiring new tasks, outperforming PLG
and ER by 4.41% and 43.81%, respectively. 4.4 Limitations & Future Work
While CLIMB-3D achieves strong performance across CI-3DIS and CI-3DSS settings, sev-
eral limitations remain and warrant discussion. The experiments are currently limited to
indoor datasets (ScanNet200 and ScanNetV2), therefore evaluating performance on outdoor
scenes would help assess generalisability across diverse environments. The current setup
also considers only three incremental phases, whereas real-world applications often involve
longer sequences; incorporating such extended setups would better reflect practical chal-
lenges. Moreover, the model operates in a uni-modal setting. Integrating multi-modal cues,
such as vision-language models, could further enhance performance, as suggested by recent
2D and 3D studies [25, 65, 66]. 5 Conclusion
This work addresses the challenge of catastrophic forgetting in class-incremental 3D in-
stance segmentation by introducing a modular framework that integrates exemplar replay
(ER), a pseudo-label generator (PLG), and class-balanced re-weighting (CBR). The pro-
posed method incrementally adapts to new classes while preserving prior knowledge, all
without requiring access to the full dataset. Extensive experiments on a three-phase bench-
mark validate the individual and combined effectiveness of each component: ER enables
efficient memory-based retention; PLG facilitates knowledge preservation through pseudo-
supervision; and CBR mitigates class imbalance to enhance learning stability. Together,
these components significantly reduce forgetting and improve segmentation performance
across all phases. By achieving a strong balance between stability and plasticity, our ap-
proach advances continual 3D scene understanding and sets a new baseline for future re-
search in this area. THENGANE ET AL.: CLIMB-3D11
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne
Tuytelaars. Memory aware synapses: Learning what (not) to forget. InECCV, pages
139â€“154, 2018. [2] Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long-tailed
recognition via weight balancing. InCVPR, pages 6897â€“6907, 2022. [3] Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying
imbalanced data. InJoint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 770â€“785.

Class-balanced loss based on effective number of samples.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021."
"What is the relevance of the concept of class balance in the paper by Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie?",Class-balanced loss based on effective number of samples.,"[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

Class-balanced loss based on effective number of samples.

It may be distributed unchanged freely in print or electronic forms. 2THENGANE ET AL.: CLIMB-3D
1 Introduction
Table Chair Couch Pillow Coffee Table Sofa Chair Recycling BinCabinet Unknown
Task-1 Task-2 Task-3
Figure 1:Overview of the CI-3DIS setting.New object categories are introduced incre-
mentally with each task. After every phase, the model must recognise both newly added
and previously learned classes. For instance, in Task 2, categories such asPillow,Coffee
Table, andSofa Chairare introduced, while the model is expected to retain recognition
of earlier classes likeTable,Chair, andCouch. 3D instance segmentation (3DIS) is a fundamental task in computer vision that involves
identifying precise object boundaries and class labels in 3D space, with broad applications in
graphics, robotics, and augmented reality [4, 39]. Traditional 3DIS methods, including top-
down [28, 70, 78], bottom-up [26, 73], and transformer-based approaches [59], perform well
under the assumption that all classes are available and balanced during training. However,
this assumption does not hold in real-world environments, where new object classes emerge
over time and exhibit natural imbalance. This gap motivates the need for Class-Incremental Learning (CIL) [17, 54], which sup-
ports learning new categories while retaining knowledge of previous ones [51]. While CIL
has seen success in 2D image tasks [1, 43, 54, 60], extensions to 3D point clouds remain
limited, often focusing on object-level classification [13, 18, 48]. Recent works on scene-
level class-incremental 3DIS (CI-3DIS) [4] and class-incremental semantic segmentation
(CI-3DSS) [74] show promise, but depend heavily on large exemplar [56] and overlook class
imbalance, limiting their practicality. To address this, we proposeCLIMB-3D, a unified framework forCLass-incremental
IMBalance-aware3DIS that jointly tackles catastrophic forgetting and class imbalance. The
framework begins with Exemplar Replay (ER), storing a small number of samples from
past classes for later replay. However, this alone does not yield promising results under the
strict memory constraints required in CI-3DIS. To address this, we introduce a Pseudo-Label
Generator (PLG), which uses a frozen model from previous task to generate supervision for
earlier classes. However, we observed that PLG tends to favour frequent classes while ig-
noring under-represented ones. To mitigate this, we propose a Class-Balanced Re-weighting
(CBR), which estimates class frequencies from the pseudo-labels to enhance the learning of
rare classes without accessing past data. Together, these components form a practical and
effective solution for real-world 3DIS. To evaluate CLIMB-3D, we designed three benchmark scenarios for CI-3DIS based on
the ScanNet200 dataset [57]. These scenarios simulate incremental learning setup under nat-
ural class imbalance, where new classes emerge based on (A) object frequency, (B) semantic
similarity, or (C) random grouping. Additionally, for comparison with existing methods,
we evaluate our approach on CI-3DSS using the ScanNetV2 dataset [16]. Experimental re-
sults show that our method significantly reduces forgetting and improves performance across
both frequent and rare categories, outperforming previous methods in both settings. Fig. 1
illustrates the CI-3DIS setup. THENGANE ET AL.: CLIMB-3D3
In summary, our contributions are: (i) a novel problem setting of imbalanced class-
incremental 3DIS with an effective method to balance learning and mitigate forgetting; (ii)
three benchmarks modelling continual object emergence with natural imbalance; and (iii)
strong experimental results, achieving up to 16.76% mAP improvement over baselines. 2 Related Work
3D Instance Segmentation.Various methods have been proposed for 3DIS. Grouping-
based approaches adopt a bottom-up pipeline that learns latent embeddings for point cluster-
ing [9, 21, 22, 29, 37, 44, 70, 78]. In contrast, proposal-based methods follow a top-down
strategy by detecting 3D boxes and segmenting objects within them [19, 26, 46, 73, 76]. Recently, transformer-based models [68] have also been applied to 3DIS and 3DSS [59, 64],
inspired by advances in 2D vision [10, 11]. However, these approaches require full anno-
tations for all classes and are not designed for progressive learning, where only new class
annotations are available and previous data is inaccessible. Another line of work aims to re-
duce annotation costs by proposing weakly supervised 3DIS methods based on sparse cues
[12, 27, 72]. While effective with limited annotations, these methods assume a fixed set of
classes and are susceptible to catastrophic forgetting in incremental settings. Incremental Learning.Continual learning involves training models sequentially to miti-
gate catastrophic forgetting. One common strategy is model regularisation, which constrains
parameter updates using techniques such as elastic weight consolidation or knowledge dis-
tillation [1, 24, 35, 43, 60]. Another approach is exemplar replay, where past data is stored
or generated to rehearse older tasks while learning new ones [5, 7, 30, 54]. A third direc-
tion involves dynamically expanding the model architecture or using modular sub-networks
to accommodate new tasks without interfering with old ones [31, 42, 53, 58, 71, 79]. Re-
cent efforts in class-incremental 3D segmentation [63, 74] have shown early promise but
often rely on basic architectures and focus primarily on semantic segmentation. Other works
[4, 36] explore continual learning in 3D settings, though they often depend on large memory
buffers. In contrast, our work introduces a tailored framework for 3D instance segmentation
that effectively transfers knowledge across tasks, even under class imbalance. Long-tailed Recognition.Imbalanced class distributions lead to poor recognition of rare
(long-tail) classes. To address this, re-sampling methods balance the data via input [8, 20, 50,
52, 61] or feature space [3, 14, 40], avoiding naÃ¯ve under-/over-sampling. Loss re-weighting
offers another direction, using class-based [6, 15, 23, 32, 33, 69] or per-example [45, 55, 62]
adjustments to ensure fair contribution. Parameter regularisation improves generalisation
through weight constraints [2], albeit requiring careful tuning. Other approaches leverage
transfer learning [77, 81], self-supervision [41, 75], or contrastive learning [34, 41, 82] to
improve rare-class representations. Long-tailed recognition is well-studied in 2D with large-
scale datasets [15, 49, 67]; in 3D, ScanNet200 [16, 57] enables similar efforts. Prior 3D
work focused on re-weighting, re-sampling, and transfer learning [57], while regularisation
remains underexplored. CeCo [80] balances class centres via auxiliary loss but overlooks
sampling and augmentation. Lahoud et al. [38] propose adaptive classifier regularisation
for 3D segmentation, outperforming prior approaches without threshold tuning. However,
neither method considers incremental learning. In contrast, we jointly address long-tail and
continual 3D semantic segmentation. 4THENGANE ET AL.: CLIMB-3D
Feature 
Backbone
Transformer 
Decoder
Feature 
Backbone
Transformer 
Decoder
(ð‘·,ð’€ð‘¡)
Current model (ð›·
ð‘¡
)
Pseudo Label Generator (PLG) 
Selection
à·¨ð‘Œð¾
ð‘¡
â„’CIâˆ’3DIS
à·¨ð‘Œð¾
ð‘¡,ð‘¤
à´¤ð‘Œð¾
ð‘¡
à· ð‘Œð‘¡
(ð·ð‘¡)
Current Dataset 
Exemplar 
Replay (ER)
+
~
Class-Balanced  
Re-weighting 
(CBR)
1
ð‘“(ð‘)
Figure 2: Overview ofCLIMB-3Dfor CI-3DIS. The model incrementally learns new classes
across sequential phases. During taskt, point cloudsPand their corresponding labelsY t are
sampled from a combination of the current training datasetD t and Exemplar Replay (ER),
which maintains a small memory of past examples. These are then passed to the current
modelÎ¦ t to produce predicted labelsY t . The Pseudo-Label Generator (PLG) selects the
top-Kpredictions from the previous modelÎ¦ tâˆ’1 . These pseudo-labels are then weighted
based on class frequencyf(c)using Class-Balanced Re-weighting (CBR), and the top-Kre-
weighted labels are selected to form the balanced pseudo-label set Â¯Yt . This pseudo-label set
is then concatenated with the ground-truth labels to form a final augmented supervision set
Yt for taskt, which is used to optimise the modelÎ¦ t using Eq. (3). 3 Methodology
3.1 Problem Formulation
3DIS.The objective of this task is to detect and segment each object instance within a point
cloud. Formally, the training dataset is denoted asD={(P i,Yi)}N
i=1, whereNis the number
of training samples. Each inputP i âˆˆR MÃ—6 is a coloured point cloud consisting ofMpoints,
where each point is represented by its 3D coordinates and RGB values. The corresponding
annotationY i ={(m i,j ,ci,j )}Ji
j=1 containsJ i object instances, wheremi,j âˆˆ {0,1}M is a binary
mask indicating which points belong to thej-th instance, andc i,j âˆˆ C={1,...,C}is the
semantic class label of that instance. GivenP i, the modelÎ¦predicts instance-level outputs
Ë†Yi ={(Ë†mi,j ,Ë†ci,j )}
Ë†Ji
j=1, where Ë†mi,j and Ë†ci,j denote the predicted mask and category label for
thej-th instance, respectively. The number of predicted instances Ë†Ji varies depending on the
modelâ€™s inference. The model is optimised using the following objective:
L3DIS(D;Î¦) = 1
|D| âˆ‘
(P,Y)âˆˆD
1
|Y| âˆ‘
(mj,cj)âˆˆY
(Lmask(mj,Ë†mj)+Î»Â·L cls(cj,Ë†cj)),(1)
whereL cls andL mask are the average classification and mask losses over all instances, andÎ»
controls their trade-off. CI-3DIS.Unlike conventional 3DIS, CI-3DIS involves sequential learning, where object
categories are introduced incrementally overTtraining tasks. Each tasktintroduces a dis-
joint set of classesC t , withC= ST
t=1 Ct andC t âˆ©C tâ€²
=/ 0 fortÌ¸=t â€². During each taskt, the THENGANE ET AL.: CLIMB-3D5
model receives a datasetD t ={(P i,Yt
i)}N
i=1, where each coloured point cloudP i âˆˆR MÃ—6
containsMpoints, and the corresponding annotationY t
i ={(m t
i,j ,ct
i,j )}
Jt
i
j=1 includes instance
masksm t
i,j âˆˆ {0,1}M and semantic class labelsc t
i,j âˆˆ Ct . The modelÎ¦ t , initialized from
Î¦tâˆ’1 , is trained onD t to predict instance-level outputs Ë†Yt
i ={(Ë†mt
i,j ,Ë†ct
i,j )}
Ë†Jt
i
j=1, where each
predicted semantic label Ë†ct
i,j belongs to the cumulative set of all classes observed so far:St
k=1 Ck. The key challenge in this setup is to learn new classes without forgetting those
encountered in previous tasks, despite limited supervision and the absence of past labels. 3.2 Method: CLIMB-3D
Overview.As shown in Fig. 2 and formulated in Sec. 3.1, the proposed framework for
CI-3DIS follows aphase-wisetraining strategy. In each phase, the model is exposed to a
carefully curated subset of the dataset, simulating real-world scenarios discussed in Sec. 3.3. Training naÃ¯vely on such phased data leads tocatastrophic forgetting[51], where the model
forgets prior knowledge when learning new tasks. To address this, we first incorporateEx-
emplar Replay(ER) [5], a strategy inspired by 2D incremental methods [1, 43, 54] and
recently extended to 3D settings [4], to mitigate forgetting by storing a small set of repre-
sentative samples from earlier phases. However, ER alone is insufficient to achieve robust
performance. To address this, two additional components are introduced: aPseudo-Label
Generator(PLG), which leverages a frozen model to generate supervision signals for previ-
ously seen classes, and aClass-Balanced Re-weighting(CBR) module that compensates for
class imbalance across phases. Each of these components is described in detail below 1. Exemplar Replay (ER).Inspired by Buzzega et al. [5], ER addresses the issue of catas-
trophic forgetting by allowing the model to retain a subset of data from earlier phases. During phaset, the model is trained not only on the current task dataD t , but also on a
set of exemplarsE 1:tâˆ’1, collected from previous phases. These exemplars are accumulated
incrementally:E 1:tâˆ’1 =E 1 âˆªÂ·Â·Â·âˆªE tâˆ’1 .Here,E t denotes the exemplar subset stored after
phaset, and|E t |denotes the exemplar replay size. The combined replay dataset is denoted
as:D t
ER =D t âˆªE 1:tâˆ’1.Epoch training consists of two stages: first, the model is trained on
the current data,D t ; then it is trained using exemplars fromE 1:tâˆ’1. While previous CI-3DIS approaches rely on large exemplar sets to mitigate forgetting [4],
such strategies become impractical under memory constraints. Consequently, although ER
aids knowledge retention, it is insufficient on its own for robust performance with a limited
exemplar budget. Therefore, PLG and CBR are introduced to better preserve past represen-
tations and enhance generalisation across phases. Pseudo-Label Generator (PLG).In the CI-3DIS setting, during phaset(wheret>1),
although ground-truth labels for previously seen classes are unavailable, the model from the
previous phaseÎ¦ tâˆ’1 , which preserves knowledge of past tasks, is retained. We use this
model to generate pseudo-labels for previously seen classes, thereby providing approximate
supervision during training. This allows the current modelÎ¦t , to retain prior knowledge and
reduce forgetting, even without access to ground-truth annotations. Given a point cloud and label pair(P,Y t ), fromD t , the previous modelÎ¦ tâˆ’1 , generates
pseudo-labels for the previously seen classes as Ë†Y1:tâˆ’1 =Î¦ tâˆ’1 (P), which we denote as ËœYt
1For clarity, modifications introduced by each component (ER, PLG, and CBR) are highlighted in green, blue,
and red, respectively. 6THENGANE ET AL.: CLIMB-3D
for brevity. However, we observe that utilising all predictions fromÎ¦tâˆ’1 introduces noisy or
incorrect labels, which degrade overall performance. To mitigate this, we select the top-K
most confident instance predictions, denoted as ËœYt
K, and concatenate them with the ground-
truth labels of the current task to obtain the supervision set: Â¯Yt =Y t âˆ¥ ËœYt
K. Following this,
the loss is computed over both the current taskâ€™s ground-truth labels and the top-Kconfi-
dent pseudo-labels from earlier phases, enabling the model to retain prior knowledge while
adapting to new classes. Class-Balanced Re-weighting (CBR).While ER and PLG contribute to preserving prior
knowledge, we observe that the modelâ€™s predictions are biased towards frequent classes,
resulting in the forgetting of rare categories. This issue is further amplified by the top-K
pseudo-labels selected by PLG, which predominantly represent dominant classes in the data. To address this, we propose aClass-Balanced Re-weighting(CBR) scheme that compensates
for class imbalance using object frequency statistics. At taskt, only the current datasetD t
and its label distributionp t (c)over classescâˆˆ Ct are available. Datasets and class distribu-
tions from earlier tasks{D i,p i(c)}tâˆ’1
i=1 are no longer accessible, which makes it challenging
to directly account for class imbalance across all previously seen tasks. Therefore, we pro-
pose to leverage the pseudo-label predictions of the frozen modelÎ¦ tâˆ’1 as a proxy for the
class distribution across previously learned categories. During each training iteration, the previous modelÎ¦ tâˆ’1 is applied to the current inputP
to generate pseudo-labels ËœYt , from which we accumulate class-wise frequency statistics for
previously learned classes. LetC1:t = St
i=1 Ci represent the union of all classes seen up to task
t. At the end of each epoch, we compute the overall class frequencyfâˆˆR |C1:t | by combining
the pseudo-label distribution Ëœpt (c)derived from ËœYt with the ground-truth label distribution
pt (c)from the current task. The resulting frequency vectorf= [f(c)] câˆˆC1:t is then used to
compute class-wise weightsw= [w(c)] câˆˆC1:t , which guide balanced pseudo-label selection
and help the model focus on rare classes. In the next epoch, the modelÎ¦ tâˆ’1 produces soft pseudo-label predictions ËœYt (as de-
scribed in PLG) , where each component ËœYt [c]denotes the confidence score for classcâˆˆ
C1:tâˆ’1. To promote balanced pseudo-label selection, class-wise re-weighting is applied using
w(c) = 1
f(c)+Îµ , whereÎµis a small constant for numerical stability. The adjusted predictions
are computed by applying the class-wise weights as: ËœYt,w [c] =w(c)Â· ËœYt [c],âˆ€câˆˆ C 1:tâˆ’1. Top-Kselection is then applied to both the original and the re-weighted scores, yielding
two pseudo-label sets: ËœYt
K and ËœYt,w
K , respectively. The final augmented target set, Yt , is
constructed by concatenating the ground-truth labelsY t with both sets of pseudo-labels:
Yt =Y t âˆ¥ ËœYt
K âˆ¥ ËœYt,w
K .(2)
Although the above re-weighting mitigates bias from prior tasks, it does not address class
imbalance within the current taskt. To resolve this, the class-balanced weightsw c are ex-
tended to incorporate statistics from both previously seen and current classes, i.e., overC 1:t
rather than onlyC 1:tâˆ’1. The updated weights, denoted asw â€²
c, are used to re-weights both
pseudo-label selection and classification loss, ensuring balanced supervision across all seen
categories, including rare ones. This unified strategy reduces bias and enhances performance
across phases. The pseudo-code for constructing the supervision set in Eq. (2), which incor-
porates both PLG and CBR components, is provided in Appendix A. Final Objective.The final training objective ofCLIMB-3D, incorporating ER, PLG, and THENGANE ET AL.: CLIMB-3D7
CBR, is formulated in Eq. (1) as:
L(Dt
ER;Î¦t ) = 1
|Dt
ER| âˆ‘
(P,Yt )âˆˆDt
ER
1
|Yt | âˆ‘
(mt
j,ct
j)âˆˆYt
 
Lmask(mt
j,Ë†mt
j)+w â€²â€²
c Â·Lcls(ct
j,Ë†ct
j)

,(3)
wherew â€²â€²
c =w â€²
c Â·Î» cls denotes the scaled weight vector, andÎ» cls is a hyperparameter for bal-
ancing the loss terms. 3.3 Benchmarking Incremental Scenarios
While CI-3DIS methods offer a wide range of practical applications, they frequently rely
on the assumption of uniform sample distribution, which rarely holds in real-world settings. In practice, the number of object categories, denoted byC, is often large and characterised
by substantial variability in category frequency, shape, structure, and size. To address these
challenges, we propose three incremental learning scenarios, each designed to capture a
different facet of real-world complexity. The design of these scenarios is detailed below; for
further information and illustrations, refer to Appendix E of the supplementary material. Split-A: Frequency Scenarios.This scenario acknowledges that datasets are often la-
belled based on the frequency of categories. To accommodate this, we propose a split where
the model learns from the most frequent categories and subsequently incorporates the less
frequent ones in later stages. By prioritising the training of frequently occurring categories,
the model can establish a strong foundation before expanding to handle rare categories. Split-B: Semantic Scenarios.Beyond frequency, semantic similarity is crucial in real-
world deployments. While objects often share visual or functional traits, models may en-
counter semantically different categories in new environments. To simulate this challenge,
we introduce theSplit-Bscenario. Here, categories are grouped based on their semantic
relationships, and the model is incrementally trained on one semantic group at a time. This
setup encourages the model to generalise across semantically similar categories and adapt
more effectively when exposed to new ones. Unlike theSplit-Ascenario, which organises
learning based on category frequency, theSplit-Bscenario may contain both frequent and
infrequent categories within a single task, focusing on semantic continuity and transfer. Split-C: Random Scenarios.In some cases, data labelling is driven by the availability of
objects rather than predefined criteria. To capture this, we introduce theSplit-Cscenario,
which represents a fully random setting where any class can appear in any task, resulting in
varying levels of class imbalance. By exposing the model to such diverse and unpredictable
distributions, we aim to improve its robustness in real-world situations where labelled data
availability is inconsistent. These incremental scenarios are designed to provide a more realistic representation of
object distributions, frequencies, and dynamics encountered in the real world. 4 Experiments
4.1 Setup
Datasets.We evaluateCLIMB-3Don ScanNet200 [57], which comprises 200 object cat-
egories and exhibits significant class imbalance, which makes it ideal for simulating and 8THENGANE ET AL.: CLIMB-3D
assessing real-world scenarios. In addition, we compare our approach against existing in-
cremental learning methods using ScanNetV2 [16] in the 3DSS setting. For this evaluation,
we follow the standard training and validation splits defined in prior works [74] to ensure
consistency with existing methods. Evaluation Metrics.We evaluate our method usingMean Average Precision(mAP), a
standard metric for 3DIS which provides a comprehensive measure of segmentation quality
by accounting for both precision and recall. For comparison with existing 3DSS methods,
we report themean Intersection over Union(mIoU), which quantifies the overlap between
predicted and ground truth segments. To assess the modelâ€™s ability to mitigate catastrophic
forgetting in incremental settings, we use theForgetting Percentage Points(FPP) metric, as
defined in [47]. FPP captures performance degradation by measuring the accuracy drop on
the initially seen categories between the first and final training phases. Detailed descriptions of the incremental scenarios (Split-A,Split-B,Split-C) and im-
plementation are provided in Appendix B. 4.2 Results and Discussion
Table 1: Comparison between the proposed method and the baseline in the 3DIS setting,
evaluated using mAP25, mAP50, mAP, and FPP after training across all phase. Scenarios Methods Average Precisionâ†‘FPPâ†“
mAP25 mAP50 mAP mAP 25 mAP50
Baseline 16.46 14.29 10.44 51.30 46.82Split-A CLIMB-3D(Ours) 35.69 31.05 22.72 3.44 2.63
Baseline 17.22 15.07 10.93 46.27 42.1Split-B CLIMB-3D(Ours) 35.48 31.56 23.69 8.00 5.51
Baseline 25.65 21.08 14.85 31.68 28.84Split-C CLIMB-3D(Ours) 31.59 26.78 18.93 9.10 7.89
To evaluate our proposed approach, we construct a baseline using ER [5] for the 3DIS
setting, as no existing incremental baselines are available for this task. In contrast, for the
3DSS setting, where incremental baselines do exist, we compare our method against three
prior approaches [43, 60, 74]. Results on CI-3DIS.Tab. 1 compares CLIMB-3D against the ER baseline across the three
CI-3DIS scenarios, evaluated after all phases using mAP 25, mAP50, overall mAP, and FPP. In theSplit-Ascenario, characterised by significant distribution shifts, our method achieves
gains of +19.23%, +16.76%, and +12.28% in mAP 25, mAP 50, and overall mAP, respec-
tively, while drastically reducing forgetting with FPP scores of 3.44% (mAP 25) and 2.63%
(mAP50), both over 45 points lower than the baseline. Under theSplit-Bscenario, with
semantically related new categories, improvements of +18.26%, +16.49%, and +12.76%
are observed alongside a reduction in forgetting to 8.00% and 5.51% from baseline levels
above 42%. For theSplit-Cscenario, featuring increasing geometric complexity, CLIMB-
3D maintains solid gains of +5.94%, +5.70%, and +4.08% across mAP metrics, with forget-
ting reduced by over 22 points. Overall, these results demonstrate robust forward transfer,
minimal forgetting, and stable learning, highlighting the methodâ€™s effectiveness for scalable
continual 3D semantic understanding. THENGANE ET AL.: CLIMB-3D9
Table 2: Comparison between the proposed method and existing baselines in the 3DSS set-
ting on ScanNetV2 [16], evaluated using mIoU. Methods Phase=1 Phase=2 All
EWC [60] 17.75 13.22 16.62
LwF [43] 30.38 13.37 26.13
Yanget al. [74] 34.16 13.43 28.98
CLIMB-3D(Ours) 69.39 32.56 59.38
Results on CI-3DSS.Although our primary focus is on CI-3DIS, we also evaluate our
method under the CI-3DSS setting to enable comparisons with existing approaches. To do
so, we adapt our predictions to assign each point the label corresponding to the highest-
confidence mask and exclude background classes (e.g., floor and wall), as these are not part
of the semantic segmentation targets. Tab. 2 presents a comparison between our method and
existing baselines on the ScanNet V2 dataset [16], evaluated using mIoU across two training
phases and overall. Although originally designed for instance segmentation, our method
generalises effectively to semantic segmentation, achieving substantial gains of +35.23%
mIoU in Phase 1 and +19.1% in Phase 2. Overall, it achieves 59.38% mIoU, significantly
higher than theâˆ¼30% mIoU of prior methods, highlighting its robustness and transferability. Refer to Appendix C, D, and F of the supplementary material for detailed analyses of per-
phase performance, rare-class evaluation, and qualitative result comparisons, respectively. 4.3 Ablation
Table 3: Ablation study illustrating the impact of each component in a three-phase setup. Each split (s) and corresponding number indicates data introduced at that phase. Best results
are highlighted inbold. Row Modules p=1â†‘p=2â†‘p=3â†‘ FPPâ†“
s1 s1 s2 All s1 s2 s3 All
1. Oracle - - - - 55.14 30.77 25.30 37.68 -
2. NaÃ¯ve 56.82 0.00 28.09 14.15 0.00 0.00 19.67 5.80 56.82
3. + ER 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28 46.44
4. + PLG 56.82 50.0034.3942.13 49.78 11.41 26.47 29.28 7.04
5. + CBR 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05 2.63
We conduct an ablation study to evaluate the contribution of each component in our
framework. As an upper bound, we report the performance of anOraclemodel, which is
trained jointly on the full dataset. For the incremental setting, we follow the previously
defined splits and first train the model naively across phases. We then incrementally add
each module to isolate its impact. Tab. 3 presents results for theSplit-Ascenario using both
mAP50 and FPP metrics. NaÃ¯ve Training.When trained naÃ¯vely without any dedicated modules, the model suffers
from severe catastrophic forgetting, as evident in row 2. The model entirely forgets pre-
viously learned classes upon entering new phases, with performance dropping to zero for
earlier splits and FPP rising to 56.82. 10THENGANE ET AL.: CLIMB-3D
Effect of ER.Adding exemplar replay (row 3) partially alleviates forgetting by maintaining
a buffer of past examples. This yields notable gains fors1in Phase 2 (+18.51%) and Phase 3
(+10.38%), and also improvess2in Phase 3 (+9.43%). However, the overall forgetting
remains substantial (FPP: 46.44), showing ER alone is insufficient. Effect of PLG.The addition of the pseudo-label generator (PLG), which generates labels for
previous classes by retaining a copy of the model from an earlier phase, facilitates knowledge
retention and forward transfer. As shown in row 4, PLG significantly reduces forgetting and
enhances performance on current tasks. Fors1, it improves mAP 50 by 31.49% in Phase 2
and 39.40% in Phase 3 over exemplar replay. Overall, PLG yields a 15.00% increase in
performance and reduces forgetting by 39.40%. Effect of CBR.Finally, class-balanced re-weighting (CBR) mitigates class imbalance dur-
ing both pseudo-labelling and current task learning by adjusting each classâ€™s contribution
based on its frequency. As shown in row 5, CBR enhancess1retention over PLG in Phases 2
and 3 (+4.67% and +4.41%), and further improves performance ons2ands3. It also achieves
the lowest FPP of 2.63, reflecting strong forgetting mitigation. Overall, CBR offers the best
trade-off between retaining past knowledge and acquiring new tasks, outperforming PLG
and ER by 4.41% and 43.81%, respectively. 4.4 Limitations & Future Work
While CLIMB-3D achieves strong performance across CI-3DIS and CI-3DSS settings, sev-
eral limitations remain and warrant discussion. The experiments are currently limited to
indoor datasets (ScanNet200 and ScanNetV2), therefore evaluating performance on outdoor
scenes would help assess generalisability across diverse environments. The current setup
also considers only three incremental phases, whereas real-world applications often involve
longer sequences; incorporating such extended setups would better reflect practical chal-
lenges. Moreover, the model operates in a uni-modal setting. Integrating multi-modal cues,
such as vision-language models, could further enhance performance, as suggested by recent
2D and 3D studies [25, 65, 66]. 5 Conclusion
This work addresses the challenge of catastrophic forgetting in class-incremental 3D in-
stance segmentation by introducing a modular framework that integrates exemplar replay
(ER), a pseudo-label generator (PLG), and class-balanced re-weighting (CBR). The pro-
posed method incrementally adapts to new classes while preserving prior knowledge, all
without requiring access to the full dataset. Extensive experiments on a three-phase bench-
mark validate the individual and combined effectiveness of each component: ER enables
efficient memory-based retention; PLG facilitates knowledge preservation through pseudo-
supervision; and CBR mitigates class imbalance to enhance learning stability. Together,
these components significantly reduce forgetting and improve segmentation performance
across all phases. By achieving a strong balance between stability and plasticity, our ap-
proach advances continual 3D scene understanding and sets a new baseline for future re-
search in this area. THENGANE ET AL.: CLIMB-3D11
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne
Tuytelaars. Memory aware synapses: Learning what (not) to forget. InECCV, pages
139â€“154, 2018. [2] Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long-tailed
recognition via weight balancing. InCVPR, pages 6897â€“6907, 2022. [3] Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying
imbalanced data. InJoint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 770â€“785."
Who are the authors of the paper Scannet: Richly-annotated 3d reconstructions of indoor scenes?,"Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias NieÃŸner","[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias NieÃŸner"
In which year was the paper Scannet: Richly-annotated 3d reconstructions of indoor scenes published?,"CVPR, 2017","[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

CVPR, 2017

[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017.

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591."
"What is the title of the paper by Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias NieÃŸner?",Scannet: Richly-annotated 3d reconstructions of indoor scenes,"5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks.

Scannet: Richly-annotated 3d reconstructions of indoor scenes

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017."
How many authors contributed to the paper Scannet: Richly-annotated 3d reconstructions of indoor scenes?,"Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias NieÃŸner","Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias NieÃŸner

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020."
What type of scenes are the 3d reconstructions in the paper Scannet focused on?,indoor scenes,"[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks.

indoor scenes"
Which conference featured the paper Scannet: Richly-annotated 3d reconstructions of indoor scenes?,CVPR,"[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

CVPR

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650."
What is the main focus of the paper Scannet: Richly-annotated 3d reconstructions of indoor scenes?,Richly-annotated 3d reconstructions of indoor scenes,"[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017.

Richly-annotated 3d reconstructions of indoor scenes

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916."
Who is the last author of the paper Scannet: Richly-annotated 3d reconstructions of indoor scenes?,Matthias NieÃŸner,"[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017.

Matthias NieÃŸner"
What is the topic of the paper presented at CVPR in 2017?,Scannet: Richly-annotated 3d reconstructions of indoor scenes,"5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks.

Scannet: Richly-annotated 3d reconstructions of indoor scenes

[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916."
How many scenes are mentioned in the paper Scannet: Richly-annotated 3d reconstructions of indoor scenes?,Richly-annotated 3d reconstructions of indoor scenes,"[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

Richly-annotated 3d reconstructions of indoor scenes

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916."
"What is the title of the paper by Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al.?",Distilling the knowledge in a neural network.,"Distilling the knowledge in a neural network.

[60] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming
catastrophic forgetting with hard attention to the task. InICML, pages 4548â€“4557.

Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710.

PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective
learning of deep convolutional neural networks. InECCV, pages 467â€“482."
Who are the authors of the paper '3d-llm: Injecting the 3d world into large language models'?,"Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan.","PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan."
In which conference was the paper 'Occuseg: Occupancy-aware 3d instance segmentation' presented?,"InCVPR, pages 2940â€“2949, 2020.","PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

InCVPR, pages 2940â€“2949, 2020.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591."
"What is the title of the paper by Lei Han, Tian Zheng, Lan Xu, and Lu Fang?",Occuseg: Occupancy-aware 3d instance segmentation.,"[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

arXiv:2502.17429v3  [cs.CV]  21 Nov 2025
THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced
3D Instance Segmentation
Vishal Thengane1
v.thengane@surrey.ac.uk
Jean Lahoud2
jean.lahoud@mbzuai.ac.ae
Hisham Cholakkal2
hisham.cholakkal@mbzuai.ac.ae
Rao Muhammad Anwer2
rao.anwer@mbzuai.ac.ae
Lu Yin1
l.yin@surrey.ac.uk
Xiatian Zhu1
xiatian.zhu@surrey.ac.uk
Salman Khan2, 3
salman.khan@mbzuai.ac.ae
1 University of Surrey,
Guildford, UK
2 Mohamed bin Zayed University of
Artificial Intelligence,
Abu Dhabi, UAE
3 Australian National University,
Canberra, Australia
Abstract
While 3D instance segmentation (3DIS) has advanced significantly, most existing
methods assume that all object classes are known in advance and uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new
classes emerge gradually and exhibit natural imbalance. Although some approaches ad-
dress the emergence of new classes, they often overlook class imbalance, which leads
to suboptimal performance, particularly on rare categories. To tackle this, we propose
CLIMB-3D, a unified framework forCLass-incrementalImbalance-aware3DIS. Build-
ing upon established exemplar replay (ER) strategies, we show that ER alone is insuf-
ficient to achieve robust performance under memory constraints. To mitigate this, we
introduce a novel pseudo-label generator (PLG) that extends supervision to previously
learned categories by leveraging predictions from a frozen model trained on prior tasks. Despite its promise, PLG tends to be biased towards frequent classes. Therefore, we
propose a class-balanced re-weighting (CBR) scheme that estimates object frequencies
from pseudo-labels and dynamically adjusts training bias, without requiring access to
past data. We design and evaluate three incremental scenarios for 3DIS on the challeng-
ing ScanNet200 dataset and additionally validate our method for semantic segmentation
on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by
up to 16.76% mAP for instance segmentation and approximately 30% mIoU for semantic
segmentation, demonstrating strong generalisation across both frequent and rare classes. Code is available at:https://github.com/vgthengane/CLIMB3D. Â© 2025.

[4] Mohamed El Amine Boudjoghra, Salwa Al Khatib, Jean Lahoud, Hisham Cholakkal,
Rao Anwer, Salman H Khan, and Fahad Shahbaz Khan. 3d indoor instance segmenta-
tion in an open-world.NeurIPS, 36, 2024. [5] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calder-
ara. Dark experience for general continual learning: a strong, simple baseline.NeurIPS,
33:15920â€“15930, 2020. [6] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning
imbalanced datasets with label-distribution-aware margin loss.NeurIPS, 32, 2019. [7] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrastive continual learning. In
ICCV, pages 9516â€“9525, 2021. [8] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique.Journal of artificial intelligence
research, 16:321â€“357, 2002. [9] Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and Xinggang Wang. Hierarchical
aggregation for 3d instance segmentation. InICCV, pages 15467â€“15476, 2021. [10] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification is not
all you need for semantic segmentation.NeurIPS, 34:17864â€“17875, 2021. [11] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Gird-
har. Masked-attention mask transformer for universal image segmentation. InCVPR,
pages 1290â€“1299, 2022. [12] Julian Chibane, Francis Engelmann, Tuan Anh Tran, and Gerard Pons-Moll. Box2mask: Weakly supervised 3d semantic instance segmentation using bounding
boxes. InECCV, pages 681â€“699.

Occuseg: Occupancy-aware 3d instance segmentation."
Who are the authors of the paper 'Dyco3d: Robust instance segmentation of 3d point clouds through dynamic convolution'?,"Tong He, Chunhua Shen, and Anton Van Den Hengel.","PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021.

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

Tong He, Chunhua Shen, and Anton Van Den Hengel."
"What is the title of the paper by Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun?",Relieving long-tailed instance segmentation via pairwise class balance.,"Relieving long-tailed instance segmentation via pairwise class balance.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021.

It may be distributed unchanged freely in print or electronic forms. 2THENGANE ET AL.: CLIMB-3D
1 Introduction
Table Chair Couch Pillow Coffee Table Sofa Chair Recycling BinCabinet Unknown
Task-1 Task-2 Task-3
Figure 1:Overview of the CI-3DIS setting.New object categories are introduced incre-
mentally with each task. After every phase, the model must recognise both newly added
and previously learned classes. For instance, in Task 2, categories such asPillow,Coffee
Table, andSofa Chairare introduced, while the model is expected to retain recognition
of earlier classes likeTable,Chair, andCouch. 3D instance segmentation (3DIS) is a fundamental task in computer vision that involves
identifying precise object boundaries and class labels in 3D space, with broad applications in
graphics, robotics, and augmented reality [4, 39]. Traditional 3DIS methods, including top-
down [28, 70, 78], bottom-up [26, 73], and transformer-based approaches [59], perform well
under the assumption that all classes are available and balanced during training. However,
this assumption does not hold in real-world environments, where new object classes emerge
over time and exhibit natural imbalance. This gap motivates the need for Class-Incremental Learning (CIL) [17, 54], which sup-
ports learning new categories while retaining knowledge of previous ones [51]. While CIL
has seen success in 2D image tasks [1, 43, 54, 60], extensions to 3D point clouds remain
limited, often focusing on object-level classification [13, 18, 48]. Recent works on scene-
level class-incremental 3DIS (CI-3DIS) [4] and class-incremental semantic segmentation
(CI-3DSS) [74] show promise, but depend heavily on large exemplar [56] and overlook class
imbalance, limiting their practicality. To address this, we proposeCLIMB-3D, a unified framework forCLass-incremental
IMBalance-aware3DIS that jointly tackles catastrophic forgetting and class imbalance. The
framework begins with Exemplar Replay (ER), storing a small number of samples from
past classes for later replay. However, this alone does not yield promising results under the
strict memory constraints required in CI-3DIS. To address this, we introduce a Pseudo-Label
Generator (PLG), which uses a frozen model from previous task to generate supervision for
earlier classes. However, we observed that PLG tends to favour frequent classes while ig-
noring under-represented ones. To mitigate this, we propose a Class-Balanced Re-weighting
(CBR), which estimates class frequencies from the pseudo-labels to enhance the learning of
rare classes without accessing past data. Together, these components form a practical and
effective solution for real-world 3DIS. To evaluate CLIMB-3D, we designed three benchmark scenarios for CI-3DIS based on
the ScanNet200 dataset [57]. These scenarios simulate incremental learning setup under nat-
ural class imbalance, where new classes emerge based on (A) object frequency, (B) semantic
similarity, or (C) random grouping. Additionally, for comparison with existing methods,
we evaluate our approach on CI-3DSS using the ScanNetV2 dataset [16]. Experimental re-
sults show that our method significantly reduces forgetting and improves performance across
both frequent and rare categories, outperforming previous methods in both settings. Fig. 1
illustrates the CI-3DIS setup. THENGANE ET AL.: CLIMB-3D3
In summary, our contributions are: (i) a novel problem setting of imbalanced class-
incremental 3DIS with an effective method to balance learning and mitigate forgetting; (ii)
three benchmarks modelling continual object emergence with natural imbalance; and (iii)
strong experimental results, achieving up to 16.76% mAP improvement over baselines. 2 Related Work
3D Instance Segmentation.Various methods have been proposed for 3DIS. Grouping-
based approaches adopt a bottom-up pipeline that learns latent embeddings for point cluster-
ing [9, 21, 22, 29, 37, 44, 70, 78]. In contrast, proposal-based methods follow a top-down
strategy by detecting 3D boxes and segmenting objects within them [19, 26, 46, 73, 76]. Recently, transformer-based models [68] have also been applied to 3DIS and 3DSS [59, 64],
inspired by advances in 2D vision [10, 11]. However, these approaches require full anno-
tations for all classes and are not designed for progressive learning, where only new class
annotations are available and previous data is inaccessible. Another line of work aims to re-
duce annotation costs by proposing weakly supervised 3DIS methods based on sparse cues
[12, 27, 72]. While effective with limited annotations, these methods assume a fixed set of
classes and are susceptible to catastrophic forgetting in incremental settings. Incremental Learning.Continual learning involves training models sequentially to miti-
gate catastrophic forgetting. One common strategy is model regularisation, which constrains
parameter updates using techniques such as elastic weight consolidation or knowledge dis-
tillation [1, 24, 35, 43, 60]. Another approach is exemplar replay, where past data is stored
or generated to rehearse older tasks while learning new ones [5, 7, 30, 54]. A third direc-
tion involves dynamically expanding the model architecture or using modular sub-networks
to accommodate new tasks without interfering with old ones [31, 42, 53, 58, 71, 79]. Re-
cent efforts in class-incremental 3D segmentation [63, 74] have shown early promise but
often rely on basic architectures and focus primarily on semantic segmentation. Other works
[4, 36] explore continual learning in 3D settings, though they often depend on large memory
buffers. In contrast, our work introduces a tailored framework for 3D instance segmentation
that effectively transfers knowledge across tasks, even under class imbalance. Long-tailed Recognition.Imbalanced class distributions lead to poor recognition of rare
(long-tail) classes. To address this, re-sampling methods balance the data via input [8, 20, 50,
52, 61] or feature space [3, 14, 40], avoiding naÃ¯ve under-/over-sampling. Loss re-weighting
offers another direction, using class-based [6, 15, 23, 32, 33, 69] or per-example [45, 55, 62]
adjustments to ensure fair contribution. Parameter regularisation improves generalisation
through weight constraints [2], albeit requiring careful tuning. Other approaches leverage
transfer learning [77, 81], self-supervision [41, 75], or contrastive learning [34, 41, 82] to
improve rare-class representations. Long-tailed recognition is well-studied in 2D with large-
scale datasets [15, 49, 67]; in 3D, ScanNet200 [16, 57] enables similar efforts. Prior 3D
work focused on re-weighting, re-sampling, and transfer learning [57], while regularisation
remains underexplored. CeCo [80] balances class centres via auxiliary loss but overlooks
sampling and augmentation. Lahoud et al. [38] propose adaptive classifier regularisation
for 3D segmentation, outperforming prior approaches without threshold tuning. However,
neither method considers incremental learning. In contrast, we jointly address long-tail and
continual 3D semantic segmentation. 4THENGANE ET AL.: CLIMB-3D
Feature 
Backbone
Transformer 
Decoder
Feature 
Backbone
Transformer 
Decoder
(ð‘·,ð’€ð‘¡)
Current model (ð›·
ð‘¡
)
Pseudo Label Generator (PLG) 
Selection
à·¨ð‘Œð¾
ð‘¡
â„’CIâˆ’3DIS
à·¨ð‘Œð¾
ð‘¡,ð‘¤
à´¤ð‘Œð¾
ð‘¡
à· ð‘Œð‘¡
(ð·ð‘¡)
Current Dataset 
Exemplar 
Replay (ER)
+
~
Class-Balanced  
Re-weighting 
(CBR)
1
ð‘“(ð‘)
Figure 2: Overview ofCLIMB-3Dfor CI-3DIS. The model incrementally learns new classes
across sequential phases. During taskt, point cloudsPand their corresponding labelsY t are
sampled from a combination of the current training datasetD t and Exemplar Replay (ER),
which maintains a small memory of past examples. These are then passed to the current
modelÎ¦ t to produce predicted labelsY t . The Pseudo-Label Generator (PLG) selects the
top-Kpredictions from the previous modelÎ¦ tâˆ’1 . These pseudo-labels are then weighted
based on class frequencyf(c)using Class-Balanced Re-weighting (CBR), and the top-Kre-
weighted labels are selected to form the balanced pseudo-label set Â¯Yt . This pseudo-label set
is then concatenated with the ground-truth labels to form a final augmented supervision set
Yt for taskt, which is used to optimise the modelÎ¦ t using Eq. (3). 3 Methodology
3.1 Problem Formulation
3DIS.The objective of this task is to detect and segment each object instance within a point
cloud. Formally, the training dataset is denoted asD={(P i,Yi)}N
i=1, whereNis the number
of training samples. Each inputP i âˆˆR MÃ—6 is a coloured point cloud consisting ofMpoints,
where each point is represented by its 3D coordinates and RGB values. The corresponding
annotationY i ={(m i,j ,ci,j )}Ji
j=1 containsJ i object instances, wheremi,j âˆˆ {0,1}M is a binary
mask indicating which points belong to thej-th instance, andc i,j âˆˆ C={1,...,C}is the
semantic class label of that instance. GivenP i, the modelÎ¦predicts instance-level outputs
Ë†Yi ={(Ë†mi,j ,Ë†ci,j )}
Ë†Ji
j=1, where Ë†mi,j and Ë†ci,j denote the predicted mask and category label for
thej-th instance, respectively. The number of predicted instances Ë†Ji varies depending on the
modelâ€™s inference. The model is optimised using the following objective:
L3DIS(D;Î¦) = 1
|D| âˆ‘
(P,Y)âˆˆD
1
|Y| âˆ‘
(mj,cj)âˆˆY
(Lmask(mj,Ë†mj)+Î»Â·L cls(cj,Ë†cj)),(1)
whereL cls andL mask are the average classification and mask losses over all instances, andÎ»
controls their trade-off. CI-3DIS.Unlike conventional 3DIS, CI-3DIS involves sequential learning, where object
categories are introduced incrementally overTtraining tasks. Each tasktintroduces a dis-
joint set of classesC t , withC= ST
t=1 Ct andC t âˆ©C tâ€²
=/ 0 fortÌ¸=t â€². During each taskt, the THENGANE ET AL.: CLIMB-3D5
model receives a datasetD t ={(P i,Yt
i)}N
i=1, where each coloured point cloudP i âˆˆR MÃ—6
containsMpoints, and the corresponding annotationY t
i ={(m t
i,j ,ct
i,j )}
Jt
i
j=1 includes instance
masksm t
i,j âˆˆ {0,1}M and semantic class labelsc t
i,j âˆˆ Ct . The modelÎ¦ t , initialized from
Î¦tâˆ’1 , is trained onD t to predict instance-level outputs Ë†Yt
i ={(Ë†mt
i,j ,Ë†ct
i,j )}
Ë†Jt
i
j=1, where each
predicted semantic label Ë†ct
i,j belongs to the cumulative set of all classes observed so far:St
k=1 Ck. The key challenge in this setup is to learn new classes without forgetting those
encountered in previous tasks, despite limited supervision and the absence of past labels. 3.2 Method: CLIMB-3D
Overview.As shown in Fig. 2 and formulated in Sec. 3.1, the proposed framework for
CI-3DIS follows aphase-wisetraining strategy. In each phase, the model is exposed to a
carefully curated subset of the dataset, simulating real-world scenarios discussed in Sec. 3.3. Training naÃ¯vely on such phased data leads tocatastrophic forgetting[51], where the model
forgets prior knowledge when learning new tasks. To address this, we first incorporateEx-
emplar Replay(ER) [5], a strategy inspired by 2D incremental methods [1, 43, 54] and
recently extended to 3D settings [4], to mitigate forgetting by storing a small set of repre-
sentative samples from earlier phases. However, ER alone is insufficient to achieve robust
performance. To address this, two additional components are introduced: aPseudo-Label
Generator(PLG), which leverages a frozen model to generate supervision signals for previ-
ously seen classes, and aClass-Balanced Re-weighting(CBR) module that compensates for
class imbalance across phases. Each of these components is described in detail below 1. Exemplar Replay (ER).Inspired by Buzzega et al. [5], ER addresses the issue of catas-
trophic forgetting by allowing the model to retain a subset of data from earlier phases. During phaset, the model is trained not only on the current task dataD t , but also on a
set of exemplarsE 1:tâˆ’1, collected from previous phases. These exemplars are accumulated
incrementally:E 1:tâˆ’1 =E 1 âˆªÂ·Â·Â·âˆªE tâˆ’1 .Here,E t denotes the exemplar subset stored after
phaset, and|E t |denotes the exemplar replay size. The combined replay dataset is denoted
as:D t
ER =D t âˆªE 1:tâˆ’1.Epoch training consists of two stages: first, the model is trained on
the current data,D t ; then it is trained using exemplars fromE 1:tâˆ’1. While previous CI-3DIS approaches rely on large exemplar sets to mitigate forgetting [4],
such strategies become impractical under memory constraints. Consequently, although ER
aids knowledge retention, it is insufficient on its own for robust performance with a limited
exemplar budget. Therefore, PLG and CBR are introduced to better preserve past represen-
tations and enhance generalisation across phases. Pseudo-Label Generator (PLG).In the CI-3DIS setting, during phaset(wheret>1),
although ground-truth labels for previously seen classes are unavailable, the model from the
previous phaseÎ¦ tâˆ’1 , which preserves knowledge of past tasks, is retained. We use this
model to generate pseudo-labels for previously seen classes, thereby providing approximate
supervision during training. This allows the current modelÎ¦t , to retain prior knowledge and
reduce forgetting, even without access to ground-truth annotations. Given a point cloud and label pair(P,Y t ), fromD t , the previous modelÎ¦ tâˆ’1 , generates
pseudo-labels for the previously seen classes as Ë†Y1:tâˆ’1 =Î¦ tâˆ’1 (P), which we denote as ËœYt
1For clarity, modifications introduced by each component (ER, PLG, and CBR) are highlighted in green, blue,
and red, respectively. 6THENGANE ET AL.: CLIMB-3D
for brevity. However, we observe that utilising all predictions fromÎ¦tâˆ’1 introduces noisy or
incorrect labels, which degrade overall performance. To mitigate this, we select the top-K
most confident instance predictions, denoted as ËœYt
K, and concatenate them with the ground-
truth labels of the current task to obtain the supervision set: Â¯Yt =Y t âˆ¥ ËœYt
K. Following this,
the loss is computed over both the current taskâ€™s ground-truth labels and the top-Kconfi-
dent pseudo-labels from earlier phases, enabling the model to retain prior knowledge while
adapting to new classes. Class-Balanced Re-weighting (CBR).While ER and PLG contribute to preserving prior
knowledge, we observe that the modelâ€™s predictions are biased towards frequent classes,
resulting in the forgetting of rare categories. This issue is further amplified by the top-K
pseudo-labels selected by PLG, which predominantly represent dominant classes in the data. To address this, we propose aClass-Balanced Re-weighting(CBR) scheme that compensates
for class imbalance using object frequency statistics. At taskt, only the current datasetD t
and its label distributionp t (c)over classescâˆˆ Ct are available. Datasets and class distribu-
tions from earlier tasks{D i,p i(c)}tâˆ’1
i=1 are no longer accessible, which makes it challenging
to directly account for class imbalance across all previously seen tasks. Therefore, we pro-
pose to leverage the pseudo-label predictions of the frozen modelÎ¦ tâˆ’1 as a proxy for the
class distribution across previously learned categories. During each training iteration, the previous modelÎ¦ tâˆ’1 is applied to the current inputP
to generate pseudo-labels ËœYt , from which we accumulate class-wise frequency statistics for
previously learned classes. LetC1:t = St
i=1 Ci represent the union of all classes seen up to task
t. At the end of each epoch, we compute the overall class frequencyfâˆˆR |C1:t | by combining
the pseudo-label distribution Ëœpt (c)derived from ËœYt with the ground-truth label distribution
pt (c)from the current task. The resulting frequency vectorf= [f(c)] câˆˆC1:t is then used to
compute class-wise weightsw= [w(c)] câˆˆC1:t , which guide balanced pseudo-label selection
and help the model focus on rare classes. In the next epoch, the modelÎ¦ tâˆ’1 produces soft pseudo-label predictions ËœYt (as de-
scribed in PLG) , where each component ËœYt [c]denotes the confidence score for classcâˆˆ
C1:tâˆ’1. To promote balanced pseudo-label selection, class-wise re-weighting is applied using
w(c) = 1
f(c)+Îµ , whereÎµis a small constant for numerical stability. The adjusted predictions
are computed by applying the class-wise weights as: ËœYt,w [c] =w(c)Â· ËœYt [c],âˆ€câˆˆ C 1:tâˆ’1. Top-Kselection is then applied to both the original and the re-weighted scores, yielding
two pseudo-label sets: ËœYt
K and ËœYt,w
K , respectively. The final augmented target set, Yt , is
constructed by concatenating the ground-truth labelsY t with both sets of pseudo-labels:
Yt =Y t âˆ¥ ËœYt
K âˆ¥ ËœYt,w
K .(2)
Although the above re-weighting mitigates bias from prior tasks, it does not address class
imbalance within the current taskt. To resolve this, the class-balanced weightsw c are ex-
tended to incorporate statistics from both previously seen and current classes, i.e., overC 1:t
rather than onlyC 1:tâˆ’1. The updated weights, denoted asw â€²
c, are used to re-weights both
pseudo-label selection and classification loss, ensuring balanced supervision across all seen
categories, including rare ones. This unified strategy reduces bias and enhances performance
across phases. The pseudo-code for constructing the supervision set in Eq. (2), which incor-
porates both PLG and CBR components, is provided in Appendix A. Final Objective.The final training objective ofCLIMB-3D, incorporating ER, PLG, and THENGANE ET AL.: CLIMB-3D7
CBR, is formulated in Eq. (1) as:
L(Dt
ER;Î¦t ) = 1
|Dt
ER| âˆ‘
(P,Yt )âˆˆDt
ER
1
|Yt | âˆ‘
(mt
j,ct
j)âˆˆYt
 
Lmask(mt
j,Ë†mt
j)+w â€²â€²
c Â·Lcls(ct
j,Ë†ct
j)

,(3)
wherew â€²â€²
c =w â€²
c Â·Î» cls denotes the scaled weight vector, andÎ» cls is a hyperparameter for bal-
ancing the loss terms. 3.3 Benchmarking Incremental Scenarios
While CI-3DIS methods offer a wide range of practical applications, they frequently rely
on the assumption of uniform sample distribution, which rarely holds in real-world settings. In practice, the number of object categories, denoted byC, is often large and characterised
by substantial variability in category frequency, shape, structure, and size. To address these
challenges, we propose three incremental learning scenarios, each designed to capture a
different facet of real-world complexity. The design of these scenarios is detailed below; for
further information and illustrations, refer to Appendix E of the supplementary material. Split-A: Frequency Scenarios.This scenario acknowledges that datasets are often la-
belled based on the frequency of categories. To accommodate this, we propose a split where
the model learns from the most frequent categories and subsequently incorporates the less
frequent ones in later stages. By prioritising the training of frequently occurring categories,
the model can establish a strong foundation before expanding to handle rare categories. Split-B: Semantic Scenarios.Beyond frequency, semantic similarity is crucial in real-
world deployments. While objects often share visual or functional traits, models may en-
counter semantically different categories in new environments. To simulate this challenge,
we introduce theSplit-Bscenario. Here, categories are grouped based on their semantic
relationships, and the model is incrementally trained on one semantic group at a time. This
setup encourages the model to generalise across semantically similar categories and adapt
more effectively when exposed to new ones. Unlike theSplit-Ascenario, which organises
learning based on category frequency, theSplit-Bscenario may contain both frequent and
infrequent categories within a single task, focusing on semantic continuity and transfer. Split-C: Random Scenarios.In some cases, data labelling is driven by the availability of
objects rather than predefined criteria. To capture this, we introduce theSplit-Cscenario,
which represents a fully random setting where any class can appear in any task, resulting in
varying levels of class imbalance. By exposing the model to such diverse and unpredictable
distributions, we aim to improve its robustness in real-world situations where labelled data
availability is inconsistent. These incremental scenarios are designed to provide a more realistic representation of
object distributions, frequencies, and dynamics encountered in the real world. 4 Experiments
4.1 Setup
Datasets.We evaluateCLIMB-3Don ScanNet200 [57], which comprises 200 object cat-
egories and exhibits significant class imbalance, which makes it ideal for simulating and 8THENGANE ET AL.: CLIMB-3D
assessing real-world scenarios. In addition, we compare our approach against existing in-
cremental learning methods using ScanNetV2 [16] in the 3DSS setting. For this evaluation,
we follow the standard training and validation splits defined in prior works [74] to ensure
consistency with existing methods. Evaluation Metrics.We evaluate our method usingMean Average Precision(mAP), a
standard metric for 3DIS which provides a comprehensive measure of segmentation quality
by accounting for both precision and recall. For comparison with existing 3DSS methods,
we report themean Intersection over Union(mIoU), which quantifies the overlap between
predicted and ground truth segments. To assess the modelâ€™s ability to mitigate catastrophic
forgetting in incremental settings, we use theForgetting Percentage Points(FPP) metric, as
defined in [47]. FPP captures performance degradation by measuring the accuracy drop on
the initially seen categories between the first and final training phases. Detailed descriptions of the incremental scenarios (Split-A,Split-B,Split-C) and im-
plementation are provided in Appendix B. 4.2 Results and Discussion
Table 1: Comparison between the proposed method and the baseline in the 3DIS setting,
evaluated using mAP25, mAP50, mAP, and FPP after training across all phase. Scenarios Methods Average Precisionâ†‘FPPâ†“
mAP25 mAP50 mAP mAP 25 mAP50
Baseline 16.46 14.29 10.44 51.30 46.82Split-A CLIMB-3D(Ours) 35.69 31.05 22.72 3.44 2.63
Baseline 17.22 15.07 10.93 46.27 42.1Split-B CLIMB-3D(Ours) 35.48 31.56 23.69 8.00 5.51
Baseline 25.65 21.08 14.85 31.68 28.84Split-C CLIMB-3D(Ours) 31.59 26.78 18.93 9.10 7.89
To evaluate our proposed approach, we construct a baseline using ER [5] for the 3DIS
setting, as no existing incremental baselines are available for this task. In contrast, for the
3DSS setting, where incremental baselines do exist, we compare our method against three
prior approaches [43, 60, 74]. Results on CI-3DIS.Tab. 1 compares CLIMB-3D against the ER baseline across the three
CI-3DIS scenarios, evaluated after all phases using mAP 25, mAP50, overall mAP, and FPP. In theSplit-Ascenario, characterised by significant distribution shifts, our method achieves
gains of +19.23%, +16.76%, and +12.28% in mAP 25, mAP 50, and overall mAP, respec-
tively, while drastically reducing forgetting with FPP scores of 3.44% (mAP 25) and 2.63%
(mAP50), both over 45 points lower than the baseline. Under theSplit-Bscenario, with
semantically related new categories, improvements of +18.26%, +16.49%, and +12.76%
are observed alongside a reduction in forgetting to 8.00% and 5.51% from baseline levels
above 42%. For theSplit-Cscenario, featuring increasing geometric complexity, CLIMB-
3D maintains solid gains of +5.94%, +5.70%, and +4.08% across mAP metrics, with forget-
ting reduced by over 22 points. Overall, these results demonstrate robust forward transfer,
minimal forgetting, and stable learning, highlighting the methodâ€™s effectiveness for scalable
continual 3D semantic understanding. THENGANE ET AL.: CLIMB-3D9
Table 2: Comparison between the proposed method and existing baselines in the 3DSS set-
ting on ScanNetV2 [16], evaluated using mIoU. Methods Phase=1 Phase=2 All
EWC [60] 17.75 13.22 16.62
LwF [43] 30.38 13.37 26.13
Yanget al. [74] 34.16 13.43 28.98
CLIMB-3D(Ours) 69.39 32.56 59.38
Results on CI-3DSS.Although our primary focus is on CI-3DIS, we also evaluate our
method under the CI-3DSS setting to enable comparisons with existing approaches. To do
so, we adapt our predictions to assign each point the label corresponding to the highest-
confidence mask and exclude background classes (e.g., floor and wall), as these are not part
of the semantic segmentation targets. Tab. 2 presents a comparison between our method and
existing baselines on the ScanNet V2 dataset [16], evaluated using mIoU across two training
phases and overall. Although originally designed for instance segmentation, our method
generalises effectively to semantic segmentation, achieving substantial gains of +35.23%
mIoU in Phase 1 and +19.1% in Phase 2. Overall, it achieves 59.38% mIoU, significantly
higher than theâˆ¼30% mIoU of prior methods, highlighting its robustness and transferability. Refer to Appendix C, D, and F of the supplementary material for detailed analyses of per-
phase performance, rare-class evaluation, and qualitative result comparisons, respectively. 4.3 Ablation
Table 3: Ablation study illustrating the impact of each component in a three-phase setup. Each split (s) and corresponding number indicates data introduced at that phase. Best results
are highlighted inbold. Row Modules p=1â†‘p=2â†‘p=3â†‘ FPPâ†“
s1 s1 s2 All s1 s2 s3 All
1. Oracle - - - - 55.14 30.77 25.30 37.68 -
2. NaÃ¯ve 56.82 0.00 28.09 14.15 0.00 0.00 19.67 5.80 56.82
3. + ER 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28 46.44
4. + PLG 56.82 50.0034.3942.13 49.78 11.41 26.47 29.28 7.04
5. + CBR 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05 2.63
We conduct an ablation study to evaluate the contribution of each component in our
framework. As an upper bound, we report the performance of anOraclemodel, which is
trained jointly on the full dataset. For the incremental setting, we follow the previously
defined splits and first train the model naively across phases. We then incrementally add
each module to isolate its impact. Tab. 3 presents results for theSplit-Ascenario using both
mAP50 and FPP metrics. NaÃ¯ve Training.When trained naÃ¯vely without any dedicated modules, the model suffers
from severe catastrophic forgetting, as evident in row 2. The model entirely forgets pre-
viously learned classes upon entering new phases, with performance dropping to zero for
earlier splits and FPP rising to 56.82. 10THENGANE ET AL.: CLIMB-3D
Effect of ER.Adding exemplar replay (row 3) partially alleviates forgetting by maintaining
a buffer of past examples. This yields notable gains fors1in Phase 2 (+18.51%) and Phase 3
(+10.38%), and also improvess2in Phase 3 (+9.43%). However, the overall forgetting
remains substantial (FPP: 46.44), showing ER alone is insufficient. Effect of PLG.The addition of the pseudo-label generator (PLG), which generates labels for
previous classes by retaining a copy of the model from an earlier phase, facilitates knowledge
retention and forward transfer. As shown in row 4, PLG significantly reduces forgetting and
enhances performance on current tasks. Fors1, it improves mAP 50 by 31.49% in Phase 2
and 39.40% in Phase 3 over exemplar replay. Overall, PLG yields a 15.00% increase in
performance and reduces forgetting by 39.40%. Effect of CBR.Finally, class-balanced re-weighting (CBR) mitigates class imbalance dur-
ing both pseudo-labelling and current task learning by adjusting each classâ€™s contribution
based on its frequency. As shown in row 5, CBR enhancess1retention over PLG in Phases 2
and 3 (+4.67% and +4.41%), and further improves performance ons2ands3. It also achieves
the lowest FPP of 2.63, reflecting strong forgetting mitigation. Overall, CBR offers the best
trade-off between retaining past knowledge and acquiring new tasks, outperforming PLG
and ER by 4.41% and 43.81%, respectively. 4.4 Limitations & Future Work
While CLIMB-3D achieves strong performance across CI-3DIS and CI-3DSS settings, sev-
eral limitations remain and warrant discussion. The experiments are currently limited to
indoor datasets (ScanNet200 and ScanNetV2), therefore evaluating performance on outdoor
scenes would help assess generalisability across diverse environments. The current setup
also considers only three incremental phases, whereas real-world applications often involve
longer sequences; incorporating such extended setups would better reflect practical chal-
lenges. Moreover, the model operates in a uni-modal setting. Integrating multi-modal cues,
such as vision-language models, could further enhance performance, as suggested by recent
2D and 3D studies [25, 65, 66]. 5 Conclusion
This work addresses the challenge of catastrophic forgetting in class-incremental 3D in-
stance segmentation by introducing a modular framework that integrates exemplar replay
(ER), a pseudo-label generator (PLG), and class-balanced re-weighting (CBR). The pro-
posed method incrementally adapts to new classes while preserving prior knowledge, all
without requiring access to the full dataset. Extensive experiments on a three-phase bench-
mark validate the individual and combined effectiveness of each component: ER enables
efficient memory-based retention; PLG facilitates knowledge preservation through pseudo-
supervision; and CBR mitigates class imbalance to enhance learning stability. Together,
these components significantly reduce forgetting and improve segmentation performance
across all phases. By achieving a strong balance between stability and plasticity, our ap-
proach advances continual 3D scene understanding and sets a new baseline for future re-
search in this area. THENGANE ET AL.: CLIMB-3D11
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne
Tuytelaars. Memory aware synapses: Learning what (not) to forget. InECCV, pages
139â€“154, 2018. [2] Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long-tailed
recognition via weight balancing. InCVPR, pages 6897â€“6907, 2022. [3] Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying
imbalanced data. InJoint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 770â€“785."
In which conference was the paper '3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation' presented?,"InCVPR, pages 9031â€“9040, 2020.","PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

InCVPR, pages 9031â€“9040, 2020.

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916."
Who are the authors of the paper '3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation'?,"Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias NieÃŸner.","Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias NieÃŸner.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020."
"What is the title of the paper by Agrim Gupta, Piotr Dollar, and Ross Girshick?",Lvis: A dataset for large vocabulary instance segmentation.,"[4] Mohamed El Amine Boudjoghra, Salwa Al Khatib, Jean Lahoud, Hisham Cholakkal,
Rao Anwer, Salman H Khan, and Fahad Shahbaz Khan. 3d indoor instance segmenta-
tion in an open-world.NeurIPS, 36, 2024. [5] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calder-
ara. Dark experience for general continual learning: a strong, simple baseline.NeurIPS,
33:15920â€“15930, 2020. [6] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning
imbalanced datasets with label-distribution-aware margin loss.NeurIPS, 32, 2019. [7] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrastive continual learning. In
ICCV, pages 9516â€“9525, 2021. [8] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique.Journal of artificial intelligence
research, 16:321â€“357, 2002. [9] Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and Xinggang Wang. Hierarchical
aggregation for 3d instance segmentation. InICCV, pages 15467â€“15476, 2021. [10] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification is not
all you need for semantic segmentation.NeurIPS, 34:17864â€“17875, 2021. [11] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Gird-
har. Masked-attention mask transformer for universal image segmentation. InCVPR,
pages 1290â€“1299, 2022. [12] Julian Chibane, Francis Engelmann, Tuan Anh Tran, and Gerard Pons-Moll. Box2mask: Weakly supervised 3d semantic instance segmentation using bounding
boxes. InECCV, pages 681â€“699.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

Lvis: A dataset for large vocabulary instance segmentation.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916."
Who are the authors of the paper 'Lvis: A dataset for large vocabulary instance segmentation'?,"Agrim Gupta, Piotr Dollar, and Ross Girshick.","[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

Agrim Gupta, Piotr Dollar, and Ross Girshick.

PMLR, 2019. 14THENGANE ET AL.: CLIMB-3D
[43] Zhizhong Li and Derek Hoiem. Learning without forgetting.IEEE PAMI, 40(12):
2935â€“2947, 2017. [44] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmenta-
tion in 3d scenes using semantic superpoint tree networks. InICCV, pages 2783â€“2792,
2021. [45] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss
for dense object detection. InICCV, pages 2980â€“2988, 2017.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020."
What is the title of the paper by Li Jiang et al.?,"Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation.","[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019.

Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020."
Which conference did the paper by Li Jiang et al. appear in?,"InCVPR, pages 4867â€“4876, 2020.","InCVPR, pages 4867â€“4876, 2020.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591."
How many pages does the paper by Li Jiang et al. span?,"InCVPR, pages 4867â€“4876, 2020.","InCVPR, pages 4867â€“4876, 2020.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591."
Who are the authors of the paper 'Pointgroup: Dual-set point grouping for 3d instance segmentation'?,"Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia.","PMLR, 2019. 14THENGANE ET AL.: CLIMB-3D
[43] Zhizhong Li and Derek Hoiem. Learning without forgetting.IEEE PAMI, 40(12):
2935â€“2947, 2017. [44] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmenta-
tion in 3d scenes using semantic superpoint tree networks. InICCV, pages 2783â€“2792,
2021. [45] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss
for dense object detection. InICCV, pages 2980â€“2988, 2017.

Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia.

PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective
learning of deep convolutional neural networks. InECCV, pages 467â€“482.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020."
What is the main focus of the paper by Li Jiang et al.?,"Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation.","Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020."
What type of segmentation does the paper by Li Jiang et al. discuss?,"Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation.","Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020."
What is the specific method proposed by Li Jiang et al. for 3D instance segmentation?,"Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation.","Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019.

Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020."
What is the significance of 'Dual-set point grouping' in 3D instance segmentation according to Li Jiang et al.?,"Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation.","Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650."
What year was the paper by Li Jiang et al. published?,"InCVPR, pages 4867â€“4876, 2020.","PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

InCVPR, pages 4867â€“4876, 2020.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916."
What is the specific area of computer vision addressed by the paper 'Pointgroup: Dual-set point grouping for 3d instance segmentation'?,"Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation.","[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation.

Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020."
Who authored the paper 'Deep generative dual memory network for continual learning' in 2017?,"Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for continual learning.arXiv preprint arXiv:1710.10368, 2017.","Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for continual learning.arXiv preprint arXiv:1710.10368, 2017.

[30] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for
continual learning.arXiv preprint arXiv:1710.10368, 2017. [31] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence
of similar and dissimilar tasks.NeurIPS, 33:18493â€“18504, 2020. [32] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019. [33] Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and
Roberto Togneri. Cost-sensitive learning of deep feature representations from im-
balanced data.IEEE transactions on neural networks and learning systems, 29(8):
3573â€“3587, 2017. [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 33:18661â€“18673, 2020. [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-
jardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Pro-
ceedings of the national academy of sciences, 114(13):3521â€“3526, 2017. [36] Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual
learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024. [37] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3d instance
segmentation via multi-task metric learning. InICCV, pages 9256â€“9266, 2019. [38] Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and
Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint
and sampling. InInternational Conference on Robotics and Automation (ICRA), pages
5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029. [39] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,
and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022. [40] Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing
Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In
CVPR, pages 5212â€“5221, 2021. [41] Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-
tailed visual recognition. InICCV, pages 630â€“639, 2021. [42] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In
ICML, pages 3925â€“3934.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

[46] Shih-Hung Liu, Shang-Yi Yu, Shao-Chi Wu, Hwann-Tzong Chen, and Tyng-Luh
Liu. Learning gaussian instance segmentation in point clouds.arXiv preprint
arXiv:2007.09860, 2020. [47] Yaoyao Liu, Bernt Schiele, Andrea Vedaldi, and Christian Rupprecht. Continual de-
tection transformer for incremental object detection. InCVPR, pages 23799â€“23808,
2023. [48] Yuyang Liu, Yang Cong, Gan Sun, Tao Zhang, Jiahua Dong, and Hongsen Liu. L3doc:
Lifelong 3d object classification.ICIP, 30:7486â€“7498, 2021. [49] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X
Yu. Large-scale long-tailed recognition in an open world. InCVPR, pages 2537â€“2546,
2019. [50] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of
weakly supervised pretraining. InECCV, pages 181â€“196, 2018. [51] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist net-
works: The sequential learning problem. InPsychology of learning and motivation,
volume 24, pages 109â€“165."
In which conference was the paper 'Striking the right balance with uncertainty' presented?,"Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019.","[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021."
What is the title of the paper authored by James Kirkpatrick in 2017?,"James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Proceedings of the national academy of sciences, 114(13):3521â€“3526, 2017.","[60] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming
catastrophic forgetting with hard attention to the task. InICML, pages 4548â€“4557.

[30] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for
continual learning.arXiv preprint arXiv:1710.10368, 2017. [31] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence
of similar and dissimilar tasks.NeurIPS, 33:18493â€“18504, 2020. [32] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019. [33] Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and
Roberto Togneri. Cost-sensitive learning of deep feature representations from im-
balanced data.IEEE transactions on neural networks and learning systems, 29(8):
3573â€“3587, 2017. [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 33:18661â€“18673, 2020. [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-
jardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Pro-
ceedings of the national academy of sciences, 114(13):3521â€“3526, 2017. [36] Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual
learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024. [37] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3d instance
segmentation via multi-task metric learning. InICCV, pages 9256â€“9266, 2019. [38] Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and
Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint
and sampling. InInternational Conference on Robotics and Automation (ICRA), pages
5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029. [39] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,
and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022. [40] Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing
Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In
CVPR, pages 5212â€“5221, 2021. [41] Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-
tailed visual recognition. InICCV, pages 630â€“639, 2021. [42] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In
ICML, pages 3925â€“3934.

James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Proceedings of the national academy of sciences, 114(13):3521â€“3526, 2017.

Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710."
Who are the authors of the paper 'Is continual learning ready for real-world challenges?',"Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024.","[46] Shih-Hung Liu, Shang-Yi Yu, Shao-Chi Wu, Hwann-Tzong Chen, and Tyng-Luh
Liu. Learning gaussian instance segmentation in point clouds.arXiv preprint
arXiv:2007.09860, 2020. [47] Yaoyao Liu, Bernt Schiele, Andrea Vedaldi, and Christian Rupprecht. Continual de-
tection transformer for incremental object detection. InCVPR, pages 23799â€“23808,
2023. [48] Yuyang Liu, Yang Cong, Gan Sun, Tao Zhang, Jiahua Dong, and Hongsen Liu. L3doc:
Lifelong 3d object classification.ICIP, 30:7486â€“7498, 2021. [49] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X
Yu. Large-scale long-tailed recognition in an open world. InCVPR, pages 2537â€“2546,
2019. [50] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of
weakly supervised pretraining. InECCV, pages 181â€“196, 2018. [51] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist net-
works: The sequential learning problem. InPsychology of learning and motivation,
volume 24, pages 109â€“165.

[30] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for
continual learning.arXiv preprint arXiv:1710.10368, 2017. [31] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence
of similar and dissimilar tasks.NeurIPS, 33:18493â€“18504, 2020. [32] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019. [33] Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and
Roberto Togneri. Cost-sensitive learning of deep feature representations from im-
balanced data.IEEE transactions on neural networks and learning systems, 29(8):
3573â€“3587, 2017. [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 33:18661â€“18673, 2020. [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-
jardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Pro-
ceedings of the national academy of sciences, 114(13):3521â€“3526, 2017. [36] Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual
learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024. [37] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3d instance
segmentation via multi-task metric learning. InICCV, pages 9256â€“9266, 2019. [38] Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and
Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint
and sampling. InInternational Conference on Robotics and Automation (ICRA), pages
5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029. [39] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,
and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022. [40] Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing
Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In
CVPR, pages 5212â€“5221, 2021. [41] Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-
tailed visual recognition. InICCV, pages 630â€“639, 2021. [42] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In
ICML, pages 3925â€“3934.

Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024.

[52] Seulki Park, Youngkyu Hong, Byeongho Heo, Sangdoo Yun, and Jin Young Choi. The
majority can help the minority: Context-rich minority oversampling for long-tailed
classification. InCVPR, pages 6887â€“6896, 2022. [53] Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan, and Ling
Shao. Random path selection for incremental learning.NeurIPS, 3, 2019. [54] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lam-
pert. icarl: Incremental classifier and representation learning. InCVPR, pages 2001â€“
2010, 2017. [55] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight
examples for robust deep learning. InICML, pages 4334â€“4343. PMLR, 2018. [56] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory
Wayne. Experience replay for continual learning.NeurIPS, 32, 2019. [57] David Rozenberszki, Or Litany, and Angela Dai. Language-grounded indoor 3d se-
mantic segmentation in the wild. InECCV, 2022. THENGANE ET AL.: CLIMB-3D15
[58] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirk-
patrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural
networks.arXiv preprint arXiv:1606.04671, 2016. [59] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bas-
tian Leibe. Mask3D: Mask Transformer for 3D Semantic Instance Segmentation. In
International Conference on Robotics and Automation (ICRA), 2023."
What is the doi of the paper 'Long-tailed 3d semantic segmentation with adaptive weight constraint and sampling'?,"Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint and sampling. InInternational Conference on Robotics and Automation (ICRA), pages 5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029.","Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710.

[4] Mohamed El Amine Boudjoghra, Salwa Al Khatib, Jean Lahoud, Hisham Cholakkal,
Rao Anwer, Salman H Khan, and Fahad Shahbaz Khan. 3d indoor instance segmenta-
tion in an open-world.NeurIPS, 36, 2024. [5] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calder-
ara. Dark experience for general continual learning: a strong, simple baseline.NeurIPS,
33:15920â€“15930, 2020. [6] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning
imbalanced datasets with label-distribution-aware margin loss.NeurIPS, 32, 2019. [7] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrastive continual learning. In
ICCV, pages 9516â€“9525, 2021. [8] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique.Journal of artificial intelligence
research, 16:321â€“357, 2002. [9] Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and Xinggang Wang. Hierarchical
aggregation for 3d instance segmentation. InICCV, pages 15467â€“15476, 2021. [10] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification is not
all you need for semantic segmentation.NeurIPS, 34:17864â€“17875, 2021. [11] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Gird-
har. Masked-attention mask transformer for universal image segmentation. InCVPR,
pages 1290â€“1299, 2022. [12] Julian Chibane, Francis Engelmann, Tuan Anh Tran, and Gerard Pons-Moll. Box2mask: Weakly supervised 3d semantic instance segmentation using bounding
boxes. InECCV, pages 681â€“699.

[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021.

Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint and sampling. InInternational Conference on Robotics and Automation (ICRA), pages 5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029."
Who are the authors of the paper 'Stratified transformer for 3d point cloud segmentation' in 2022?,"Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022.","PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019.

Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022.

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591."
In which conference was the paper 'Metasaug: Meta semantic augmentation for long-tailed visual recognition' presented?,"Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In CVPR, pages 5212â€“5221, 2021.","Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In CVPR, pages 5212â€“5221, 2021.

[30] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for
continual learning.arXiv preprint arXiv:1710.10368, 2017. [31] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence
of similar and dissimilar tasks.NeurIPS, 33:18493â€“18504, 2020. [32] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019. [33] Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and
Roberto Togneri. Cost-sensitive learning of deep feature representations from im-
balanced data.IEEE transactions on neural networks and learning systems, 29(8):
3573â€“3587, 2017. [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 33:18661â€“18673, 2020. [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-
jardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Pro-
ceedings of the national academy of sciences, 114(13):3521â€“3526, 2017. [36] Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual
learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024. [37] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3d instance
segmentation via multi-task metric learning. InICCV, pages 9256â€“9266, 2019. [38] Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and
Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint
and sampling. InInternational Conference on Robotics and Automation (ICRA), pages
5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029. [39] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,
and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022. [40] Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing
Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In
CVPR, pages 5212â€“5221, 2021. [41] Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-
tailed visual recognition. InICCV, pages 630â€“639, 2021. [42] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In
ICML, pages 3925â€“3934.

Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710.

[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021."
What is the title of the paper authored by Tianhao Li in 2021?,"Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-tailed visual recognition. InICCV, pages 630â€“639, 2021.","[46] Shih-Hung Liu, Shang-Yi Yu, Shao-Chi Wu, Hwann-Tzong Chen, and Tyng-Luh
Liu. Learning gaussian instance segmentation in point clouds.arXiv preprint
arXiv:2007.09860, 2020. [47] Yaoyao Liu, Bernt Schiele, Andrea Vedaldi, and Christian Rupprecht. Continual de-
tection transformer for incremental object detection. InCVPR, pages 23799â€“23808,
2023. [48] Yuyang Liu, Yang Cong, Gan Sun, Tao Zhang, Jiahua Dong, and Hongsen Liu. L3doc:
Lifelong 3d object classification.ICIP, 30:7486â€“7498, 2021. [49] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X
Yu. Large-scale long-tailed recognition in an open world. InCVPR, pages 2537â€“2546,
2019. [50] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of
weakly supervised pretraining. InECCV, pages 181â€“196, 2018. [51] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist net-
works: The sequential learning problem. InPsychology of learning and motivation,
volume 24, pages 109â€“165.

[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021.

[30] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for
continual learning.arXiv preprint arXiv:1710.10368, 2017. [31] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence
of similar and dissimilar tasks.NeurIPS, 33:18493â€“18504, 2020. [32] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019. [33] Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and
Roberto Togneri. Cost-sensitive learning of deep feature representations from im-
balanced data.IEEE transactions on neural networks and learning systems, 29(8):
3573â€“3587, 2017. [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 33:18661â€“18673, 2020. [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-
jardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Pro-
ceedings of the national academy of sciences, 114(13):3521â€“3526, 2017. [36] Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual
learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024. [37] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3d instance
segmentation via multi-task metric learning. InICCV, pages 9256â€“9266, 2019. [38] Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and
Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint
and sampling. InInternational Conference on Robotics and Automation (ICRA), pages
5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029. [39] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,
and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022. [40] Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing
Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In
CVPR, pages 5212â€“5221, 2021. [41] Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-
tailed visual recognition. InICCV, pages 630â€“639, 2021. [42] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In
ICML, pages 3925â€“3934.

Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-tailed visual recognition. InICCV, pages 630â€“639, 2021."
Who are the authors of the paper 'Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting'?,"Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting. InICML, pages 3925â€“3934.","Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting. InICML, pages 3925â€“3934.

[60] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming
catastrophic forgetting with hard attention to the task. InICML, pages 4548â€“4557.

[30] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for
continual learning.arXiv preprint arXiv:1710.10368, 2017. [31] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence
of similar and dissimilar tasks.NeurIPS, 33:18493â€“18504, 2020. [32] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019. [33] Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and
Roberto Togneri. Cost-sensitive learning of deep feature representations from im-
balanced data.IEEE transactions on neural networks and learning systems, 29(8):
3573â€“3587, 2017. [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 33:18661â€“18673, 2020. [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-
jardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Pro-
ceedings of the national academy of sciences, 114(13):3521â€“3526, 2017. [36] Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual
learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024. [37] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3d instance
segmentation via multi-task metric learning. InICCV, pages 9256â€“9266, 2019. [38] Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and
Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint
and sampling. InInternational Conference on Robotics and Automation (ICRA), pages
5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029. [39] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,
and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022. [40] Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing
Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In
CVPR, pages 5212â€“5221, 2021. [41] Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-
tailed visual recognition. InICCV, pages 630â€“639, 2021. [42] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In
ICML, pages 3925â€“3934.

PMLR, 2019. 14THENGANE ET AL.: CLIMB-3D
[43] Zhizhong Li and Derek Hoiem. Learning without forgetting.IEEE PAMI, 40(12):
2935â€“2947, 2017. [44] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmenta-
tion in 3d scenes using semantic superpoint tree networks. InICCV, pages 2783â€“2792,
2021. [45] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss
for dense object detection. InICCV, pages 2980â€“2988, 2017."
What is the publication year of the paper 'Cost-sensitive learning of deep feature representations from imbalanced data'?,"Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and Roberto Togneri. Cost-sensitive learning of deep feature representations from imbalanced data.IEEE transactions on neural networks and learning systems, 29(8): 3573â€“3587, 2017.","Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and Roberto Togneri. Cost-sensitive learning of deep feature representations from imbalanced data.IEEE transactions on neural networks and learning systems, 29(8): 3573â€“3587, 2017.

[30] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for
continual learning.arXiv preprint arXiv:1710.10368, 2017. [31] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence
of similar and dissimilar tasks.NeurIPS, 33:18493â€“18504, 2020. [32] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019. [33] Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and
Roberto Togneri. Cost-sensitive learning of deep feature representations from im-
balanced data.IEEE transactions on neural networks and learning systems, 29(8):
3573â€“3587, 2017. [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 33:18661â€“18673, 2020. [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-
jardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Pro-
ceedings of the national academy of sciences, 114(13):3521â€“3526, 2017. [36] Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual
learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024. [37] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3d instance
segmentation via multi-task metric learning. InICCV, pages 9256â€“9266, 2019. [38] Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and
Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint
and sampling. InInternational Conference on Robotics and Automation (ICRA), pages
5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029. [39] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,
and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022. [40] Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing
Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In
CVPR, pages 5212â€“5221, 2021. [41] Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-
tailed visual recognition. InICCV, pages 630â€“639, 2021. [42] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In
ICML, pages 3925â€“3934."
What is the title of the paper by Zhizhong Li and Derek Hoiem in 2017?,"Learning without forgetting.IEEE PAMI, 40(12): 2935â€“2947, 2017.","Learning without forgetting.IEEE PAMI, 40(12): 2935â€“2947, 2017.

Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710.

PMLR, 2019. 14THENGANE ET AL.: CLIMB-3D
[43] Zhizhong Li and Derek Hoiem. Learning without forgetting.IEEE PAMI, 40(12):
2935â€“2947, 2017. [44] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmenta-
tion in 3d scenes using semantic superpoint tree networks. InICCV, pages 2783â€“2792,
2021. [45] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss
for dense object detection. InICCV, pages 2980â€“2988, 2017.

[60] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming
catastrophic forgetting with hard attention to the task. InICML, pages 4548â€“4557."
Who authored the paper on instance segmentation in 3D scenes using semantic superpoint tree networks in ICCV 2021?,"Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmentation in 3d scenes using semantic superpoint tree networks. InICCV, pages 2783â€“2792, 2021.","PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019.

Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmentation in 3d scenes using semantic superpoint tree networks. InICCV, pages 2783â€“2792, 2021.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916."
Which conference featured the paper on focal loss for dense object detection in 2017?,"Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss for dense object detection. InICCV, pages 2980â€“2988, 2017.","[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021.

PMLR, 2019. 14THENGANE ET AL.: CLIMB-3D
[43] Zhizhong Li and Derek Hoiem. Learning without forgetting.IEEE PAMI, 40(12):
2935â€“2947, 2017. [44] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmenta-
tion in 3d scenes using semantic superpoint tree networks. InICCV, pages 2783â€“2792,
2021. [45] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss
for dense object detection. InICCV, pages 2980â€“2988, 2017.

Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss for dense object detection. InICCV, pages 2980â€“2988, 2017.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019."
What is the title of the paper by Shih-Hung Liu et al.?,"Shih-Hung Liu, Shang-Yi Yu, Shao-Chi Wu, Hwann-Tzong Chen, and Tyng-Luh Liu. Learning gaussian instance segmentation in point clouds.","[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

Shih-Hung Liu, Shang-Yi Yu, Shao-Chi Wu, Hwann-Tzong Chen, and Tyng-Luh Liu. Learning gaussian instance segmentation in point clouds."
In which year was the paper by Yaoyao Liu et al. published?,"Yaoyao Liu, Bernt Schiele, Andrea Vedaldi, and Christian Rupprecht. Continual detection transformer for incremental object detection. InCVPR, pages 23799â€“23808, 2023.","Yaoyao Liu, Bernt Schiele, Andrea Vedaldi, and Christian Rupprecht. Continual detection transformer for incremental object detection. InCVPR, pages 23799â€“23808, 2023.

[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021.

[46] Shih-Hung Liu, Shang-Yi Yu, Shao-Chi Wu, Hwann-Tzong Chen, and Tyng-Luh
Liu. Learning gaussian instance segmentation in point clouds.arXiv preprint
arXiv:2007.09860, 2020. [47] Yaoyao Liu, Bernt Schiele, Andrea Vedaldi, and Christian Rupprecht. Continual de-
tection transformer for incremental object detection. InCVPR, pages 23799â€“23808,
2023. [48] Yuyang Liu, Yang Cong, Gan Sun, Tao Zhang, Jiahua Dong, and Hongsen Liu. L3doc:
Lifelong 3d object classification.ICIP, 30:7486â€“7498, 2021. [49] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X
Yu. Large-scale long-tailed recognition in an open world. InCVPR, pages 2537â€“2546,
2019. [50] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of
weakly supervised pretraining. InECCV, pages 181â€“196, 2018. [51] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist net-
works: The sequential learning problem. InPsychology of learning and motivation,
volume 24, pages 109â€“165.

[52] Seulki Park, Youngkyu Hong, Byeongho Heo, Sangdoo Yun, and Jin Young Choi. The
majority can help the minority: Context-rich minority oversampling for long-tailed
classification. InCVPR, pages 6887â€“6896, 2022. [53] Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan, and Ling
Shao. Random path selection for incremental learning.NeurIPS, 3, 2019. [54] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lam-
pert. icarl: Incremental classifier and representation learning. InCVPR, pages 2001â€“
2010, 2017. [55] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight
examples for robust deep learning. InICML, pages 4334â€“4343. PMLR, 2018. [56] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory
Wayne. Experience replay for continual learning.NeurIPS, 32, 2019. [57] David Rozenberszki, Or Litany, and Angela Dai. Language-grounded indoor 3d se-
mantic segmentation in the wild. InECCV, 2022. THENGANE ET AL.: CLIMB-3D15
[58] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirk-
patrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural
networks.arXiv preprint arXiv:1606.04671, 2016. [59] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bas-
tian Leibe. Mask3D: Mask Transformer for 3D Semantic Instance Segmentation. In
International Conference on Robotics and Automation (ICRA), 2023."
What is the title of the paper by Yuyang Liu et al.?,"Yuyang Liu, Yang Cong, Gan Sun, Tao Zhang, Jiahua Dong, and Hongsen Liu. L3doc: Lifelong 3d object classification.","[46] Shih-Hung Liu, Shang-Yi Yu, Shao-Chi Wu, Hwann-Tzong Chen, and Tyng-Luh
Liu. Learning gaussian instance segmentation in point clouds.arXiv preprint
arXiv:2007.09860, 2020. [47] Yaoyao Liu, Bernt Schiele, Andrea Vedaldi, and Christian Rupprecht. Continual de-
tection transformer for incremental object detection. InCVPR, pages 23799â€“23808,
2023. [48] Yuyang Liu, Yang Cong, Gan Sun, Tao Zhang, Jiahua Dong, and Hongsen Liu. L3doc:
Lifelong 3d object classification.ICIP, 30:7486â€“7498, 2021. [49] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X
Yu. Large-scale long-tailed recognition in an open world. InCVPR, pages 2537â€“2546,
2019. [50] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of
weakly supervised pretraining. InECCV, pages 181â€“196, 2018. [51] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist net-
works: The sequential learning problem. InPsychology of learning and motivation,
volume 24, pages 109â€“165.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

Yuyang Liu, Yang Cong, Gan Sun, Tao Zhang, Jiahua Dong, and Hongsen Liu. L3doc: Lifelong 3d object classification.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650."
Which conference did Ziwei Liu et al. present their paper at?,"Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X Yu. Large-scale long-tailed recognition in an open world. InCVPR, pages 2537â€“2546, 2019.","[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021.

Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X Yu. Large-scale long-tailed recognition in an open world. InCVPR, pages 2537â€“2546, 2019.

[46] Shih-Hung Liu, Shang-Yi Yu, Shao-Chi Wu, Hwann-Tzong Chen, and Tyng-Luh
Liu. Learning gaussian instance segmentation in point clouds.arXiv preprint
arXiv:2007.09860, 2020. [47] Yaoyao Liu, Bernt Schiele, Andrea Vedaldi, and Christian Rupprecht. Continual de-
tection transformer for incremental object detection. InCVPR, pages 23799â€“23808,
2023. [48] Yuyang Liu, Yang Cong, Gan Sun, Tao Zhang, Jiahua Dong, and Hongsen Liu. L3doc:
Lifelong 3d object classification.ICIP, 30:7486â€“7498, 2021. [49] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X
Yu. Large-scale long-tailed recognition in an open world. InCVPR, pages 2537â€“2546,
2019. [50] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of
weakly supervised pretraining. InECCV, pages 181â€“196, 2018. [51] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist net-
works: The sequential learning problem. InPsychology of learning and motivation,
volume 24, pages 109â€“165."
Who are the authors of the paper by Dhruv Mahajan et al.?,"Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining.","Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining.

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

[46] Shih-Hung Liu, Shang-Yi Yu, Shao-Chi Wu, Hwann-Tzong Chen, and Tyng-Luh
Liu. Learning gaussian instance segmentation in point clouds.arXiv preprint
arXiv:2007.09860, 2020. [47] Yaoyao Liu, Bernt Schiele, Andrea Vedaldi, and Christian Rupprecht. Continual de-
tection transformer for incremental object detection. InCVPR, pages 23799â€“23808,
2023. [48] Yuyang Liu, Yang Cong, Gan Sun, Tao Zhang, Jiahua Dong, and Hongsen Liu. L3doc:
Lifelong 3d object classification.ICIP, 30:7486â€“7498, 2021. [49] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X
Yu. Large-scale long-tailed recognition in an open world. InCVPR, pages 2537â€“2546,
2019. [50] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of
weakly supervised pretraining. InECCV, pages 181â€“196, 2018. [51] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist net-
works: The sequential learning problem. InPsychology of learning and motivation,
volume 24, pages 109â€“165.

[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021."
What is the topic of the paper by Michael McCloskey and Neal J Cohen?,Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem.,"Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem.

[60] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming
catastrophic forgetting with hard attention to the task. InICML, pages 4548â€“4557.

[46] Shih-Hung Liu, Shang-Yi Yu, Shao-Chi Wu, Hwann-Tzong Chen, and Tyng-Luh
Liu. Learning gaussian instance segmentation in point clouds.arXiv preprint
arXiv:2007.09860, 2020. [47] Yaoyao Liu, Bernt Schiele, Andrea Vedaldi, and Christian Rupprecht. Continual de-
tection transformer for incremental object detection. InCVPR, pages 23799â€“23808,
2023. [48] Yuyang Liu, Yang Cong, Gan Sun, Tao Zhang, Jiahua Dong, and Hongsen Liu. L3doc:
Lifelong 3d object classification.ICIP, 30:7486â€“7498, 2021. [49] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X
Yu. Large-scale long-tailed recognition in an open world. InCVPR, pages 2537â€“2546,
2019. [50] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of
weakly supervised pretraining. InECCV, pages 181â€“196, 2018. [51] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist net-
works: The sequential learning problem. InPsychology of learning and motivation,
volume 24, pages 109â€“165.

[30] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for
continual learning.arXiv preprint arXiv:1710.10368, 2017. [31] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence
of similar and dissimilar tasks.NeurIPS, 33:18493â€“18504, 2020. [32] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019. [33] Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and
Roberto Togneri. Cost-sensitive learning of deep feature representations from im-
balanced data.IEEE transactions on neural networks and learning systems, 29(8):
3573â€“3587, 2017. [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 33:18661â€“18673, 2020. [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-
jardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Pro-
ceedings of the national academy of sciences, 114(13):3521â€“3526, 2017. [36] Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual
learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024. [37] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3d instance
segmentation via multi-task metric learning. InICCV, pages 9256â€“9266, 2019. [38] Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and
Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint
and sampling. InInternational Conference on Robotics and Automation (ICRA), pages
5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029. [39] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,
and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022. [40] Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing
Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In
CVPR, pages 5212â€“5221, 2021. [41] Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-
tailed visual recognition. InICCV, pages 630â€“639, 2021. [42] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In
ICML, pages 3925â€“3934."
What is the publication year of Elsevier?,"Elsevier, 1989.","Elsevier, 1989.

Springer,
2016.

The copyright of this document resides with its authors.

Springer, 2017."
Which company is mentioned in the document chunk?,"Elsevier, 1989.","Springer, 2017.

Springer,
2016.

The copyright of this document resides with its authors.

Elsevier, 1989."
When was Elsevier established?,"Elsevier, 1989.","The copyright of this document resides with its authors.

Springer, 2017.

Elsevier, 1989.

Springer,
2016."
What is the significance of the year 1989 in the context of Elsevier?,"Elsevier, 1989.","Springer, 2017.

Elsevier, 1989.

The copyright of this document resides with its authors.

Springer,
2016."
Is there any additional information provided about Elsevier in the chunk?,"Elsevier, 1989.","Elsevier, 1989.

The copyright of this document resides with its authors.

Springer,
2016.

Springer, 2017."
What type of document is being referred to in the chunk?,"Elsevier, 1989.","The copyright of this document resides with its authors.

Springer, 2017.

Springer,
2016.

Elsevier, 1989."
Does the chunk mention any specific event related to Elsevier in 1989?,"Elsevier, 1989.","Elsevier, 1989.

Springer,
2016.

The copyright of this document resides with its authors.

Springer, 2017."
What is the format of the information provided about Elsevier in the chunk?,"Elsevier, 1989.","Elsevier, 1989.

The copyright of this document resides with its authors.

Springer,
2016.

Springer, 2017."
Can the year 1989 be associated with any milestone for Elsevier?,"Elsevier, 1989.","The copyright of this document resides with its authors.

Springer, 2017.

Elsevier, 1989.

Springer,
2016."
Is there any indication of the document's content or subject matter in the chunk?,"Elsevier, 1989.","Springer, 2017.

The copyright of this document resides with its authors.

Springer,
2016.

Elsevier, 1989."
"What is the title of the paper by Seulki Park, Youngkyu Hong, Byeongho Heo, Sangdoo Yun, and Jin Young Choi?","Seulki Park, Youngkyu Hong, Byeongho Heo, Sangdoo Yun, and Jin Young Choi. The majority can help the minority: Context-rich minority oversampling for long-tailed classification.","[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021.

[52] Seulki Park, Youngkyu Hong, Byeongho Heo, Sangdoo Yun, and Jin Young Choi. The
majority can help the minority: Context-rich minority oversampling for long-tailed
classification. InCVPR, pages 6887â€“6896, 2022. [53] Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan, and Ling
Shao. Random path selection for incremental learning.NeurIPS, 3, 2019. [54] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lam-
pert. icarl: Incremental classifier and representation learning. InCVPR, pages 2001â€“
2010, 2017. [55] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight
examples for robust deep learning. InICML, pages 4334â€“4343. PMLR, 2018. [56] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory
Wayne. Experience replay for continual learning.NeurIPS, 32, 2019. [57] David Rozenberszki, Or Litany, and Angela Dai. Language-grounded indoor 3d se-
mantic segmentation in the wild. InECCV, 2022. THENGANE ET AL.: CLIMB-3D15
[58] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirk-
patrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural
networks.arXiv preprint arXiv:1606.04671, 2016. [59] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bas-
tian Leibe. Mask3D: Mask Transformer for 3D Semantic Instance Segmentation. In
International Conference on Robotics and Automation (ICRA), 2023.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

Seulki Park, Youngkyu Hong, Byeongho Heo, Sangdoo Yun, and Jin Young Choi. The majority can help the minority: Context-rich minority oversampling for long-tailed classification."
"In which conference was the paper by Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan, and Ling Shao presented?","Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan, and Ling Shao. Random path selection for incremental learning. NeurIPS, 3, 2019.","Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan, and Ling Shao. Random path selection for incremental learning. NeurIPS, 3, 2019.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

[30] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for
continual learning.arXiv preprint arXiv:1710.10368, 2017. [31] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence
of similar and dissimilar tasks.NeurIPS, 33:18493â€“18504, 2020. [32] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019. [33] Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and
Roberto Togneri. Cost-sensitive learning of deep feature representations from im-
balanced data.IEEE transactions on neural networks and learning systems, 29(8):
3573â€“3587, 2017. [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 33:18661â€“18673, 2020. [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-
jardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Pro-
ceedings of the national academy of sciences, 114(13):3521â€“3526, 2017. [36] Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual
learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024. [37] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3d instance
segmentation via multi-task metric learning. InICCV, pages 9256â€“9266, 2019. [38] Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and
Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint
and sampling. InInternational Conference on Robotics and Automation (ICRA), pages
5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029. [39] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,
and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022. [40] Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing
Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In
CVPR, pages 5212â€“5221, 2021. [41] Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-
tailed visual recognition. InICCV, pages 630â€“639, 2021. [42] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In
ICML, pages 3925â€“3934.

[52] Seulki Park, Youngkyu Hong, Byeongho Heo, Sangdoo Yun, and Jin Young Choi. The
majority can help the minority: Context-rich minority oversampling for long-tailed
classification. InCVPR, pages 6887â€“6896, 2022. [53] Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan, and Ling
Shao. Random path selection for incremental learning.NeurIPS, 3, 2019. [54] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lam-
pert. icarl: Incremental classifier and representation learning. InCVPR, pages 2001â€“
2010, 2017. [55] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight
examples for robust deep learning. InICML, pages 4334â€“4343. PMLR, 2018. [56] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory
Wayne. Experience replay for continual learning.NeurIPS, 32, 2019. [57] David Rozenberszki, Or Litany, and Angela Dai. Language-grounded indoor 3d se-
mantic segmentation in the wild. InECCV, 2022. THENGANE ET AL.: CLIMB-3D15
[58] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirk-
patrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural
networks.arXiv preprint arXiv:1606.04671, 2016. [59] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bas-
tian Leibe. Mask3D: Mask Transformer for 3D Semantic Instance Segmentation. In
International Conference on Robotics and Automation (ICRA), 2023."
"What is the title of the paper by Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert?","Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. InCVPR, pages 2001â€“2010, 2017.","[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

[52] Seulki Park, Youngkyu Hong, Byeongho Heo, Sangdoo Yun, and Jin Young Choi. The
majority can help the minority: Context-rich minority oversampling for long-tailed
classification. InCVPR, pages 6887â€“6896, 2022. [53] Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan, and Ling
Shao. Random path selection for incremental learning.NeurIPS, 3, 2019. [54] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lam-
pert. icarl: Incremental classifier and representation learning. InCVPR, pages 2001â€“
2010, 2017. [55] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight
examples for robust deep learning. InICML, pages 4334â€“4343. PMLR, 2018. [56] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory
Wayne. Experience replay for continual learning.NeurIPS, 32, 2019. [57] David Rozenberszki, Or Litany, and Angela Dai. Language-grounded indoor 3d se-
mantic segmentation in the wild. InECCV, 2022. THENGANE ET AL.: CLIMB-3D15
[58] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirk-
patrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural
networks.arXiv preprint arXiv:1606.04671, 2016. [59] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bas-
tian Leibe. Mask3D: Mask Transformer for 3D Semantic Instance Segmentation. In
International Conference on Robotics and Automation (ICRA), 2023.

Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. InCVPR, pages 2001â€“2010, 2017.

[46] Shih-Hung Liu, Shang-Yi Yu, Shao-Chi Wu, Hwann-Tzong Chen, and Tyng-Luh
Liu. Learning gaussian instance segmentation in point clouds.arXiv preprint
arXiv:2007.09860, 2020. [47] Yaoyao Liu, Bernt Schiele, Andrea Vedaldi, and Christian Rupprecht. Continual de-
tection transformer for incremental object detection. InCVPR, pages 23799â€“23808,
2023. [48] Yuyang Liu, Yang Cong, Gan Sun, Tao Zhang, Jiahua Dong, and Hongsen Liu. L3doc:
Lifelong 3d object classification.ICIP, 30:7486â€“7498, 2021. [49] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X
Yu. Large-scale long-tailed recognition in an open world. InCVPR, pages 2537â€“2546,
2019. [50] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of
weakly supervised pretraining. InECCV, pages 181â€“196, 2018. [51] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist net-
works: The sequential learning problem. InPsychology of learning and motivation,
volume 24, pages 109â€“165."
Who are the authors of the paper on learning to reweight examples for robust deep learning?,"Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. InICML, pages 4334â€“4343. PMLR, 2018.","[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021.

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. InICML, pages 4334â€“4343. PMLR, 2018."
"What is the topic of the paper by David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne?","David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience replay for continual learning. NeurIPS, 32, 2019.","[60] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming
catastrophic forgetting with hard attention to the task. InICML, pages 4548â€“4557.

David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience replay for continual learning. NeurIPS, 32, 2019.

[30] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for
continual learning.arXiv preprint arXiv:1710.10368, 2017. [31] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence
of similar and dissimilar tasks.NeurIPS, 33:18493â€“18504, 2020. [32] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019. [33] Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and
Roberto Togneri. Cost-sensitive learning of deep feature representations from im-
balanced data.IEEE transactions on neural networks and learning systems, 29(8):
3573â€“3587, 2017. [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 33:18661â€“18673, 2020. [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-
jardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Pro-
ceedings of the national academy of sciences, 114(13):3521â€“3526, 2017. [36] Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual
learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024. [37] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3d instance
segmentation via multi-task metric learning. InICCV, pages 9256â€“9266, 2019. [38] Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and
Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint
and sampling. InInternational Conference on Robotics and Automation (ICRA), pages
5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029. [39] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,
and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022. [40] Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing
Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In
CVPR, pages 5212â€“5221, 2021. [41] Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-
tailed visual recognition. InICCV, pages 630â€“639, 2021. [42] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In
ICML, pages 3925â€“3934.

[52] Seulki Park, Youngkyu Hong, Byeongho Heo, Sangdoo Yun, and Jin Young Choi. The
majority can help the minority: Context-rich minority oversampling for long-tailed
classification. InCVPR, pages 6887â€“6896, 2022. [53] Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan, and Ling
Shao. Random path selection for incremental learning.NeurIPS, 3, 2019. [54] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lam-
pert. icarl: Incremental classifier and representation learning. InCVPR, pages 2001â€“
2010, 2017. [55] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight
examples for robust deep learning. InICML, pages 4334â€“4343. PMLR, 2018. [56] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory
Wayne. Experience replay for continual learning.NeurIPS, 32, 2019. [57] David Rozenberszki, Or Litany, and Angela Dai. Language-grounded indoor 3d se-
mantic segmentation in the wild. InECCV, 2022. THENGANE ET AL.: CLIMB-3D15
[58] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirk-
patrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural
networks.arXiv preprint arXiv:1606.04671, 2016. [59] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bas-
tian Leibe. Mask3D: Mask Transformer for 3D Semantic Instance Segmentation. In
International Conference on Robotics and Automation (ICRA), 2023."
In which conference was the paper on language-grounded indoor 3D semantic segmentation in the wild presented?,"David Rozenberszki, Or Litany, and Angela Dai. Language-grounded indoor 3D semantic segmentation in the wild. InECCV, 2022.","[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

David Rozenberszki, Or Litany, and Angela Dai. Language-grounded indoor 3D semantic segmentation in the wild. InECCV, 2022.

[4] Mohamed El Amine Boudjoghra, Salwa Al Khatib, Jean Lahoud, Hisham Cholakkal,
Rao Anwer, Salman H Khan, and Fahad Shahbaz Khan. 3d indoor instance segmenta-
tion in an open-world.NeurIPS, 36, 2024. [5] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calder-
ara. Dark experience for general continual learning: a strong, simple baseline.NeurIPS,
33:15920â€“15930, 2020. [6] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning
imbalanced datasets with label-distribution-aware margin loss.NeurIPS, 32, 2019. [7] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrastive continual learning. In
ICCV, pages 9516â€“9525, 2021. [8] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique.Journal of artificial intelligence
research, 16:321â€“357, 2002. [9] Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and Xinggang Wang. Hierarchical
aggregation for 3d instance segmentation. InICCV, pages 15467â€“15476, 2021. [10] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification is not
all you need for semantic segmentation.NeurIPS, 34:17864â€“17875, 2021. [11] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Gird-
har. Masked-attention mask transformer for universal image segmentation. InCVPR,
pages 1290â€“1299, 2022. [12] Julian Chibane, Francis Engelmann, Tuan Anh Tran, and Gerard Pons-Moll. Box2mask: Weakly supervised 3d semantic instance segmentation using bounding
boxes. InECCV, pages 681â€“699."
"What is the title of the paper by Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell?","Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.","Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.

PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective
learning of deep convolutional neural networks. InECCV, pages 467â€“482.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

[52] Seulki Park, Youngkyu Hong, Byeongho Heo, Sangdoo Yun, and Jin Young Choi. The
majority can help the minority: Context-rich minority oversampling for long-tailed
classification. InCVPR, pages 6887â€“6896, 2022. [53] Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan, and Ling
Shao. Random path selection for incremental learning.NeurIPS, 3, 2019. [54] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lam-
pert. icarl: Incremental classifier and representation learning. InCVPR, pages 2001â€“
2010, 2017. [55] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight
examples for robust deep learning. InICML, pages 4334â€“4343. PMLR, 2018. [56] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory
Wayne. Experience replay for continual learning.NeurIPS, 32, 2019. [57] David Rozenberszki, Or Litany, and Angela Dai. Language-grounded indoor 3d se-
mantic segmentation in the wild. InECCV, 2022. THENGANE ET AL.: CLIMB-3D15
[58] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirk-
patrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural
networks.arXiv preprint arXiv:1606.04671, 2016. [59] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bas-
tian Leibe. Mask3D: Mask Transformer for 3D Semantic Instance Segmentation. In
International Conference on Robotics and Automation (ICRA), 2023."
Who are the authors of the paper on Mask3D: Mask Transformer for 3D Semantic Instance Segmentation?,"Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bastian Leibe. Mask3D: Mask Transformer for 3D Semantic Instance Segmentation. In International Conference on Robotics and Automation (ICRA), 2023.","[4] Mohamed El Amine Boudjoghra, Salwa Al Khatib, Jean Lahoud, Hisham Cholakkal,
Rao Anwer, Salman H Khan, and Fahad Shahbaz Khan. 3d indoor instance segmenta-
tion in an open-world.NeurIPS, 36, 2024. [5] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calder-
ara. Dark experience for general continual learning: a strong, simple baseline.NeurIPS,
33:15920â€“15930, 2020. [6] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning
imbalanced datasets with label-distribution-aware margin loss.NeurIPS, 32, 2019. [7] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrastive continual learning. In
ICCV, pages 9516â€“9525, 2021. [8] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique.Journal of artificial intelligence
research, 16:321â€“357, 2002. [9] Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and Xinggang Wang. Hierarchical
aggregation for 3d instance segmentation. InICCV, pages 15467â€“15476, 2021. [10] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification is not
all you need for semantic segmentation.NeurIPS, 34:17864â€“17875, 2021. [11] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Gird-
har. Masked-attention mask transformer for universal image segmentation. InCVPR,
pages 1290â€“1299, 2022. [12] Julian Chibane, Francis Engelmann, Tuan Anh Tran, and Gerard Pons-Moll. Box2mask: Weakly supervised 3d semantic instance segmentation using bounding
boxes. InECCV, pages 681â€“699.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bastian Leibe. Mask3D: Mask Transformer for 3D Semantic Instance Segmentation. In International Conference on Robotics and Automation (ICRA), 2023."
Who are the authors of the paper titled 'Overcoming catastrophic forgetting with hard attention to the task'?,"Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou","The copyright of this document resides with its authors.

Springer,
2016.

[60] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming
catastrophic forgetting with hard attention to the task. InICML, pages 4548â€“4557.

Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou"
In which conference was the paper 'Overcoming catastrophic forgetting with hard attention to the task' presented?,ICML,"arXiv:2502.17429v3  [cs.CV]  21 Nov 2025
THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced
3D Instance Segmentation
Vishal Thengane1
v.thengane@surrey.ac.uk
Jean Lahoud2
jean.lahoud@mbzuai.ac.ae
Hisham Cholakkal2
hisham.cholakkal@mbzuai.ac.ae
Rao Muhammad Anwer2
rao.anwer@mbzuai.ac.ae
Lu Yin1
l.yin@surrey.ac.uk
Xiatian Zhu1
xiatian.zhu@surrey.ac.uk
Salman Khan2, 3
salman.khan@mbzuai.ac.ae
1 University of Surrey,
Guildford, UK
2 Mohamed bin Zayed University of
Artificial Intelligence,
Abu Dhabi, UAE
3 Australian National University,
Canberra, Australia
Abstract
While 3D instance segmentation (3DIS) has advanced significantly, most existing
methods assume that all object classes are known in advance and uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new
classes emerge gradually and exhibit natural imbalance. Although some approaches ad-
dress the emergence of new classes, they often overlook class imbalance, which leads
to suboptimal performance, particularly on rare categories. To tackle this, we propose
CLIMB-3D, a unified framework forCLass-incrementalImbalance-aware3DIS. Build-
ing upon established exemplar replay (ER) strategies, we show that ER alone is insuf-
ficient to achieve robust performance under memory constraints. To mitigate this, we
introduce a novel pseudo-label generator (PLG) that extends supervision to previously
learned categories by leveraging predictions from a frozen model trained on prior tasks. Despite its promise, PLG tends to be biased towards frequent classes. Therefore, we
propose a class-balanced re-weighting (CBR) scheme that estimates object frequencies
from pseudo-labels and dynamically adjusts training bias, without requiring access to
past data. We design and evaluate three incremental scenarios for 3DIS on the challeng-
ing ScanNet200 dataset and additionally validate our method for semantic segmentation
on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by
up to 16.76% mAP for instance segmentation and approximately 30% mIoU for semantic
segmentation, demonstrating strong generalisation across both frequent and rare classes. Code is available at:https://github.com/vgthengane/CLIMB3D. Â© 2025.

ICML

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

PMLR, 2019. 14THENGANE ET AL.: CLIMB-3D
[43] Zhizhong Li and Derek Hoiem. Learning without forgetting.IEEE PAMI, 40(12):
2935â€“2947, 2017. [44] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmenta-
tion in 3d scenes using semantic superpoint tree networks. InICCV, pages 2783â€“2792,
2021. [45] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss
for dense object detection. InICCV, pages 2980â€“2988, 2017."
What are the page numbers of the paper 'Overcoming catastrophic forgetting with hard attention to the task'?,4548â€“4557,"PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

[60] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming
catastrophic forgetting with hard attention to the task. InICML, pages 4548â€“4557.

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

4548â€“4557"
How many authors contributed to the paper 'Overcoming catastrophic forgetting with hard attention to the task'?,"Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou","[60] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming
catastrophic forgetting with hard attention to the task. InICML, pages 4548â€“4557.

Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou

Springer,
2016.

The copyright of this document resides with its authors."
What is the main focus of the paper 'Overcoming catastrophic forgetting with hard attention to the task'?,Overcoming catastrophic forgetting with hard attention to the task,"[30] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for
continual learning.arXiv preprint arXiv:1710.10368, 2017. [31] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence
of similar and dissimilar tasks.NeurIPS, 33:18493â€“18504, 2020. [32] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019. [33] Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and
Roberto Togneri. Cost-sensitive learning of deep feature representations from im-
balanced data.IEEE transactions on neural networks and learning systems, 29(8):
3573â€“3587, 2017. [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 33:18661â€“18673, 2020. [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-
jardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Pro-
ceedings of the national academy of sciences, 114(13):3521â€“3526, 2017. [36] Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual
learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024. [37] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3d instance
segmentation via multi-task metric learning. InICCV, pages 9256â€“9266, 2019. [38] Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and
Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint
and sampling. InInternational Conference on Robotics and Automation (ICRA), pages
5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029. [39] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,
and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022. [40] Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing
Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In
CVPR, pages 5212â€“5221, 2021. [41] Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-
tailed visual recognition. InICCV, pages 630â€“639, 2021. [42] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In
ICML, pages 3925â€“3934.

Overcoming catastrophic forgetting with hard attention to the task

Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710.

[60] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming
catastrophic forgetting with hard attention to the task. InICML, pages 4548â€“4557."
Who is the first author of the paper 'Overcoming catastrophic forgetting with hard attention to the task'?,Joan Serra,"[60] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming
catastrophic forgetting with hard attention to the task. InICML, pages 4548â€“4557.

Joan Serra

Springer, 2017.

Springer,
2016."
Which author is listed last in the paper 'Overcoming catastrophic forgetting with hard attention to the task'?,Alexandros Karatzoglou,"Alexandros Karatzoglou

[60] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming
catastrophic forgetting with hard attention to the task. InICML, pages 4548â€“4557.

Springer,
2016.

[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017."
What is the title of the paper that was presented in ICML on pages 4548â€“4557?,Overcoming catastrophic forgetting with hard attention to the task,"Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710.

[30] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for
continual learning.arXiv preprint arXiv:1710.10368, 2017. [31] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence
of similar and dissimilar tasks.NeurIPS, 33:18493â€“18504, 2020. [32] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019. [33] Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and
Roberto Togneri. Cost-sensitive learning of deep feature representations from im-
balanced data.IEEE transactions on neural networks and learning systems, 29(8):
3573â€“3587, 2017. [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 33:18661â€“18673, 2020. [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-
jardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Pro-
ceedings of the national academy of sciences, 114(13):3521â€“3526, 2017. [36] Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual
learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024. [37] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3d instance
segmentation via multi-task metric learning. InICCV, pages 9256â€“9266, 2019. [38] Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and
Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint
and sampling. InInternational Conference on Robotics and Automation (ICRA), pages
5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029. [39] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,
and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022. [40] Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing
Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In
CVPR, pages 5212â€“5221, 2021. [41] Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-
tailed visual recognition. InICCV, pages 630â€“639, 2021. [42] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In
ICML, pages 3925â€“3934.

[60] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming
catastrophic forgetting with hard attention to the task. InICML, pages 4548â€“4557.

Overcoming catastrophic forgetting with hard attention to the task"
How many pages does the paper 'Overcoming catastrophic forgetting with hard attention to the task' span?,4548â€“4557,"4548â€“4557

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

[60] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming
catastrophic forgetting with hard attention to the task. InICML, pages 4548â€“4557."
What is the specific topic addressed in the paper 'Overcoming catastrophic forgetting with hard attention to the task'?,Overcoming catastrophic forgetting with hard attention to the task,"[30] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for
continual learning.arXiv preprint arXiv:1710.10368, 2017. [31] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence
of similar and dissimilar tasks.NeurIPS, 33:18493â€“18504, 2020. [32] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019. [33] Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and
Roberto Togneri. Cost-sensitive learning of deep feature representations from im-
balanced data.IEEE transactions on neural networks and learning systems, 29(8):
3573â€“3587, 2017. [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 33:18661â€“18673, 2020. [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-
jardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Pro-
ceedings of the national academy of sciences, 114(13):3521â€“3526, 2017. [36] Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual
learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024. [37] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3d instance
segmentation via multi-task metric learning. InICCV, pages 9256â€“9266, 2019. [38] Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and
Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint
and sampling. InInternational Conference on Robotics and Automation (ICRA), pages
5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029. [39] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,
and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022. [40] Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing
Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In
CVPR, pages 5212â€“5221, 2021. [41] Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-
tailed visual recognition. InICCV, pages 630â€“639, 2021. [42] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In
ICML, pages 3925â€“3934.

Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710.

Overcoming catastrophic forgetting with hard attention to the task

[60] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming
catastrophic forgetting with hard attention to the task. InICML, pages 4548â€“4557."
"What is the title of the paper in PMLR 2018 by Li Shen, Zhouchen Lin, and Qingming Huang?","PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective learning of deep convolutional neural networks.","PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective learning of deep convolutional neural networks.

PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective
learning of deep convolutional neural networks. InECCV, pages 467â€“482.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

PMLR, 2019. 14THENGANE ET AL.: CLIMB-3D
[43] Zhizhong Li and Derek Hoiem. Learning without forgetting.IEEE PAMI, 40(12):
2935â€“2947, 2017. [44] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmenta-
tion in 3d scenes using semantic superpoint tree networks. InICCV, pages 2783â€“2792,
2021. [45] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss
for dense object detection. InICCV, pages 2980â€“2988, 2017."
Which conference is mentioned in the document chunk?,"PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective learning of deep convolutional neural networks. InECCV, pages 467â€“482.","PMLR, 2019. 14THENGANE ET AL.: CLIMB-3D
[43] Zhizhong Li and Derek Hoiem. Learning without forgetting.IEEE PAMI, 40(12):
2935â€“2947, 2017. [44] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmenta-
tion in 3d scenes using semantic superpoint tree networks. InICCV, pages 2783â€“2792,
2021. [45] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss
for dense object detection. InICCV, pages 2980â€“2988, 2017.

PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective
learning of deep convolutional neural networks. InECCV, pages 467â€“482.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective learning of deep convolutional neural networks. InECCV, pages 467â€“482."
Who are the authors of the paper mentioned in the document chunk?,"PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective learning of deep convolutional neural networks.","PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective
learning of deep convolutional neural networks. InECCV, pages 467â€“482.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective learning of deep convolutional neural networks.

PMLR, 2019. 14THENGANE ET AL.: CLIMB-3D
[43] Zhizhong Li and Derek Hoiem. Learning without forgetting.IEEE PAMI, 40(12):
2935â€“2947, 2017. [44] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmenta-
tion in 3d scenes using semantic superpoint tree networks. InICCV, pages 2783â€“2792,
2021. [45] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss
for dense object detection. InICCV, pages 2980â€“2988, 2017."
"What is the specific focus of the paper by Li Shen, Zhouchen Lin, and Qingming Huang?","PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective learning of deep convolutional neural networks.","[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective learning of deep convolutional neural networks.

PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective
learning of deep convolutional neural networks. InECCV, pages 467â€“482.

PMLR, 2019. 14THENGANE ET AL.: CLIMB-3D
[43] Zhizhong Li and Derek Hoiem. Learning without forgetting.IEEE PAMI, 40(12):
2935â€“2947, 2017. [44] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmenta-
tion in 3d scenes using semantic superpoint tree networks. InICCV, pages 2783â€“2792,
2021. [45] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss
for dense object detection. InICCV, pages 2980â€“2988, 2017."
What page range is mentioned for the paper in the document chunk?,"PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective learning of deep convolutional neural networks. InECCV, pages 467â€“482.","PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective
learning of deep convolutional neural networks. InECCV, pages 467â€“482.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

PMLR, 2019. 14THENGANE ET AL.: CLIMB-3D
[43] Zhizhong Li and Derek Hoiem. Learning without forgetting.IEEE PAMI, 40(12):
2935â€“2947, 2017. [44] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmenta-
tion in 3d scenes using semantic superpoint tree networks. InICCV, pages 2783â€“2792,
2021. [45] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss
for dense object detection. InICCV, pages 2980â€“2988, 2017.

PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective learning of deep convolutional neural networks. InECCV, pages 467â€“482."
"What is the main concept discussed in the paper by Li Shen, Zhouchen Lin, and Qingming Huang?","PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective learning of deep convolutional neural networks.","PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective learning of deep convolutional neural networks.

PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective
learning of deep convolutional neural networks. InECCV, pages 467â€“482.

PMLR, 2019. 14THENGANE ET AL.: CLIMB-3D
[43] Zhizhong Li and Derek Hoiem. Learning without forgetting.IEEE PAMI, 40(12):
2935â€“2947, 2017. [44] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmenta-
tion in 3d scenes using semantic superpoint tree networks. InICCV, pages 2783â€“2792,
2021. [45] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss
for dense object detection. InICCV, pages 2980â€“2988, 2017.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650."
"In what year was the paper by Li Shen, Zhouchen Lin, and Qingming Huang published?","PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective learning of deep convolutional neural networks.","PMLR, 2019. 14THENGANE ET AL.: CLIMB-3D
[43] Zhizhong Li and Derek Hoiem. Learning without forgetting.IEEE PAMI, 40(12):
2935â€“2947, 2017. [44] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmenta-
tion in 3d scenes using semantic superpoint tree networks. InICCV, pages 2783â€“2792,
2021. [45] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss
for dense object detection. InICCV, pages 2980â€“2988, 2017.

PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective learning of deep convolutional neural networks.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective
learning of deep convolutional neural networks. InECCV, pages 467â€“482."
What type of neural networks are the authors focusing on in their paper?,"PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective learning of deep convolutional neural networks.","PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective
learning of deep convolutional neural networks. InECCV, pages 467â€“482.

PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective learning of deep convolutional neural networks.

PMLR, 2019. 14THENGANE ET AL.: CLIMB-3D
[43] Zhizhong Li and Derek Hoiem. Learning without forgetting.IEEE PAMI, 40(12):
2935â€“2947, 2017. [44] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmenta-
tion in 3d scenes using semantic superpoint tree networks. InICCV, pages 2783â€“2792,
2021. [45] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss
for dense object detection. InICCV, pages 2980â€“2988, 2017.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650."
"What is the method proposed by Li Shen, Zhouchen Lin, and Qingming Huang for learning deep convolutional neural networks effectively?","PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective learning of deep convolutional neural networks.","PMLR, 2019. 14THENGANE ET AL.: CLIMB-3D
[43] Zhizhong Li and Derek Hoiem. Learning without forgetting.IEEE PAMI, 40(12):
2935â€“2947, 2017. [44] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmenta-
tion in 3d scenes using semantic superpoint tree networks. InICCV, pages 2783â€“2792,
2021. [45] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss
for dense object detection. InICCV, pages 2980â€“2988, 2017.

PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective
learning of deep convolutional neural networks. InECCV, pages 467â€“482.

PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective learning of deep convolutional neural networks.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650."
"What is the significance of relay backpropagation in the context of deep convolutional neural networks according to Li Shen, Zhouchen Lin, and Qingming Huang?","PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective learning of deep convolutional neural networks.","PMLR, 2019. 14THENGANE ET AL.: CLIMB-3D
[43] Zhizhong Li and Derek Hoiem. Learning without forgetting.IEEE PAMI, 40(12):
2935â€“2947, 2017. [44] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmenta-
tion in 3d scenes using semantic superpoint tree networks. InICCV, pages 2783â€“2792,
2021. [45] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Focal loss
for dense object detection. InICCV, pages 2980â€“2988, 2017.

PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective learning of deep convolutional neural networks.

PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective
learning of deep convolutional neural networks. InECCV, pages 467â€“482.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650."
What year was the document chunk published?,"Springer,
2016.","Elsevier, 1989.

Springer, 2017.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

Springer,
2016."
What is the name of the publisher mentioned in the chunk?,"Springer,","Elsevier, 1989.

Springer,

Springer, 2017.

Springer,
2016."
Is there any specific month mentioned in the chunk?,"Springer,
2016.","Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

Springer, 2017.

Elsevier, 1989.

Springer,
2016."
What is the significance of the comma after 'Springer'?,"Springer,","Springer, 2017.

Springer,
2016.

Elsevier, 1989.

Springer,"
Does the chunk provide any additional details about the publication year 2016?,"Springer,
2016.","Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

Springer,
2016.

Elsevier, 1989.

Springer, 2017."
Is there any indication of a specific publication within the year 2016?,"Springer,
2016.","Springer,
2016.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

Springer, 2017.

Elsevier, 1989."
What type of document is mentioned in the chunk?,"Springer,","Springer,

Springer, 2017.

Elsevier, 1989.

Springer,
2016."
Is there any mention of an author in the chunk?,"Springer,","Springer, 2017.

Elsevier, 1989.

Springer,

Springer,
2016."
Does the chunk provide any context about the content of the document?,"Springer,","Springer,

Springer, 2017.

Elsevier, 1989.

Springer,
2016."
Is there any reference to a specific edition in the chunk?,"Springer,","Springer,
2016.

Springer,

Springer, 2017.

Elsevier, 1989."
Who are the authors of Meta-weight-net?,"Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng.","[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020."
"What is the title of the paper by Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan?",Clip model is an efficient continual learner.,"[52] Seulki Park, Youngkyu Hong, Byeongho Heo, Sangdoo Yun, and Jin Young Choi. The
majority can help the minority: Context-rich minority oversampling for long-tailed
classification. InCVPR, pages 6887â€“6896, 2022. [53] Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan, and Ling
Shao. Random path selection for incremental learning.NeurIPS, 3, 2019. [54] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lam-
pert. icarl: Incremental classifier and representation learning. InCVPR, pages 2001â€“
2010, 2017. [55] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight
examples for robust deep learning. InICML, pages 4334â€“4343. PMLR, 2018. [56] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory
Wayne. Experience replay for continual learning.NeurIPS, 32, 2019. [57] David Rozenberszki, Or Litany, and Angela Dai. Language-grounded indoor 3d se-
mantic segmentation in the wild. InECCV, 2022. THENGANE ET AL.: CLIMB-3D15
[58] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirk-
patrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural
networks.arXiv preprint arXiv:1606.04671, 2016. [59] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bas-
tian Leibe. Mask3D: Mask Transformer for 3D Semantic Instance Segmentation. In
International Conference on Robotics and Automation (ICRA), 2023.

Clip model is an efficient continual learner.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

[30] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for
continual learning.arXiv preprint arXiv:1710.10368, 2017. [31] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence
of similar and dissimilar tasks.NeurIPS, 33:18493â€“18504, 2020. [32] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019. [33] Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and
Roberto Togneri. Cost-sensitive learning of deep feature representations from im-
balanced data.IEEE transactions on neural networks and learning systems, 29(8):
3573â€“3587, 2017. [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 33:18661â€“18673, 2020. [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-
jardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Pro-
ceedings of the national academy of sciences, 114(13):3521â€“3526, 2017. [36] Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual
learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024. [37] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3d instance
segmentation via multi-task metric learning. InICCV, pages 9256â€“9266, 2019. [38] Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and
Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint
and sampling. InInternational Conference on Robotics and Automation (ICRA), pages
5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029. [39] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,
and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022. [40] Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing
Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In
CVPR, pages 5212â€“5221, 2021. [41] Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-
tailed visual recognition. InICCV, pages 630â€“639, 2021. [42] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In
ICML, pages 3925â€“3934."
In which year was the paper 'Balanced residual distillation learning for 3d point cloud class-incremental semantic segmentation' published?,"arXiv preprint arXiv:2408.01356, 2024.","arXiv preprint arXiv:2408.01356, 2024.

Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710.

arXiv:2502.17429v3  [cs.CV]  21 Nov 2025
THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced
3D Instance Segmentation
Vishal Thengane1
v.thengane@surrey.ac.uk
Jean Lahoud2
jean.lahoud@mbzuai.ac.ae
Hisham Cholakkal2
hisham.cholakkal@mbzuai.ac.ae
Rao Muhammad Anwer2
rao.anwer@mbzuai.ac.ae
Lu Yin1
l.yin@surrey.ac.uk
Xiatian Zhu1
xiatian.zhu@surrey.ac.uk
Salman Khan2, 3
salman.khan@mbzuai.ac.ae
1 University of Surrey,
Guildford, UK
2 Mohamed bin Zayed University of
Artificial Intelligence,
Abu Dhabi, UAE
3 Australian National University,
Canberra, Australia
Abstract
While 3D instance segmentation (3DIS) has advanced significantly, most existing
methods assume that all object classes are known in advance and uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new
classes emerge gradually and exhibit natural imbalance. Although some approaches ad-
dress the emergence of new classes, they often overlook class imbalance, which leads
to suboptimal performance, particularly on rare categories. To tackle this, we propose
CLIMB-3D, a unified framework forCLass-incrementalImbalance-aware3DIS. Build-
ing upon established exemplar replay (ER) strategies, we show that ER alone is insuf-
ficient to achieve robust performance under memory constraints. To mitigate this, we
introduce a novel pseudo-label generator (PLG) that extends supervision to previously
learned categories by leveraging predictions from a frozen model trained on prior tasks. Despite its promise, PLG tends to be biased towards frequent classes. Therefore, we
propose a class-balanced re-weighting (CBR) scheme that estimates object frequencies
from pseudo-labels and dynamically adjusts training bias, without requiring access to
past data. We design and evaluate three incremental scenarios for 3DIS on the challeng-
ing ScanNet200 dataset and additionally validate our method for semantic segmentation
on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by
up to 16.76% mAP for instance segmentation and approximately 30% mIoU for semantic
segmentation, demonstrating strong generalisation across both frequent and rare classes. Code is available at:https://github.com/vgthengane/CLIMB3D. Â© 2025.

Springer,
2016."
Who are the authors of the paper 'Foundational models for 3d point clouds: A survey and outlook'?,"Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng Li.","PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng Li.

PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective
learning of deep convolutional neural networks. InECCV, pages 467â€“482."
"What is the title of the paper by Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie?",The inaturalist species classification and detection dataset.,"Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

The inaturalist species classification and detection dataset.

[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017.

[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021."
Who are the authors of the paper 'Attention is all you need'?,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin.","Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin.

[60] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming
catastrophic forgetting with hard attention to the task. InICML, pages 4548â€“4557.

[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916."
"What is the focus of the paper by Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang?",Adaptive class suppression loss for long-tail object detection.,"Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

Adaptive class suppression loss for long-tail object detection.

[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021.

It may be distributed unchanged freely in print or electronic forms. 2THENGANE ET AL.: CLIMB-3D
1 Introduction
Table Chair Couch Pillow Coffee Table Sofa Chair Recycling BinCabinet Unknown
Task-1 Task-2 Task-3
Figure 1:Overview of the CI-3DIS setting.New object categories are introduced incre-
mentally with each task. After every phase, the model must recognise both newly added
and previously learned classes. For instance, in Task 2, categories such asPillow,Coffee
Table, andSofa Chairare introduced, while the model is expected to retain recognition
of earlier classes likeTable,Chair, andCouch. 3D instance segmentation (3DIS) is a fundamental task in computer vision that involves
identifying precise object boundaries and class labels in 3D space, with broad applications in
graphics, robotics, and augmented reality [4, 39]. Traditional 3DIS methods, including top-
down [28, 70, 78], bottom-up [26, 73], and transformer-based approaches [59], perform well
under the assumption that all classes are available and balanced during training. However,
this assumption does not hold in real-world environments, where new object classes emerge
over time and exhibit natural imbalance. This gap motivates the need for Class-Incremental Learning (CIL) [17, 54], which sup-
ports learning new categories while retaining knowledge of previous ones [51]. While CIL
has seen success in 2D image tasks [1, 43, 54, 60], extensions to 3D point clouds remain
limited, often focusing on object-level classification [13, 18, 48]. Recent works on scene-
level class-incremental 3DIS (CI-3DIS) [4] and class-incremental semantic segmentation
(CI-3DSS) [74] show promise, but depend heavily on large exemplar [56] and overlook class
imbalance, limiting their practicality. To address this, we proposeCLIMB-3D, a unified framework forCLass-incremental
IMBalance-aware3DIS that jointly tackles catastrophic forgetting and class imbalance. The
framework begins with Exemplar Replay (ER), storing a small number of samples from
past classes for later replay. However, this alone does not yield promising results under the
strict memory constraints required in CI-3DIS. To address this, we introduce a Pseudo-Label
Generator (PLG), which uses a frozen model from previous task to generate supervision for
earlier classes. However, we observed that PLG tends to favour frequent classes while ig-
noring under-represented ones. To mitigate this, we propose a Class-Balanced Re-weighting
(CBR), which estimates class frequencies from the pseudo-labels to enhance the learning of
rare classes without accessing past data. Together, these components form a practical and
effective solution for real-world 3DIS. To evaluate CLIMB-3D, we designed three benchmark scenarios for CI-3DIS based on
the ScanNet200 dataset [57]. These scenarios simulate incremental learning setup under nat-
ural class imbalance, where new classes emerge based on (A) object frequency, (B) semantic
similarity, or (C) random grouping. Additionally, for comparison with existing methods,
we evaluate our approach on CI-3DSS using the ScanNetV2 dataset [16]. Experimental re-
sults show that our method significantly reduces forgetting and improves performance across
both frequent and rare categories, outperforming previous methods in both settings. Fig. 1
illustrates the CI-3DIS setup. THENGANE ET AL.: CLIMB-3D3
In summary, our contributions are: (i) a novel problem setting of imbalanced class-
incremental 3DIS with an effective method to balance learning and mitigate forgetting; (ii)
three benchmarks modelling continual object emergence with natural imbalance; and (iii)
strong experimental results, achieving up to 16.76% mAP improvement over baselines. 2 Related Work
3D Instance Segmentation.Various methods have been proposed for 3DIS. Grouping-
based approaches adopt a bottom-up pipeline that learns latent embeddings for point cluster-
ing [9, 21, 22, 29, 37, 44, 70, 78]. In contrast, proposal-based methods follow a top-down
strategy by detecting 3D boxes and segmenting objects within them [19, 26, 46, 73, 76]. Recently, transformer-based models [68] have also been applied to 3DIS and 3DSS [59, 64],
inspired by advances in 2D vision [10, 11]. However, these approaches require full anno-
tations for all classes and are not designed for progressive learning, where only new class
annotations are available and previous data is inaccessible. Another line of work aims to re-
duce annotation costs by proposing weakly supervised 3DIS methods based on sparse cues
[12, 27, 72]. While effective with limited annotations, these methods assume a fixed set of
classes and are susceptible to catastrophic forgetting in incremental settings. Incremental Learning.Continual learning involves training models sequentially to miti-
gate catastrophic forgetting. One common strategy is model regularisation, which constrains
parameter updates using techniques such as elastic weight consolidation or knowledge dis-
tillation [1, 24, 35, 43, 60]. Another approach is exemplar replay, where past data is stored
or generated to rehearse older tasks while learning new ones [5, 7, 30, 54]. A third direc-
tion involves dynamically expanding the model architecture or using modular sub-networks
to accommodate new tasks without interfering with old ones [31, 42, 53, 58, 71, 79]. Re-
cent efforts in class-incremental 3D segmentation [63, 74] have shown early promise but
often rely on basic architectures and focus primarily on semantic segmentation. Other works
[4, 36] explore continual learning in 3D settings, though they often depend on large memory
buffers. In contrast, our work introduces a tailored framework for 3D instance segmentation
that effectively transfers knowledge across tasks, even under class imbalance. Long-tailed Recognition.Imbalanced class distributions lead to poor recognition of rare
(long-tail) classes. To address this, re-sampling methods balance the data via input [8, 20, 50,
52, 61] or feature space [3, 14, 40], avoiding naÃ¯ve under-/over-sampling. Loss re-weighting
offers another direction, using class-based [6, 15, 23, 32, 33, 69] or per-example [45, 55, 62]
adjustments to ensure fair contribution. Parameter regularisation improves generalisation
through weight constraints [2], albeit requiring careful tuning. Other approaches leverage
transfer learning [77, 81], self-supervision [41, 75], or contrastive learning [34, 41, 82] to
improve rare-class representations. Long-tailed recognition is well-studied in 2D with large-
scale datasets [15, 49, 67]; in 3D, ScanNet200 [16, 57] enables similar efforts. Prior 3D
work focused on re-weighting, re-sampling, and transfer learning [57], while regularisation
remains underexplored. CeCo [80] balances class centres via auxiliary loss but overlooks
sampling and augmentation. Lahoud et al. [38] propose adaptive classifier regularisation
for 3D segmentation, outperforming prior approaches without threshold tuning. However,
neither method considers incremental learning. In contrast, we jointly address long-tail and
continual 3D semantic segmentation. 4THENGANE ET AL.: CLIMB-3D
Feature 
Backbone
Transformer 
Decoder
Feature 
Backbone
Transformer 
Decoder
(ð‘·,ð’€ð‘¡)
Current model (ð›·
ð‘¡
)
Pseudo Label Generator (PLG) 
Selection
à·¨ð‘Œð¾
ð‘¡
â„’CIâˆ’3DIS
à·¨ð‘Œð¾
ð‘¡,ð‘¤
à´¤ð‘Œð¾
ð‘¡
à· ð‘Œð‘¡
(ð·ð‘¡)
Current Dataset 
Exemplar 
Replay (ER)
+
~
Class-Balanced  
Re-weighting 
(CBR)
1
ð‘“(ð‘)
Figure 2: Overview ofCLIMB-3Dfor CI-3DIS. The model incrementally learns new classes
across sequential phases. During taskt, point cloudsPand their corresponding labelsY t are
sampled from a combination of the current training datasetD t and Exemplar Replay (ER),
which maintains a small memory of past examples. These are then passed to the current
modelÎ¦ t to produce predicted labelsY t . The Pseudo-Label Generator (PLG) selects the
top-Kpredictions from the previous modelÎ¦ tâˆ’1 . These pseudo-labels are then weighted
based on class frequencyf(c)using Class-Balanced Re-weighting (CBR), and the top-Kre-
weighted labels are selected to form the balanced pseudo-label set Â¯Yt . This pseudo-label set
is then concatenated with the ground-truth labels to form a final augmented supervision set
Yt for taskt, which is used to optimise the modelÎ¦ t using Eq. (3). 3 Methodology
3.1 Problem Formulation
3DIS.The objective of this task is to detect and segment each object instance within a point
cloud. Formally, the training dataset is denoted asD={(P i,Yi)}N
i=1, whereNis the number
of training samples. Each inputP i âˆˆR MÃ—6 is a coloured point cloud consisting ofMpoints,
where each point is represented by its 3D coordinates and RGB values. The corresponding
annotationY i ={(m i,j ,ci,j )}Ji
j=1 containsJ i object instances, wheremi,j âˆˆ {0,1}M is a binary
mask indicating which points belong to thej-th instance, andc i,j âˆˆ C={1,...,C}is the
semantic class label of that instance. GivenP i, the modelÎ¦predicts instance-level outputs
Ë†Yi ={(Ë†mi,j ,Ë†ci,j )}
Ë†Ji
j=1, where Ë†mi,j and Ë†ci,j denote the predicted mask and category label for
thej-th instance, respectively. The number of predicted instances Ë†Ji varies depending on the
modelâ€™s inference. The model is optimised using the following objective:
L3DIS(D;Î¦) = 1
|D| âˆ‘
(P,Y)âˆˆD
1
|Y| âˆ‘
(mj,cj)âˆˆY
(Lmask(mj,Ë†mj)+Î»Â·L cls(cj,Ë†cj)),(1)
whereL cls andL mask are the average classification and mask losses over all instances, andÎ»
controls their trade-off. CI-3DIS.Unlike conventional 3DIS, CI-3DIS involves sequential learning, where object
categories are introduced incrementally overTtraining tasks. Each tasktintroduces a dis-
joint set of classesC t , withC= ST
t=1 Ct andC t âˆ©C tâ€²
=/ 0 fortÌ¸=t â€². During each taskt, the THENGANE ET AL.: CLIMB-3D5
model receives a datasetD t ={(P i,Yt
i)}N
i=1, where each coloured point cloudP i âˆˆR MÃ—6
containsMpoints, and the corresponding annotationY t
i ={(m t
i,j ,ct
i,j )}
Jt
i
j=1 includes instance
masksm t
i,j âˆˆ {0,1}M and semantic class labelsc t
i,j âˆˆ Ct . The modelÎ¦ t , initialized from
Î¦tâˆ’1 , is trained onD t to predict instance-level outputs Ë†Yt
i ={(Ë†mt
i,j ,Ë†ct
i,j )}
Ë†Jt
i
j=1, where each
predicted semantic label Ë†ct
i,j belongs to the cumulative set of all classes observed so far:St
k=1 Ck. The key challenge in this setup is to learn new classes without forgetting those
encountered in previous tasks, despite limited supervision and the absence of past labels. 3.2 Method: CLIMB-3D
Overview.As shown in Fig. 2 and formulated in Sec. 3.1, the proposed framework for
CI-3DIS follows aphase-wisetraining strategy. In each phase, the model is exposed to a
carefully curated subset of the dataset, simulating real-world scenarios discussed in Sec. 3.3. Training naÃ¯vely on such phased data leads tocatastrophic forgetting[51], where the model
forgets prior knowledge when learning new tasks. To address this, we first incorporateEx-
emplar Replay(ER) [5], a strategy inspired by 2D incremental methods [1, 43, 54] and
recently extended to 3D settings [4], to mitigate forgetting by storing a small set of repre-
sentative samples from earlier phases. However, ER alone is insufficient to achieve robust
performance. To address this, two additional components are introduced: aPseudo-Label
Generator(PLG), which leverages a frozen model to generate supervision signals for previ-
ously seen classes, and aClass-Balanced Re-weighting(CBR) module that compensates for
class imbalance across phases. Each of these components is described in detail below 1. Exemplar Replay (ER).Inspired by Buzzega et al. [5], ER addresses the issue of catas-
trophic forgetting by allowing the model to retain a subset of data from earlier phases. During phaset, the model is trained not only on the current task dataD t , but also on a
set of exemplarsE 1:tâˆ’1, collected from previous phases. These exemplars are accumulated
incrementally:E 1:tâˆ’1 =E 1 âˆªÂ·Â·Â·âˆªE tâˆ’1 .Here,E t denotes the exemplar subset stored after
phaset, and|E t |denotes the exemplar replay size. The combined replay dataset is denoted
as:D t
ER =D t âˆªE 1:tâˆ’1.Epoch training consists of two stages: first, the model is trained on
the current data,D t ; then it is trained using exemplars fromE 1:tâˆ’1. While previous CI-3DIS approaches rely on large exemplar sets to mitigate forgetting [4],
such strategies become impractical under memory constraints. Consequently, although ER
aids knowledge retention, it is insufficient on its own for robust performance with a limited
exemplar budget. Therefore, PLG and CBR are introduced to better preserve past represen-
tations and enhance generalisation across phases. Pseudo-Label Generator (PLG).In the CI-3DIS setting, during phaset(wheret>1),
although ground-truth labels for previously seen classes are unavailable, the model from the
previous phaseÎ¦ tâˆ’1 , which preserves knowledge of past tasks, is retained. We use this
model to generate pseudo-labels for previously seen classes, thereby providing approximate
supervision during training. This allows the current modelÎ¦t , to retain prior knowledge and
reduce forgetting, even without access to ground-truth annotations. Given a point cloud and label pair(P,Y t ), fromD t , the previous modelÎ¦ tâˆ’1 , generates
pseudo-labels for the previously seen classes as Ë†Y1:tâˆ’1 =Î¦ tâˆ’1 (P), which we denote as ËœYt
1For clarity, modifications introduced by each component (ER, PLG, and CBR) are highlighted in green, blue,
and red, respectively. 6THENGANE ET AL.: CLIMB-3D
for brevity. However, we observe that utilising all predictions fromÎ¦tâˆ’1 introduces noisy or
incorrect labels, which degrade overall performance. To mitigate this, we select the top-K
most confident instance predictions, denoted as ËœYt
K, and concatenate them with the ground-
truth labels of the current task to obtain the supervision set: Â¯Yt =Y t âˆ¥ ËœYt
K. Following this,
the loss is computed over both the current taskâ€™s ground-truth labels and the top-Kconfi-
dent pseudo-labels from earlier phases, enabling the model to retain prior knowledge while
adapting to new classes. Class-Balanced Re-weighting (CBR).While ER and PLG contribute to preserving prior
knowledge, we observe that the modelâ€™s predictions are biased towards frequent classes,
resulting in the forgetting of rare categories. This issue is further amplified by the top-K
pseudo-labels selected by PLG, which predominantly represent dominant classes in the data. To address this, we propose aClass-Balanced Re-weighting(CBR) scheme that compensates
for class imbalance using object frequency statistics. At taskt, only the current datasetD t
and its label distributionp t (c)over classescâˆˆ Ct are available. Datasets and class distribu-
tions from earlier tasks{D i,p i(c)}tâˆ’1
i=1 are no longer accessible, which makes it challenging
to directly account for class imbalance across all previously seen tasks. Therefore, we pro-
pose to leverage the pseudo-label predictions of the frozen modelÎ¦ tâˆ’1 as a proxy for the
class distribution across previously learned categories. During each training iteration, the previous modelÎ¦ tâˆ’1 is applied to the current inputP
to generate pseudo-labels ËœYt , from which we accumulate class-wise frequency statistics for
previously learned classes. LetC1:t = St
i=1 Ci represent the union of all classes seen up to task
t. At the end of each epoch, we compute the overall class frequencyfâˆˆR |C1:t | by combining
the pseudo-label distribution Ëœpt (c)derived from ËœYt with the ground-truth label distribution
pt (c)from the current task. The resulting frequency vectorf= [f(c)] câˆˆC1:t is then used to
compute class-wise weightsw= [w(c)] câˆˆC1:t , which guide balanced pseudo-label selection
and help the model focus on rare classes. In the next epoch, the modelÎ¦ tâˆ’1 produces soft pseudo-label predictions ËœYt (as de-
scribed in PLG) , where each component ËœYt [c]denotes the confidence score for classcâˆˆ
C1:tâˆ’1. To promote balanced pseudo-label selection, class-wise re-weighting is applied using
w(c) = 1
f(c)+Îµ , whereÎµis a small constant for numerical stability. The adjusted predictions
are computed by applying the class-wise weights as: ËœYt,w [c] =w(c)Â· ËœYt [c],âˆ€câˆˆ C 1:tâˆ’1. Top-Kselection is then applied to both the original and the re-weighted scores, yielding
two pseudo-label sets: ËœYt
K and ËœYt,w
K , respectively. The final augmented target set, Yt , is
constructed by concatenating the ground-truth labelsY t with both sets of pseudo-labels:
Yt =Y t âˆ¥ ËœYt
K âˆ¥ ËœYt,w
K .(2)
Although the above re-weighting mitigates bias from prior tasks, it does not address class
imbalance within the current taskt. To resolve this, the class-balanced weightsw c are ex-
tended to incorporate statistics from both previously seen and current classes, i.e., overC 1:t
rather than onlyC 1:tâˆ’1. The updated weights, denoted asw â€²
c, are used to re-weights both
pseudo-label selection and classification loss, ensuring balanced supervision across all seen
categories, including rare ones. This unified strategy reduces bias and enhances performance
across phases. The pseudo-code for constructing the supervision set in Eq. (2), which incor-
porates both PLG and CBR components, is provided in Appendix A. Final Objective.The final training objective ofCLIMB-3D, incorporating ER, PLG, and THENGANE ET AL.: CLIMB-3D7
CBR, is formulated in Eq. (1) as:
L(Dt
ER;Î¦t ) = 1
|Dt
ER| âˆ‘
(P,Yt )âˆˆDt
ER
1
|Yt | âˆ‘
(mt
j,ct
j)âˆˆYt
 
Lmask(mt
j,Ë†mt
j)+w â€²â€²
c Â·Lcls(ct
j,Ë†ct
j)

,(3)
wherew â€²â€²
c =w â€²
c Â·Î» cls denotes the scaled weight vector, andÎ» cls is a hyperparameter for bal-
ancing the loss terms. 3.3 Benchmarking Incremental Scenarios
While CI-3DIS methods offer a wide range of practical applications, they frequently rely
on the assumption of uniform sample distribution, which rarely holds in real-world settings. In practice, the number of object categories, denoted byC, is often large and characterised
by substantial variability in category frequency, shape, structure, and size. To address these
challenges, we propose three incremental learning scenarios, each designed to capture a
different facet of real-world complexity. The design of these scenarios is detailed below; for
further information and illustrations, refer to Appendix E of the supplementary material. Split-A: Frequency Scenarios.This scenario acknowledges that datasets are often la-
belled based on the frequency of categories. To accommodate this, we propose a split where
the model learns from the most frequent categories and subsequently incorporates the less
frequent ones in later stages. By prioritising the training of frequently occurring categories,
the model can establish a strong foundation before expanding to handle rare categories. Split-B: Semantic Scenarios.Beyond frequency, semantic similarity is crucial in real-
world deployments. While objects often share visual or functional traits, models may en-
counter semantically different categories in new environments. To simulate this challenge,
we introduce theSplit-Bscenario. Here, categories are grouped based on their semantic
relationships, and the model is incrementally trained on one semantic group at a time. This
setup encourages the model to generalise across semantically similar categories and adapt
more effectively when exposed to new ones. Unlike theSplit-Ascenario, which organises
learning based on category frequency, theSplit-Bscenario may contain both frequent and
infrequent categories within a single task, focusing on semantic continuity and transfer. Split-C: Random Scenarios.In some cases, data labelling is driven by the availability of
objects rather than predefined criteria. To capture this, we introduce theSplit-Cscenario,
which represents a fully random setting where any class can appear in any task, resulting in
varying levels of class imbalance. By exposing the model to such diverse and unpredictable
distributions, we aim to improve its robustness in real-world situations where labelled data
availability is inconsistent. These incremental scenarios are designed to provide a more realistic representation of
object distributions, frequencies, and dynamics encountered in the real world. 4 Experiments
4.1 Setup
Datasets.We evaluateCLIMB-3Don ScanNet200 [57], which comprises 200 object cat-
egories and exhibits significant class imbalance, which makes it ideal for simulating and 8THENGANE ET AL.: CLIMB-3D
assessing real-world scenarios. In addition, we compare our approach against existing in-
cremental learning methods using ScanNetV2 [16] in the 3DSS setting. For this evaluation,
we follow the standard training and validation splits defined in prior works [74] to ensure
consistency with existing methods. Evaluation Metrics.We evaluate our method usingMean Average Precision(mAP), a
standard metric for 3DIS which provides a comprehensive measure of segmentation quality
by accounting for both precision and recall. For comparison with existing 3DSS methods,
we report themean Intersection over Union(mIoU), which quantifies the overlap between
predicted and ground truth segments. To assess the modelâ€™s ability to mitigate catastrophic
forgetting in incremental settings, we use theForgetting Percentage Points(FPP) metric, as
defined in [47]. FPP captures performance degradation by measuring the accuracy drop on
the initially seen categories between the first and final training phases. Detailed descriptions of the incremental scenarios (Split-A,Split-B,Split-C) and im-
plementation are provided in Appendix B. 4.2 Results and Discussion
Table 1: Comparison between the proposed method and the baseline in the 3DIS setting,
evaluated using mAP25, mAP50, mAP, and FPP after training across all phase. Scenarios Methods Average Precisionâ†‘FPPâ†“
mAP25 mAP50 mAP mAP 25 mAP50
Baseline 16.46 14.29 10.44 51.30 46.82Split-A CLIMB-3D(Ours) 35.69 31.05 22.72 3.44 2.63
Baseline 17.22 15.07 10.93 46.27 42.1Split-B CLIMB-3D(Ours) 35.48 31.56 23.69 8.00 5.51
Baseline 25.65 21.08 14.85 31.68 28.84Split-C CLIMB-3D(Ours) 31.59 26.78 18.93 9.10 7.89
To evaluate our proposed approach, we construct a baseline using ER [5] for the 3DIS
setting, as no existing incremental baselines are available for this task. In contrast, for the
3DSS setting, where incremental baselines do exist, we compare our method against three
prior approaches [43, 60, 74]. Results on CI-3DIS.Tab. 1 compares CLIMB-3D against the ER baseline across the three
CI-3DIS scenarios, evaluated after all phases using mAP 25, mAP50, overall mAP, and FPP. In theSplit-Ascenario, characterised by significant distribution shifts, our method achieves
gains of +19.23%, +16.76%, and +12.28% in mAP 25, mAP 50, and overall mAP, respec-
tively, while drastically reducing forgetting with FPP scores of 3.44% (mAP 25) and 2.63%
(mAP50), both over 45 points lower than the baseline. Under theSplit-Bscenario, with
semantically related new categories, improvements of +18.26%, +16.49%, and +12.76%
are observed alongside a reduction in forgetting to 8.00% and 5.51% from baseline levels
above 42%. For theSplit-Cscenario, featuring increasing geometric complexity, CLIMB-
3D maintains solid gains of +5.94%, +5.70%, and +4.08% across mAP metrics, with forget-
ting reduced by over 22 points. Overall, these results demonstrate robust forward transfer,
minimal forgetting, and stable learning, highlighting the methodâ€™s effectiveness for scalable
continual 3D semantic understanding. THENGANE ET AL.: CLIMB-3D9
Table 2: Comparison between the proposed method and existing baselines in the 3DSS set-
ting on ScanNetV2 [16], evaluated using mIoU. Methods Phase=1 Phase=2 All
EWC [60] 17.75 13.22 16.62
LwF [43] 30.38 13.37 26.13
Yanget al. [74] 34.16 13.43 28.98
CLIMB-3D(Ours) 69.39 32.56 59.38
Results on CI-3DSS.Although our primary focus is on CI-3DIS, we also evaluate our
method under the CI-3DSS setting to enable comparisons with existing approaches. To do
so, we adapt our predictions to assign each point the label corresponding to the highest-
confidence mask and exclude background classes (e.g., floor and wall), as these are not part
of the semantic segmentation targets. Tab. 2 presents a comparison between our method and
existing baselines on the ScanNet V2 dataset [16], evaluated using mIoU across two training
phases and overall. Although originally designed for instance segmentation, our method
generalises effectively to semantic segmentation, achieving substantial gains of +35.23%
mIoU in Phase 1 and +19.1% in Phase 2. Overall, it achieves 59.38% mIoU, significantly
higher than theâˆ¼30% mIoU of prior methods, highlighting its robustness and transferability. Refer to Appendix C, D, and F of the supplementary material for detailed analyses of per-
phase performance, rare-class evaluation, and qualitative result comparisons, respectively. 4.3 Ablation
Table 3: Ablation study illustrating the impact of each component in a three-phase setup. Each split (s) and corresponding number indicates data introduced at that phase. Best results
are highlighted inbold. Row Modules p=1â†‘p=2â†‘p=3â†‘ FPPâ†“
s1 s1 s2 All s1 s2 s3 All
1. Oracle - - - - 55.14 30.77 25.30 37.68 -
2. NaÃ¯ve 56.82 0.00 28.09 14.15 0.00 0.00 19.67 5.80 56.82
3. + ER 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28 46.44
4. + PLG 56.82 50.0034.3942.13 49.78 11.41 26.47 29.28 7.04
5. + CBR 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05 2.63
We conduct an ablation study to evaluate the contribution of each component in our
framework. As an upper bound, we report the performance of anOraclemodel, which is
trained jointly on the full dataset. For the incremental setting, we follow the previously
defined splits and first train the model naively across phases. We then incrementally add
each module to isolate its impact. Tab. 3 presents results for theSplit-Ascenario using both
mAP50 and FPP metrics. NaÃ¯ve Training.When trained naÃ¯vely without any dedicated modules, the model suffers
from severe catastrophic forgetting, as evident in row 2. The model entirely forgets pre-
viously learned classes upon entering new phases, with performance dropping to zero for
earlier splits and FPP rising to 56.82. 10THENGANE ET AL.: CLIMB-3D
Effect of ER.Adding exemplar replay (row 3) partially alleviates forgetting by maintaining
a buffer of past examples. This yields notable gains fors1in Phase 2 (+18.51%) and Phase 3
(+10.38%), and also improvess2in Phase 3 (+9.43%). However, the overall forgetting
remains substantial (FPP: 46.44), showing ER alone is insufficient. Effect of PLG.The addition of the pseudo-label generator (PLG), which generates labels for
previous classes by retaining a copy of the model from an earlier phase, facilitates knowledge
retention and forward transfer. As shown in row 4, PLG significantly reduces forgetting and
enhances performance on current tasks. Fors1, it improves mAP 50 by 31.49% in Phase 2
and 39.40% in Phase 3 over exemplar replay. Overall, PLG yields a 15.00% increase in
performance and reduces forgetting by 39.40%. Effect of CBR.Finally, class-balanced re-weighting (CBR) mitigates class imbalance dur-
ing both pseudo-labelling and current task learning by adjusting each classâ€™s contribution
based on its frequency. As shown in row 5, CBR enhancess1retention over PLG in Phases 2
and 3 (+4.67% and +4.41%), and further improves performance ons2ands3. It also achieves
the lowest FPP of 2.63, reflecting strong forgetting mitigation. Overall, CBR offers the best
trade-off between retaining past knowledge and acquiring new tasks, outperforming PLG
and ER by 4.41% and 43.81%, respectively. 4.4 Limitations & Future Work
While CLIMB-3D achieves strong performance across CI-3DIS and CI-3DSS settings, sev-
eral limitations remain and warrant discussion. The experiments are currently limited to
indoor datasets (ScanNet200 and ScanNetV2), therefore evaluating performance on outdoor
scenes would help assess generalisability across diverse environments. The current setup
also considers only three incremental phases, whereas real-world applications often involve
longer sequences; incorporating such extended setups would better reflect practical chal-
lenges. Moreover, the model operates in a uni-modal setting. Integrating multi-modal cues,
such as vision-language models, could further enhance performance, as suggested by recent
2D and 3D studies [25, 65, 66]. 5 Conclusion
This work addresses the challenge of catastrophic forgetting in class-incremental 3D in-
stance segmentation by introducing a modular framework that integrates exemplar replay
(ER), a pseudo-label generator (PLG), and class-balanced re-weighting (CBR). The pro-
posed method incrementally adapts to new classes while preserving prior knowledge, all
without requiring access to the full dataset. Extensive experiments on a three-phase bench-
mark validate the individual and combined effectiveness of each component: ER enables
efficient memory-based retention; PLG facilitates knowledge preservation through pseudo-
supervision; and CBR mitigates class imbalance to enhance learning stability. Together,
these components significantly reduce forgetting and improve segmentation performance
across all phases. By achieving a strong balance between stability and plasticity, our ap-
proach advances continual 3D scene understanding and sets a new baseline for future re-
search in this area. THENGANE ET AL.: CLIMB-3D11
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne
Tuytelaars. Memory aware synapses: Learning what (not) to forget. InECCV, pages
139â€“154, 2018. [2] Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long-tailed
recognition via weight balancing. InCVPR, pages 6897â€“6907, 2022. [3] Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying
imbalanced data. InJoint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 770â€“785."
Who are the authors of the paper 'Superpoint transformer for 3d scene instance segmentation'?,"Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu.","PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019."
"What type of learning is discussed in the paper by Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang?",Balanced residual distillation learning for 3d point cloud class-incremental semantic segmentation.,"Balanced residual distillation learning for 3d point cloud class-incremental semantic segmentation.

arXiv:2502.17429v3  [cs.CV]  21 Nov 2025
THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced
3D Instance Segmentation
Vishal Thengane1
v.thengane@surrey.ac.uk
Jean Lahoud2
jean.lahoud@mbzuai.ac.ae
Hisham Cholakkal2
hisham.cholakkal@mbzuai.ac.ae
Rao Muhammad Anwer2
rao.anwer@mbzuai.ac.ae
Lu Yin1
l.yin@surrey.ac.uk
Xiatian Zhu1
xiatian.zhu@surrey.ac.uk
Salman Khan2, 3
salman.khan@mbzuai.ac.ae
1 University of Surrey,
Guildford, UK
2 Mohamed bin Zayed University of
Artificial Intelligence,
Abu Dhabi, UAE
3 Australian National University,
Canberra, Australia
Abstract
While 3D instance segmentation (3DIS) has advanced significantly, most existing
methods assume that all object classes are known in advance and uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new
classes emerge gradually and exhibit natural imbalance. Although some approaches ad-
dress the emergence of new classes, they often overlook class imbalance, which leads
to suboptimal performance, particularly on rare categories. To tackle this, we propose
CLIMB-3D, a unified framework forCLass-incrementalImbalance-aware3DIS. Build-
ing upon established exemplar replay (ER) strategies, we show that ER alone is insuf-
ficient to achieve robust performance under memory constraints. To mitigate this, we
introduce a novel pseudo-label generator (PLG) that extends supervision to previously
learned categories by leveraging predictions from a frozen model trained on prior tasks. Despite its promise, PLG tends to be biased towards frequent classes. Therefore, we
propose a class-balanced re-weighting (CBR) scheme that estimates object frequencies
from pseudo-labels and dynamically adjusts training bias, without requiring access to
past data. We design and evaluate three incremental scenarios for 3DIS on the challeng-
ing ScanNet200 dataset and additionally validate our method for semantic segmentation
on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by
up to 16.76% mAP for instance segmentation and approximately 30% mIoU for semantic
segmentation, demonstrating strong generalisation across both frequent and rare classes. Code is available at:https://github.com/vgthengane/CLIMB3D. Â© 2025.

[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021.

Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019."
In which conference was the paper 'The inaturalist species classification and detection dataset' presented?,"InCVPR, pages 8769â€“8778, 2018.","IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

InCVPR, pages 8769â€“8778, 2018.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916."
"What is the title of the paper by Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann?",Sgpn: Similarity group proposal network for 3d point cloud instance segmentation,"Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

Sgpn: Similarity group proposal network for 3d point cloud instance segmentation"
"In which year was the paper by Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann published?","InProceedings of the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578, 2018","InProceedings of the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578, 2018

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591."
Who are the authors of the paper on 'Learn-prune-share for lifelong learning'?,"Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis Ioannidis","Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis Ioannidis

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017."
What is the topic of the paper on 'Learn-prune-share for lifelong learning'?,Learn-prune-share for lifelong learning,"Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

[30] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for
continual learning.arXiv preprint arXiv:1710.10368, 2017. [31] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence
of similar and dissimilar tasks.NeurIPS, 33:18493â€“18504, 2020. [32] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019. [33] Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and
Roberto Togneri. Cost-sensitive learning of deep feature representations from im-
balanced data.IEEE transactions on neural networks and learning systems, 29(8):
3573â€“3587, 2017. [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 33:18661â€“18673, 2020. [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-
jardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Pro-
ceedings of the national academy of sciences, 114(13):3521â€“3526, 2017. [36] Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual
learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024. [37] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3d instance
segmentation via multi-task metric learning. InICCV, pages 9256â€“9266, 2019. [38] Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and
Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint
and sampling. InInternational Conference on Robotics and Automation (ICRA), pages
5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029. [39] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,
and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022. [40] Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing
Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In
CVPR, pages 5212â€“5221, 2021. [41] Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-
tailed visual recognition. InICCV, pages 630â€“639, 2021. [42] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In
ICML, pages 3925â€“3934.

Learn-prune-share for lifelong learning"
Where was the paper on 'Learn-prune-share for lifelong learning' presented?,"In2020 IEEE International Conference on Data Mining (ICDM), pages 641â€“650","IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

In2020 IEEE International Conference on Data Mining (ICDM), pages 641â€“650

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019."
How many pages does the paper on 'Learn-prune-share for lifelong learning' span?,"In2020 IEEE International Conference on Data Mining (ICDM), pages 641â€“650","Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

In2020 IEEE International Conference on Data Mining (ICDM), pages 641â€“650

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650."
What is the title of the conference where the paper on 'Learn-prune-share for lifelong learning' was presented?,"In2020 IEEE International Conference on Data Mining (ICDM), pages 641â€“650","Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

In2020 IEEE International Conference on Data Mining (ICDM), pages 641â€“650

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591."
"Who are the authors of the paper by Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann?","Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann","Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann"
"What is the abbreviation 'Sgpn' in the paper title by Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann?",Sgpn: Similarity group proposal network for 3d point cloud instance segmentation,"[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

Sgpn: Similarity group proposal network for 3d point cloud instance segmentation

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019."
"What type of segmentation does the paper by Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann focus on?",Sgpn: Similarity group proposal network for 3d point cloud instance segmentation,"Sgpn: Similarity group proposal network for 3d point cloud instance segmentation

Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020."
"What is the title of the paper by Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany?",Pointcontrast: Unsupervised pre-training for 3d point cloud understanding,"Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

Pointcontrast: Unsupervised pre-training for 3d point cloud understanding

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591."
Who are the authors of the paper 'Pointcontrast: Unsupervised pre-training for 3d point cloud understanding'?,"Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany","PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017.

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany"
In which year was the paper 'Pointcontrast: Unsupervised pre-training for 3d point cloud understanding' published?,"IEEE, 2020","Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710.

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

IEEE, 2020"
What type of pre-training is mentioned in the document chunk?,Unsupervised pre-training for 3d point cloud understanding,"Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019.

Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710.

Unsupervised pre-training for 3d point cloud understanding

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591."
What aspect of 3D data is the pre-training in the document chunk focused on?,3d point cloud understanding,"IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

3d point cloud understanding

Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710.

Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019."
What conference is mentioned in the document chunk where the paper 'Pointcontrast: Unsupervised pre-training for 3d point cloud understanding' was presented?,ECCV,"ECCV

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

PMLR, 2018. [61] Li Shen, Zhouchen Lin, and Qingming Huang. Relay backpropagation for effective
learning of deep convolutional neural networks. InECCV, pages 467â€“482."
What are the page numbers associated with the paper 'Pointcontrast: Unsupervised pre-training for 3d point cloud understanding'?,pages 574â€“591,"pages 574â€“591

Springer,
2016.

Elsevier, 1989.

Springer, 2017."
Who is the first author of the paper 'Pointcontrast: Unsupervised pre-training for 3d point cloud understanding'?,Saining Xie,"IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

Saining Xie

[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019."
What is the main focus of the paper 'Pointcontrast: Unsupervised pre-training for 3d point cloud understanding'?,Unsupervised pre-training for 3d point cloud understanding,"Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019.

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

Unsupervised pre-training for 3d point cloud understanding

Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710."
"What is the topic of the paper by Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany?",Unsupervised pre-training for 3d point cloud understanding,"Unsupervised pre-training for 3d point cloud understanding

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019.

Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710."
"What is the title of the paper by Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham, and Niki Trigoni?",Learning object bounding boxes for 3d instance segmentation on point clouds.,"Learning object bounding boxes for 3d instance segmentation on point clouds.

Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019.

[4] Mohamed El Amine Boudjoghra, Salwa Al Khatib, Jean Lahoud, Hisham Cholakkal,
Rao Anwer, Salman H Khan, and Fahad Shahbaz Khan. 3d indoor instance segmenta-
tion in an open-world.NeurIPS, 36, 2024. [5] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calder-
ara. Dark experience for general continual learning: a strong, simple baseline.NeurIPS,
33:15920â€“15930, 2020. [6] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning
imbalanced datasets with label-distribution-aware margin loss.NeurIPS, 32, 2019. [7] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrastive continual learning. In
ICCV, pages 9516â€“9525, 2021. [8] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique.Journal of artificial intelligence
research, 16:321â€“357, 2002. [9] Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and Xinggang Wang. Hierarchical
aggregation for 3d instance segmentation. InICCV, pages 15467â€“15476, 2021. [10] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification is not
all you need for semantic segmentation.NeurIPS, 34:17864â€“17875, 2021. [11] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Gird-
har. Masked-attention mask transformer for universal image segmentation. InCVPR,
pages 1290â€“1299, 2022. [12] Julian Chibane, Francis Engelmann, Tuan Anh Tran, and Gerard Pons-Moll. Box2mask: Weakly supervised 3d semantic instance segmentation using bounding
boxes. InECCV, pages 681â€“699.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916."
"In which conference was the paper by Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei presented?","InCVPR, pages 21759â€“21768, June 2023.","IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

Springer, 2022. [13] Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, and Shafin Rahman. Learning
without forgetting for 3d point cloud objects. InAdvances in Computational Intel-
ligence: 16th International Work-Conference on Artificial Neural Networks, IWANN
2021, Virtual Event, June 16â€“18, 2021, Proceedings, Part I 16, pages 484â€“497. Springer, 2021. [14] Peng Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation for
long-tailed data. InECCV, pages 694â€“710.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

InCVPR, pages 21759â€“21768, June 2023."
What is the topic of the paper by Yuzhe Yang and Zhi Xu?,Rethinking the value of labels for improving class-imbalanced learning.,"[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

Rethinking the value of labels for improving class-imbalanced learning.

It may be distributed unchanged freely in print or electronic forms. 2THENGANE ET AL.: CLIMB-3D
1 Introduction
Table Chair Couch Pillow Coffee Table Sofa Chair Recycling BinCabinet Unknown
Task-1 Task-2 Task-3
Figure 1:Overview of the CI-3DIS setting.New object categories are introduced incre-
mentally with each task. After every phase, the model must recognise both newly added
and previously learned classes. For instance, in Task 2, categories such asPillow,Coffee
Table, andSofa Chairare introduced, while the model is expected to retain recognition
of earlier classes likeTable,Chair, andCouch. 3D instance segmentation (3DIS) is a fundamental task in computer vision that involves
identifying precise object boundaries and class labels in 3D space, with broad applications in
graphics, robotics, and augmented reality [4, 39]. Traditional 3DIS methods, including top-
down [28, 70, 78], bottom-up [26, 73], and transformer-based approaches [59], perform well
under the assumption that all classes are available and balanced during training. However,
this assumption does not hold in real-world environments, where new object classes emerge
over time and exhibit natural imbalance. This gap motivates the need for Class-Incremental Learning (CIL) [17, 54], which sup-
ports learning new categories while retaining knowledge of previous ones [51]. While CIL
has seen success in 2D image tasks [1, 43, 54, 60], extensions to 3D point clouds remain
limited, often focusing on object-level classification [13, 18, 48]. Recent works on scene-
level class-incremental 3DIS (CI-3DIS) [4] and class-incremental semantic segmentation
(CI-3DSS) [74] show promise, but depend heavily on large exemplar [56] and overlook class
imbalance, limiting their practicality. To address this, we proposeCLIMB-3D, a unified framework forCLass-incremental
IMBalance-aware3DIS that jointly tackles catastrophic forgetting and class imbalance. The
framework begins with Exemplar Replay (ER), storing a small number of samples from
past classes for later replay. However, this alone does not yield promising results under the
strict memory constraints required in CI-3DIS. To address this, we introduce a Pseudo-Label
Generator (PLG), which uses a frozen model from previous task to generate supervision for
earlier classes. However, we observed that PLG tends to favour frequent classes while ig-
noring under-represented ones. To mitigate this, we propose a Class-Balanced Re-weighting
(CBR), which estimates class frequencies from the pseudo-labels to enhance the learning of
rare classes without accessing past data. Together, these components form a practical and
effective solution for real-world 3DIS. To evaluate CLIMB-3D, we designed three benchmark scenarios for CI-3DIS based on
the ScanNet200 dataset [57]. These scenarios simulate incremental learning setup under nat-
ural class imbalance, where new classes emerge based on (A) object frequency, (B) semantic
similarity, or (C) random grouping. Additionally, for comparison with existing methods,
we evaluate our approach on CI-3DSS using the ScanNetV2 dataset [16]. Experimental re-
sults show that our method significantly reduces forgetting and improves performance across
both frequent and rare categories, outperforming previous methods in both settings. Fig. 1
illustrates the CI-3DIS setup. THENGANE ET AL.: CLIMB-3D3
In summary, our contributions are: (i) a novel problem setting of imbalanced class-
incremental 3DIS with an effective method to balance learning and mitigate forgetting; (ii)
three benchmarks modelling continual object emergence with natural imbalance; and (iii)
strong experimental results, achieving up to 16.76% mAP improvement over baselines. 2 Related Work
3D Instance Segmentation.Various methods have been proposed for 3DIS. Grouping-
based approaches adopt a bottom-up pipeline that learns latent embeddings for point cluster-
ing [9, 21, 22, 29, 37, 44, 70, 78]. In contrast, proposal-based methods follow a top-down
strategy by detecting 3D boxes and segmenting objects within them [19, 26, 46, 73, 76]. Recently, transformer-based models [68] have also been applied to 3DIS and 3DSS [59, 64],
inspired by advances in 2D vision [10, 11]. However, these approaches require full anno-
tations for all classes and are not designed for progressive learning, where only new class
annotations are available and previous data is inaccessible. Another line of work aims to re-
duce annotation costs by proposing weakly supervised 3DIS methods based on sparse cues
[12, 27, 72]. While effective with limited annotations, these methods assume a fixed set of
classes and are susceptible to catastrophic forgetting in incremental settings. Incremental Learning.Continual learning involves training models sequentially to miti-
gate catastrophic forgetting. One common strategy is model regularisation, which constrains
parameter updates using techniques such as elastic weight consolidation or knowledge dis-
tillation [1, 24, 35, 43, 60]. Another approach is exemplar replay, where past data is stored
or generated to rehearse older tasks while learning new ones [5, 7, 30, 54]. A third direc-
tion involves dynamically expanding the model architecture or using modular sub-networks
to accommodate new tasks without interfering with old ones [31, 42, 53, 58, 71, 79]. Re-
cent efforts in class-incremental 3D segmentation [63, 74] have shown early promise but
often rely on basic architectures and focus primarily on semantic segmentation. Other works
[4, 36] explore continual learning in 3D settings, though they often depend on large memory
buffers. In contrast, our work introduces a tailored framework for 3D instance segmentation
that effectively transfers knowledge across tasks, even under class imbalance. Long-tailed Recognition.Imbalanced class distributions lead to poor recognition of rare
(long-tail) classes. To address this, re-sampling methods balance the data via input [8, 20, 50,
52, 61] or feature space [3, 14, 40], avoiding naÃ¯ve under-/over-sampling. Loss re-weighting
offers another direction, using class-based [6, 15, 23, 32, 33, 69] or per-example [45, 55, 62]
adjustments to ensure fair contribution. Parameter regularisation improves generalisation
through weight constraints [2], albeit requiring careful tuning. Other approaches leverage
transfer learning [77, 81], self-supervision [41, 75], or contrastive learning [34, 41, 82] to
improve rare-class representations. Long-tailed recognition is well-studied in 2D with large-
scale datasets [15, 49, 67]; in 3D, ScanNet200 [16, 57] enables similar efforts. Prior 3D
work focused on re-weighting, re-sampling, and transfer learning [57], while regularisation
remains underexplored. CeCo [80] balances class centres via auxiliary loss but overlooks
sampling and augmentation. Lahoud et al. [38] propose adaptive classifier regularisation
for 3D segmentation, outperforming prior approaches without threshold tuning. However,
neither method considers incremental learning. In contrast, we jointly address long-tail and
continual 3D semantic segmentation. 4THENGANE ET AL.: CLIMB-3D
Feature 
Backbone
Transformer 
Decoder
Feature 
Backbone
Transformer 
Decoder
(ð‘·,ð’€ð‘¡)
Current model (ð›·
ð‘¡
)
Pseudo Label Generator (PLG) 
Selection
à·¨ð‘Œð¾
ð‘¡
â„’CIâˆ’3DIS
à·¨ð‘Œð¾
ð‘¡,ð‘¤
à´¤ð‘Œð¾
ð‘¡
à· ð‘Œð‘¡
(ð·ð‘¡)
Current Dataset 
Exemplar 
Replay (ER)
+
~
Class-Balanced  
Re-weighting 
(CBR)
1
ð‘“(ð‘)
Figure 2: Overview ofCLIMB-3Dfor CI-3DIS. The model incrementally learns new classes
across sequential phases. During taskt, point cloudsPand their corresponding labelsY t are
sampled from a combination of the current training datasetD t and Exemplar Replay (ER),
which maintains a small memory of past examples. These are then passed to the current
modelÎ¦ t to produce predicted labelsY t . The Pseudo-Label Generator (PLG) selects the
top-Kpredictions from the previous modelÎ¦ tâˆ’1 . These pseudo-labels are then weighted
based on class frequencyf(c)using Class-Balanced Re-weighting (CBR), and the top-Kre-
weighted labels are selected to form the balanced pseudo-label set Â¯Yt . This pseudo-label set
is then concatenated with the ground-truth labels to form a final augmented supervision set
Yt for taskt, which is used to optimise the modelÎ¦ t using Eq. (3). 3 Methodology
3.1 Problem Formulation
3DIS.The objective of this task is to detect and segment each object instance within a point
cloud. Formally, the training dataset is denoted asD={(P i,Yi)}N
i=1, whereNis the number
of training samples. Each inputP i âˆˆR MÃ—6 is a coloured point cloud consisting ofMpoints,
where each point is represented by its 3D coordinates and RGB values. The corresponding
annotationY i ={(m i,j ,ci,j )}Ji
j=1 containsJ i object instances, wheremi,j âˆˆ {0,1}M is a binary
mask indicating which points belong to thej-th instance, andc i,j âˆˆ C={1,...,C}is the
semantic class label of that instance. GivenP i, the modelÎ¦predicts instance-level outputs
Ë†Yi ={(Ë†mi,j ,Ë†ci,j )}
Ë†Ji
j=1, where Ë†mi,j and Ë†ci,j denote the predicted mask and category label for
thej-th instance, respectively. The number of predicted instances Ë†Ji varies depending on the
modelâ€™s inference. The model is optimised using the following objective:
L3DIS(D;Î¦) = 1
|D| âˆ‘
(P,Y)âˆˆD
1
|Y| âˆ‘
(mj,cj)âˆˆY
(Lmask(mj,Ë†mj)+Î»Â·L cls(cj,Ë†cj)),(1)
whereL cls andL mask are the average classification and mask losses over all instances, andÎ»
controls their trade-off. CI-3DIS.Unlike conventional 3DIS, CI-3DIS involves sequential learning, where object
categories are introduced incrementally overTtraining tasks. Each tasktintroduces a dis-
joint set of classesC t , withC= ST
t=1 Ct andC t âˆ©C tâ€²
=/ 0 fortÌ¸=t â€². During each taskt, the THENGANE ET AL.: CLIMB-3D5
model receives a datasetD t ={(P i,Yt
i)}N
i=1, where each coloured point cloudP i âˆˆR MÃ—6
containsMpoints, and the corresponding annotationY t
i ={(m t
i,j ,ct
i,j )}
Jt
i
j=1 includes instance
masksm t
i,j âˆˆ {0,1}M and semantic class labelsc t
i,j âˆˆ Ct . The modelÎ¦ t , initialized from
Î¦tâˆ’1 , is trained onD t to predict instance-level outputs Ë†Yt
i ={(Ë†mt
i,j ,Ë†ct
i,j )}
Ë†Jt
i
j=1, where each
predicted semantic label Ë†ct
i,j belongs to the cumulative set of all classes observed so far:St
k=1 Ck. The key challenge in this setup is to learn new classes without forgetting those
encountered in previous tasks, despite limited supervision and the absence of past labels. 3.2 Method: CLIMB-3D
Overview.As shown in Fig. 2 and formulated in Sec. 3.1, the proposed framework for
CI-3DIS follows aphase-wisetraining strategy. In each phase, the model is exposed to a
carefully curated subset of the dataset, simulating real-world scenarios discussed in Sec. 3.3. Training naÃ¯vely on such phased data leads tocatastrophic forgetting[51], where the model
forgets prior knowledge when learning new tasks. To address this, we first incorporateEx-
emplar Replay(ER) [5], a strategy inspired by 2D incremental methods [1, 43, 54] and
recently extended to 3D settings [4], to mitigate forgetting by storing a small set of repre-
sentative samples from earlier phases. However, ER alone is insufficient to achieve robust
performance. To address this, two additional components are introduced: aPseudo-Label
Generator(PLG), which leverages a frozen model to generate supervision signals for previ-
ously seen classes, and aClass-Balanced Re-weighting(CBR) module that compensates for
class imbalance across phases. Each of these components is described in detail below 1. Exemplar Replay (ER).Inspired by Buzzega et al. [5], ER addresses the issue of catas-
trophic forgetting by allowing the model to retain a subset of data from earlier phases. During phaset, the model is trained not only on the current task dataD t , but also on a
set of exemplarsE 1:tâˆ’1, collected from previous phases. These exemplars are accumulated
incrementally:E 1:tâˆ’1 =E 1 âˆªÂ·Â·Â·âˆªE tâˆ’1 .Here,E t denotes the exemplar subset stored after
phaset, and|E t |denotes the exemplar replay size. The combined replay dataset is denoted
as:D t
ER =D t âˆªE 1:tâˆ’1.Epoch training consists of two stages: first, the model is trained on
the current data,D t ; then it is trained using exemplars fromE 1:tâˆ’1. While previous CI-3DIS approaches rely on large exemplar sets to mitigate forgetting [4],
such strategies become impractical under memory constraints. Consequently, although ER
aids knowledge retention, it is insufficient on its own for robust performance with a limited
exemplar budget. Therefore, PLG and CBR are introduced to better preserve past represen-
tations and enhance generalisation across phases. Pseudo-Label Generator (PLG).In the CI-3DIS setting, during phaset(wheret>1),
although ground-truth labels for previously seen classes are unavailable, the model from the
previous phaseÎ¦ tâˆ’1 , which preserves knowledge of past tasks, is retained. We use this
model to generate pseudo-labels for previously seen classes, thereby providing approximate
supervision during training. This allows the current modelÎ¦t , to retain prior knowledge and
reduce forgetting, even without access to ground-truth annotations. Given a point cloud and label pair(P,Y t ), fromD t , the previous modelÎ¦ tâˆ’1 , generates
pseudo-labels for the previously seen classes as Ë†Y1:tâˆ’1 =Î¦ tâˆ’1 (P), which we denote as ËœYt
1For clarity, modifications introduced by each component (ER, PLG, and CBR) are highlighted in green, blue,
and red, respectively. 6THENGANE ET AL.: CLIMB-3D
for brevity. However, we observe that utilising all predictions fromÎ¦tâˆ’1 introduces noisy or
incorrect labels, which degrade overall performance. To mitigate this, we select the top-K
most confident instance predictions, denoted as ËœYt
K, and concatenate them with the ground-
truth labels of the current task to obtain the supervision set: Â¯Yt =Y t âˆ¥ ËœYt
K. Following this,
the loss is computed over both the current taskâ€™s ground-truth labels and the top-Kconfi-
dent pseudo-labels from earlier phases, enabling the model to retain prior knowledge while
adapting to new classes. Class-Balanced Re-weighting (CBR).While ER and PLG contribute to preserving prior
knowledge, we observe that the modelâ€™s predictions are biased towards frequent classes,
resulting in the forgetting of rare categories. This issue is further amplified by the top-K
pseudo-labels selected by PLG, which predominantly represent dominant classes in the data. To address this, we propose aClass-Balanced Re-weighting(CBR) scheme that compensates
for class imbalance using object frequency statistics. At taskt, only the current datasetD t
and its label distributionp t (c)over classescâˆˆ Ct are available. Datasets and class distribu-
tions from earlier tasks{D i,p i(c)}tâˆ’1
i=1 are no longer accessible, which makes it challenging
to directly account for class imbalance across all previously seen tasks. Therefore, we pro-
pose to leverage the pseudo-label predictions of the frozen modelÎ¦ tâˆ’1 as a proxy for the
class distribution across previously learned categories. During each training iteration, the previous modelÎ¦ tâˆ’1 is applied to the current inputP
to generate pseudo-labels ËœYt , from which we accumulate class-wise frequency statistics for
previously learned classes. LetC1:t = St
i=1 Ci represent the union of all classes seen up to task
t. At the end of each epoch, we compute the overall class frequencyfâˆˆR |C1:t | by combining
the pseudo-label distribution Ëœpt (c)derived from ËœYt with the ground-truth label distribution
pt (c)from the current task. The resulting frequency vectorf= [f(c)] câˆˆC1:t is then used to
compute class-wise weightsw= [w(c)] câˆˆC1:t , which guide balanced pseudo-label selection
and help the model focus on rare classes. In the next epoch, the modelÎ¦ tâˆ’1 produces soft pseudo-label predictions ËœYt (as de-
scribed in PLG) , where each component ËœYt [c]denotes the confidence score for classcâˆˆ
C1:tâˆ’1. To promote balanced pseudo-label selection, class-wise re-weighting is applied using
w(c) = 1
f(c)+Îµ , whereÎµis a small constant for numerical stability. The adjusted predictions
are computed by applying the class-wise weights as: ËœYt,w [c] =w(c)Â· ËœYt [c],âˆ€câˆˆ C 1:tâˆ’1. Top-Kselection is then applied to both the original and the re-weighted scores, yielding
two pseudo-label sets: ËœYt
K and ËœYt,w
K , respectively. The final augmented target set, Yt , is
constructed by concatenating the ground-truth labelsY t with both sets of pseudo-labels:
Yt =Y t âˆ¥ ËœYt
K âˆ¥ ËœYt,w
K .(2)
Although the above re-weighting mitigates bias from prior tasks, it does not address class
imbalance within the current taskt. To resolve this, the class-balanced weightsw c are ex-
tended to incorporate statistics from both previously seen and current classes, i.e., overC 1:t
rather than onlyC 1:tâˆ’1. The updated weights, denoted asw â€²
c, are used to re-weights both
pseudo-label selection and classification loss, ensuring balanced supervision across all seen
categories, including rare ones. This unified strategy reduces bias and enhances performance
across phases. The pseudo-code for constructing the supervision set in Eq. (2), which incor-
porates both PLG and CBR components, is provided in Appendix A. Final Objective.The final training objective ofCLIMB-3D, incorporating ER, PLG, and THENGANE ET AL.: CLIMB-3D7
CBR, is formulated in Eq. (1) as:
L(Dt
ER;Î¦t ) = 1
|Dt
ER| âˆ‘
(P,Yt )âˆˆDt
ER
1
|Yt | âˆ‘
(mt
j,ct
j)âˆˆYt
 
Lmask(mt
j,Ë†mt
j)+w â€²â€²
c Â·Lcls(ct
j,Ë†ct
j)

,(3)
wherew â€²â€²
c =w â€²
c Â·Î» cls denotes the scaled weight vector, andÎ» cls is a hyperparameter for bal-
ancing the loss terms. 3.3 Benchmarking Incremental Scenarios
While CI-3DIS methods offer a wide range of practical applications, they frequently rely
on the assumption of uniform sample distribution, which rarely holds in real-world settings. In practice, the number of object categories, denoted byC, is often large and characterised
by substantial variability in category frequency, shape, structure, and size. To address these
challenges, we propose three incremental learning scenarios, each designed to capture a
different facet of real-world complexity. The design of these scenarios is detailed below; for
further information and illustrations, refer to Appendix E of the supplementary material. Split-A: Frequency Scenarios.This scenario acknowledges that datasets are often la-
belled based on the frequency of categories. To accommodate this, we propose a split where
the model learns from the most frequent categories and subsequently incorporates the less
frequent ones in later stages. By prioritising the training of frequently occurring categories,
the model can establish a strong foundation before expanding to handle rare categories. Split-B: Semantic Scenarios.Beyond frequency, semantic similarity is crucial in real-
world deployments. While objects often share visual or functional traits, models may en-
counter semantically different categories in new environments. To simulate this challenge,
we introduce theSplit-Bscenario. Here, categories are grouped based on their semantic
relationships, and the model is incrementally trained on one semantic group at a time. This
setup encourages the model to generalise across semantically similar categories and adapt
more effectively when exposed to new ones. Unlike theSplit-Ascenario, which organises
learning based on category frequency, theSplit-Bscenario may contain both frequent and
infrequent categories within a single task, focusing on semantic continuity and transfer. Split-C: Random Scenarios.In some cases, data labelling is driven by the availability of
objects rather than predefined criteria. To capture this, we introduce theSplit-Cscenario,
which represents a fully random setting where any class can appear in any task, resulting in
varying levels of class imbalance. By exposing the model to such diverse and unpredictable
distributions, we aim to improve its robustness in real-world situations where labelled data
availability is inconsistent. These incremental scenarios are designed to provide a more realistic representation of
object distributions, frequencies, and dynamics encountered in the real world. 4 Experiments
4.1 Setup
Datasets.We evaluateCLIMB-3Don ScanNet200 [57], which comprises 200 object cat-
egories and exhibits significant class imbalance, which makes it ideal for simulating and 8THENGANE ET AL.: CLIMB-3D
assessing real-world scenarios. In addition, we compare our approach against existing in-
cremental learning methods using ScanNetV2 [16] in the 3DSS setting. For this evaluation,
we follow the standard training and validation splits defined in prior works [74] to ensure
consistency with existing methods. Evaluation Metrics.We evaluate our method usingMean Average Precision(mAP), a
standard metric for 3DIS which provides a comprehensive measure of segmentation quality
by accounting for both precision and recall. For comparison with existing 3DSS methods,
we report themean Intersection over Union(mIoU), which quantifies the overlap between
predicted and ground truth segments. To assess the modelâ€™s ability to mitigate catastrophic
forgetting in incremental settings, we use theForgetting Percentage Points(FPP) metric, as
defined in [47]. FPP captures performance degradation by measuring the accuracy drop on
the initially seen categories between the first and final training phases. Detailed descriptions of the incremental scenarios (Split-A,Split-B,Split-C) and im-
plementation are provided in Appendix B. 4.2 Results and Discussion
Table 1: Comparison between the proposed method and the baseline in the 3DIS setting,
evaluated using mAP25, mAP50, mAP, and FPP after training across all phase. Scenarios Methods Average Precisionâ†‘FPPâ†“
mAP25 mAP50 mAP mAP 25 mAP50
Baseline 16.46 14.29 10.44 51.30 46.82Split-A CLIMB-3D(Ours) 35.69 31.05 22.72 3.44 2.63
Baseline 17.22 15.07 10.93 46.27 42.1Split-B CLIMB-3D(Ours) 35.48 31.56 23.69 8.00 5.51
Baseline 25.65 21.08 14.85 31.68 28.84Split-C CLIMB-3D(Ours) 31.59 26.78 18.93 9.10 7.89
To evaluate our proposed approach, we construct a baseline using ER [5] for the 3DIS
setting, as no existing incremental baselines are available for this task. In contrast, for the
3DSS setting, where incremental baselines do exist, we compare our method against three
prior approaches [43, 60, 74]. Results on CI-3DIS.Tab. 1 compares CLIMB-3D against the ER baseline across the three
CI-3DIS scenarios, evaluated after all phases using mAP 25, mAP50, overall mAP, and FPP. In theSplit-Ascenario, characterised by significant distribution shifts, our method achieves
gains of +19.23%, +16.76%, and +12.28% in mAP 25, mAP 50, and overall mAP, respec-
tively, while drastically reducing forgetting with FPP scores of 3.44% (mAP 25) and 2.63%
(mAP50), both over 45 points lower than the baseline. Under theSplit-Bscenario, with
semantically related new categories, improvements of +18.26%, +16.49%, and +12.76%
are observed alongside a reduction in forgetting to 8.00% and 5.51% from baseline levels
above 42%. For theSplit-Cscenario, featuring increasing geometric complexity, CLIMB-
3D maintains solid gains of +5.94%, +5.70%, and +4.08% across mAP metrics, with forget-
ting reduced by over 22 points. Overall, these results demonstrate robust forward transfer,
minimal forgetting, and stable learning, highlighting the methodâ€™s effectiveness for scalable
continual 3D semantic understanding. THENGANE ET AL.: CLIMB-3D9
Table 2: Comparison between the proposed method and existing baselines in the 3DSS set-
ting on ScanNetV2 [16], evaluated using mIoU. Methods Phase=1 Phase=2 All
EWC [60] 17.75 13.22 16.62
LwF [43] 30.38 13.37 26.13
Yanget al. [74] 34.16 13.43 28.98
CLIMB-3D(Ours) 69.39 32.56 59.38
Results on CI-3DSS.Although our primary focus is on CI-3DIS, we also evaluate our
method under the CI-3DSS setting to enable comparisons with existing approaches. To do
so, we adapt our predictions to assign each point the label corresponding to the highest-
confidence mask and exclude background classes (e.g., floor and wall), as these are not part
of the semantic segmentation targets. Tab. 2 presents a comparison between our method and
existing baselines on the ScanNet V2 dataset [16], evaluated using mIoU across two training
phases and overall. Although originally designed for instance segmentation, our method
generalises effectively to semantic segmentation, achieving substantial gains of +35.23%
mIoU in Phase 1 and +19.1% in Phase 2. Overall, it achieves 59.38% mIoU, significantly
higher than theâˆ¼30% mIoU of prior methods, highlighting its robustness and transferability. Refer to Appendix C, D, and F of the supplementary material for detailed analyses of per-
phase performance, rare-class evaluation, and qualitative result comparisons, respectively. 4.3 Ablation
Table 3: Ablation study illustrating the impact of each component in a three-phase setup. Each split (s) and corresponding number indicates data introduced at that phase. Best results
are highlighted inbold. Row Modules p=1â†‘p=2â†‘p=3â†‘ FPPâ†“
s1 s1 s2 All s1 s2 s3 All
1. Oracle - - - - 55.14 30.77 25.30 37.68 -
2. NaÃ¯ve 56.82 0.00 28.09 14.15 0.00 0.00 19.67 5.80 56.82
3. + ER 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28 46.44
4. + PLG 56.82 50.0034.3942.13 49.78 11.41 26.47 29.28 7.04
5. + CBR 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05 2.63
We conduct an ablation study to evaluate the contribution of each component in our
framework. As an upper bound, we report the performance of anOraclemodel, which is
trained jointly on the full dataset. For the incremental setting, we follow the previously
defined splits and first train the model naively across phases. We then incrementally add
each module to isolate its impact. Tab. 3 presents results for theSplit-Ascenario using both
mAP50 and FPP metrics. NaÃ¯ve Training.When trained naÃ¯vely without any dedicated modules, the model suffers
from severe catastrophic forgetting, as evident in row 2. The model entirely forgets pre-
viously learned classes upon entering new phases, with performance dropping to zero for
earlier splits and FPP rising to 56.82. 10THENGANE ET AL.: CLIMB-3D
Effect of ER.Adding exemplar replay (row 3) partially alleviates forgetting by maintaining
a buffer of past examples. This yields notable gains fors1in Phase 2 (+18.51%) and Phase 3
(+10.38%), and also improvess2in Phase 3 (+9.43%). However, the overall forgetting
remains substantial (FPP: 46.44), showing ER alone is insufficient. Effect of PLG.The addition of the pseudo-label generator (PLG), which generates labels for
previous classes by retaining a copy of the model from an earlier phase, facilitates knowledge
retention and forward transfer. As shown in row 4, PLG significantly reduces forgetting and
enhances performance on current tasks. Fors1, it improves mAP 50 by 31.49% in Phase 2
and 39.40% in Phase 3 over exemplar replay. Overall, PLG yields a 15.00% increase in
performance and reduces forgetting by 39.40%. Effect of CBR.Finally, class-balanced re-weighting (CBR) mitigates class imbalance dur-
ing both pseudo-labelling and current task learning by adjusting each classâ€™s contribution
based on its frequency. As shown in row 5, CBR enhancess1retention over PLG in Phases 2
and 3 (+4.67% and +4.41%), and further improves performance ons2ands3. It also achieves
the lowest FPP of 2.63, reflecting strong forgetting mitigation. Overall, CBR offers the best
trade-off between retaining past knowledge and acquiring new tasks, outperforming PLG
and ER by 4.41% and 43.81%, respectively. 4.4 Limitations & Future Work
While CLIMB-3D achieves strong performance across CI-3DIS and CI-3DSS settings, sev-
eral limitations remain and warrant discussion. The experiments are currently limited to
indoor datasets (ScanNet200 and ScanNetV2), therefore evaluating performance on outdoor
scenes would help assess generalisability across diverse environments. The current setup
also considers only three incremental phases, whereas real-world applications often involve
longer sequences; incorporating such extended setups would better reflect practical chal-
lenges. Moreover, the model operates in a uni-modal setting. Integrating multi-modal cues,
such as vision-language models, could further enhance performance, as suggested by recent
2D and 3D studies [25, 65, 66]. 5 Conclusion
This work addresses the challenge of catastrophic forgetting in class-incremental 3D in-
stance segmentation by introducing a modular framework that integrates exemplar replay
(ER), a pseudo-label generator (PLG), and class-balanced re-weighting (CBR). The pro-
posed method incrementally adapts to new classes while preserving prior knowledge, all
without requiring access to the full dataset. Extensive experiments on a three-phase bench-
mark validate the individual and combined effectiveness of each component: ER enables
efficient memory-based retention; PLG facilitates knowledge preservation through pseudo-
supervision; and CBR mitigates class imbalance to enhance learning stability. Together,
these components significantly reduce forgetting and improve segmentation performance
across all phases. By achieving a strong balance between stability and plasticity, our ap-
proach advances continual 3D scene understanding and sets a new baseline for future re-
search in this area. THENGANE ET AL.: CLIMB-3D11
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne
Tuytelaars. Memory aware synapses: Learning what (not) to forget. InECCV, pages
139â€“154, 2018. [2] Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long-tailed
recognition via weight balancing. InCVPR, pages 6897â€“6907, 2022. [3] Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying
imbalanced data. InJoint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 770â€“785."
Who are the authors of the paper on Gspn: Generative shape proposal network for 3d instance segmentation in point cloud?,"Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas.","PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas.

Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650."
"What year was the paper by Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham, and Niki Trigoni published?","NeurIPS, 32, 2019.","[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021.

[60] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming
catastrophic forgetting with hard attention to the task. InICML, pages 4548â€“4557.

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

NeurIPS, 32, 2019."
"What is the page range of the paper by Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas?","InCVPR, pages 3947â€“3956, 2019.","IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

InCVPR, pages 3947â€“3956, 2019."
How many authors are listed for the paper on Geometry and uncertainty-aware 3d point cloud class-incremental semantic segmentation?,"Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei.","Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei.

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019."
What is the abbreviation NeurIPS stand for in the context of the document chunk?,"NeurIPS, 32, 2019.","NeurIPS, 32, 2019.

[60] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming
catastrophic forgetting with hard attention to the task. InICML, pages 4548â€“4557.

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021."
What type of segmentation is focused on in the paper by Yuzhe Yang and Zhi Xu?,Class-imbalanced learning.,"[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

It may be distributed unchanged freely in print or electronic forms. 2THENGANE ET AL.: CLIMB-3D
1 Introduction
Table Chair Couch Pillow Coffee Table Sofa Chair Recycling BinCabinet Unknown
Task-1 Task-2 Task-3
Figure 1:Overview of the CI-3DIS setting.New object categories are introduced incre-
mentally with each task. After every phase, the model must recognise both newly added
and previously learned classes. For instance, in Task 2, categories such asPillow,Coffee
Table, andSofa Chairare introduced, while the model is expected to retain recognition
of earlier classes likeTable,Chair, andCouch. 3D instance segmentation (3DIS) is a fundamental task in computer vision that involves
identifying precise object boundaries and class labels in 3D space, with broad applications in
graphics, robotics, and augmented reality [4, 39]. Traditional 3DIS methods, including top-
down [28, 70, 78], bottom-up [26, 73], and transformer-based approaches [59], perform well
under the assumption that all classes are available and balanced during training. However,
this assumption does not hold in real-world environments, where new object classes emerge
over time and exhibit natural imbalance. This gap motivates the need for Class-Incremental Learning (CIL) [17, 54], which sup-
ports learning new categories while retaining knowledge of previous ones [51]. While CIL
has seen success in 2D image tasks [1, 43, 54, 60], extensions to 3D point clouds remain
limited, often focusing on object-level classification [13, 18, 48]. Recent works on scene-
level class-incremental 3DIS (CI-3DIS) [4] and class-incremental semantic segmentation
(CI-3DSS) [74] show promise, but depend heavily on large exemplar [56] and overlook class
imbalance, limiting their practicality. To address this, we proposeCLIMB-3D, a unified framework forCLass-incremental
IMBalance-aware3DIS that jointly tackles catastrophic forgetting and class imbalance. The
framework begins with Exemplar Replay (ER), storing a small number of samples from
past classes for later replay. However, this alone does not yield promising results under the
strict memory constraints required in CI-3DIS. To address this, we introduce a Pseudo-Label
Generator (PLG), which uses a frozen model from previous task to generate supervision for
earlier classes. However, we observed that PLG tends to favour frequent classes while ig-
noring under-represented ones. To mitigate this, we propose a Class-Balanced Re-weighting
(CBR), which estimates class frequencies from the pseudo-labels to enhance the learning of
rare classes without accessing past data. Together, these components form a practical and
effective solution for real-world 3DIS. To evaluate CLIMB-3D, we designed three benchmark scenarios for CI-3DIS based on
the ScanNet200 dataset [57]. These scenarios simulate incremental learning setup under nat-
ural class imbalance, where new classes emerge based on (A) object frequency, (B) semantic
similarity, or (C) random grouping. Additionally, for comparison with existing methods,
we evaluate our approach on CI-3DSS using the ScanNetV2 dataset [16]. Experimental re-
sults show that our method significantly reduces forgetting and improves performance across
both frequent and rare categories, outperforming previous methods in both settings. Fig. 1
illustrates the CI-3DIS setup. THENGANE ET AL.: CLIMB-3D3
In summary, our contributions are: (i) a novel problem setting of imbalanced class-
incremental 3DIS with an effective method to balance learning and mitigate forgetting; (ii)
three benchmarks modelling continual object emergence with natural imbalance; and (iii)
strong experimental results, achieving up to 16.76% mAP improvement over baselines. 2 Related Work
3D Instance Segmentation.Various methods have been proposed for 3DIS. Grouping-
based approaches adopt a bottom-up pipeline that learns latent embeddings for point cluster-
ing [9, 21, 22, 29, 37, 44, 70, 78]. In contrast, proposal-based methods follow a top-down
strategy by detecting 3D boxes and segmenting objects within them [19, 26, 46, 73, 76]. Recently, transformer-based models [68] have also been applied to 3DIS and 3DSS [59, 64],
inspired by advances in 2D vision [10, 11]. However, these approaches require full anno-
tations for all classes and are not designed for progressive learning, where only new class
annotations are available and previous data is inaccessible. Another line of work aims to re-
duce annotation costs by proposing weakly supervised 3DIS methods based on sparse cues
[12, 27, 72]. While effective with limited annotations, these methods assume a fixed set of
classes and are susceptible to catastrophic forgetting in incremental settings. Incremental Learning.Continual learning involves training models sequentially to miti-
gate catastrophic forgetting. One common strategy is model regularisation, which constrains
parameter updates using techniques such as elastic weight consolidation or knowledge dis-
tillation [1, 24, 35, 43, 60]. Another approach is exemplar replay, where past data is stored
or generated to rehearse older tasks while learning new ones [5, 7, 30, 54]. A third direc-
tion involves dynamically expanding the model architecture or using modular sub-networks
to accommodate new tasks without interfering with old ones [31, 42, 53, 58, 71, 79]. Re-
cent efforts in class-incremental 3D segmentation [63, 74] have shown early promise but
often rely on basic architectures and focus primarily on semantic segmentation. Other works
[4, 36] explore continual learning in 3D settings, though they often depend on large memory
buffers. In contrast, our work introduces a tailored framework for 3D instance segmentation
that effectively transfers knowledge across tasks, even under class imbalance. Long-tailed Recognition.Imbalanced class distributions lead to poor recognition of rare
(long-tail) classes. To address this, re-sampling methods balance the data via input [8, 20, 50,
52, 61] or feature space [3, 14, 40], avoiding naÃ¯ve under-/over-sampling. Loss re-weighting
offers another direction, using class-based [6, 15, 23, 32, 33, 69] or per-example [45, 55, 62]
adjustments to ensure fair contribution. Parameter regularisation improves generalisation
through weight constraints [2], albeit requiring careful tuning. Other approaches leverage
transfer learning [77, 81], self-supervision [41, 75], or contrastive learning [34, 41, 82] to
improve rare-class representations. Long-tailed recognition is well-studied in 2D with large-
scale datasets [15, 49, 67]; in 3D, ScanNet200 [16, 57] enables similar efforts. Prior 3D
work focused on re-weighting, re-sampling, and transfer learning [57], while regularisation
remains underexplored. CeCo [80] balances class centres via auxiliary loss but overlooks
sampling and augmentation. Lahoud et al. [38] propose adaptive classifier regularisation
for 3D segmentation, outperforming prior approaches without threshold tuning. However,
neither method considers incremental learning. In contrast, we jointly address long-tail and
continual 3D semantic segmentation. 4THENGANE ET AL.: CLIMB-3D
Feature 
Backbone
Transformer 
Decoder
Feature 
Backbone
Transformer 
Decoder
(ð‘·,ð’€ð‘¡)
Current model (ð›·
ð‘¡
)
Pseudo Label Generator (PLG) 
Selection
à·¨ð‘Œð¾
ð‘¡
â„’CIâˆ’3DIS
à·¨ð‘Œð¾
ð‘¡,ð‘¤
à´¤ð‘Œð¾
ð‘¡
à· ð‘Œð‘¡
(ð·ð‘¡)
Current Dataset 
Exemplar 
Replay (ER)
+
~
Class-Balanced  
Re-weighting 
(CBR)
1
ð‘“(ð‘)
Figure 2: Overview ofCLIMB-3Dfor CI-3DIS. The model incrementally learns new classes
across sequential phases. During taskt, point cloudsPand their corresponding labelsY t are
sampled from a combination of the current training datasetD t and Exemplar Replay (ER),
which maintains a small memory of past examples. These are then passed to the current
modelÎ¦ t to produce predicted labelsY t . The Pseudo-Label Generator (PLG) selects the
top-Kpredictions from the previous modelÎ¦ tâˆ’1 . These pseudo-labels are then weighted
based on class frequencyf(c)using Class-Balanced Re-weighting (CBR), and the top-Kre-
weighted labels are selected to form the balanced pseudo-label set Â¯Yt . This pseudo-label set
is then concatenated with the ground-truth labels to form a final augmented supervision set
Yt for taskt, which is used to optimise the modelÎ¦ t using Eq. (3). 3 Methodology
3.1 Problem Formulation
3DIS.The objective of this task is to detect and segment each object instance within a point
cloud. Formally, the training dataset is denoted asD={(P i,Yi)}N
i=1, whereNis the number
of training samples. Each inputP i âˆˆR MÃ—6 is a coloured point cloud consisting ofMpoints,
where each point is represented by its 3D coordinates and RGB values. The corresponding
annotationY i ={(m i,j ,ci,j )}Ji
j=1 containsJ i object instances, wheremi,j âˆˆ {0,1}M is a binary
mask indicating which points belong to thej-th instance, andc i,j âˆˆ C={1,...,C}is the
semantic class label of that instance. GivenP i, the modelÎ¦predicts instance-level outputs
Ë†Yi ={(Ë†mi,j ,Ë†ci,j )}
Ë†Ji
j=1, where Ë†mi,j and Ë†ci,j denote the predicted mask and category label for
thej-th instance, respectively. The number of predicted instances Ë†Ji varies depending on the
modelâ€™s inference. The model is optimised using the following objective:
L3DIS(D;Î¦) = 1
|D| âˆ‘
(P,Y)âˆˆD
1
|Y| âˆ‘
(mj,cj)âˆˆY
(Lmask(mj,Ë†mj)+Î»Â·L cls(cj,Ë†cj)),(1)
whereL cls andL mask are the average classification and mask losses over all instances, andÎ»
controls their trade-off. CI-3DIS.Unlike conventional 3DIS, CI-3DIS involves sequential learning, where object
categories are introduced incrementally overTtraining tasks. Each tasktintroduces a dis-
joint set of classesC t , withC= ST
t=1 Ct andC t âˆ©C tâ€²
=/ 0 fortÌ¸=t â€². During each taskt, the THENGANE ET AL.: CLIMB-3D5
model receives a datasetD t ={(P i,Yt
i)}N
i=1, where each coloured point cloudP i âˆˆR MÃ—6
containsMpoints, and the corresponding annotationY t
i ={(m t
i,j ,ct
i,j )}
Jt
i
j=1 includes instance
masksm t
i,j âˆˆ {0,1}M and semantic class labelsc t
i,j âˆˆ Ct . The modelÎ¦ t , initialized from
Î¦tâˆ’1 , is trained onD t to predict instance-level outputs Ë†Yt
i ={(Ë†mt
i,j ,Ë†ct
i,j )}
Ë†Jt
i
j=1, where each
predicted semantic label Ë†ct
i,j belongs to the cumulative set of all classes observed so far:St
k=1 Ck. The key challenge in this setup is to learn new classes without forgetting those
encountered in previous tasks, despite limited supervision and the absence of past labels. 3.2 Method: CLIMB-3D
Overview.As shown in Fig. 2 and formulated in Sec. 3.1, the proposed framework for
CI-3DIS follows aphase-wisetraining strategy. In each phase, the model is exposed to a
carefully curated subset of the dataset, simulating real-world scenarios discussed in Sec. 3.3. Training naÃ¯vely on such phased data leads tocatastrophic forgetting[51], where the model
forgets prior knowledge when learning new tasks. To address this, we first incorporateEx-
emplar Replay(ER) [5], a strategy inspired by 2D incremental methods [1, 43, 54] and
recently extended to 3D settings [4], to mitigate forgetting by storing a small set of repre-
sentative samples from earlier phases. However, ER alone is insufficient to achieve robust
performance. To address this, two additional components are introduced: aPseudo-Label
Generator(PLG), which leverages a frozen model to generate supervision signals for previ-
ously seen classes, and aClass-Balanced Re-weighting(CBR) module that compensates for
class imbalance across phases. Each of these components is described in detail below 1. Exemplar Replay (ER).Inspired by Buzzega et al. [5], ER addresses the issue of catas-
trophic forgetting by allowing the model to retain a subset of data from earlier phases. During phaset, the model is trained not only on the current task dataD t , but also on a
set of exemplarsE 1:tâˆ’1, collected from previous phases. These exemplars are accumulated
incrementally:E 1:tâˆ’1 =E 1 âˆªÂ·Â·Â·âˆªE tâˆ’1 .Here,E t denotes the exemplar subset stored after
phaset, and|E t |denotes the exemplar replay size. The combined replay dataset is denoted
as:D t
ER =D t âˆªE 1:tâˆ’1.Epoch training consists of two stages: first, the model is trained on
the current data,D t ; then it is trained using exemplars fromE 1:tâˆ’1. While previous CI-3DIS approaches rely on large exemplar sets to mitigate forgetting [4],
such strategies become impractical under memory constraints. Consequently, although ER
aids knowledge retention, it is insufficient on its own for robust performance with a limited
exemplar budget. Therefore, PLG and CBR are introduced to better preserve past represen-
tations and enhance generalisation across phases. Pseudo-Label Generator (PLG).In the CI-3DIS setting, during phaset(wheret>1),
although ground-truth labels for previously seen classes are unavailable, the model from the
previous phaseÎ¦ tâˆ’1 , which preserves knowledge of past tasks, is retained. We use this
model to generate pseudo-labels for previously seen classes, thereby providing approximate
supervision during training. This allows the current modelÎ¦t , to retain prior knowledge and
reduce forgetting, even without access to ground-truth annotations. Given a point cloud and label pair(P,Y t ), fromD t , the previous modelÎ¦ tâˆ’1 , generates
pseudo-labels for the previously seen classes as Ë†Y1:tâˆ’1 =Î¦ tâˆ’1 (P), which we denote as ËœYt
1For clarity, modifications introduced by each component (ER, PLG, and CBR) are highlighted in green, blue,
and red, respectively. 6THENGANE ET AL.: CLIMB-3D
for brevity. However, we observe that utilising all predictions fromÎ¦tâˆ’1 introduces noisy or
incorrect labels, which degrade overall performance. To mitigate this, we select the top-K
most confident instance predictions, denoted as ËœYt
K, and concatenate them with the ground-
truth labels of the current task to obtain the supervision set: Â¯Yt =Y t âˆ¥ ËœYt
K. Following this,
the loss is computed over both the current taskâ€™s ground-truth labels and the top-Kconfi-
dent pseudo-labels from earlier phases, enabling the model to retain prior knowledge while
adapting to new classes. Class-Balanced Re-weighting (CBR).While ER and PLG contribute to preserving prior
knowledge, we observe that the modelâ€™s predictions are biased towards frequent classes,
resulting in the forgetting of rare categories. This issue is further amplified by the top-K
pseudo-labels selected by PLG, which predominantly represent dominant classes in the data. To address this, we propose aClass-Balanced Re-weighting(CBR) scheme that compensates
for class imbalance using object frequency statistics. At taskt, only the current datasetD t
and its label distributionp t (c)over classescâˆˆ Ct are available. Datasets and class distribu-
tions from earlier tasks{D i,p i(c)}tâˆ’1
i=1 are no longer accessible, which makes it challenging
to directly account for class imbalance across all previously seen tasks. Therefore, we pro-
pose to leverage the pseudo-label predictions of the frozen modelÎ¦ tâˆ’1 as a proxy for the
class distribution across previously learned categories. During each training iteration, the previous modelÎ¦ tâˆ’1 is applied to the current inputP
to generate pseudo-labels ËœYt , from which we accumulate class-wise frequency statistics for
previously learned classes. LetC1:t = St
i=1 Ci represent the union of all classes seen up to task
t. At the end of each epoch, we compute the overall class frequencyfâˆˆR |C1:t | by combining
the pseudo-label distribution Ëœpt (c)derived from ËœYt with the ground-truth label distribution
pt (c)from the current task. The resulting frequency vectorf= [f(c)] câˆˆC1:t is then used to
compute class-wise weightsw= [w(c)] câˆˆC1:t , which guide balanced pseudo-label selection
and help the model focus on rare classes. In the next epoch, the modelÎ¦ tâˆ’1 produces soft pseudo-label predictions ËœYt (as de-
scribed in PLG) , where each component ËœYt [c]denotes the confidence score for classcâˆˆ
C1:tâˆ’1. To promote balanced pseudo-label selection, class-wise re-weighting is applied using
w(c) = 1
f(c)+Îµ , whereÎµis a small constant for numerical stability. The adjusted predictions
are computed by applying the class-wise weights as: ËœYt,w [c] =w(c)Â· ËœYt [c],âˆ€câˆˆ C 1:tâˆ’1. Top-Kselection is then applied to both the original and the re-weighted scores, yielding
two pseudo-label sets: ËœYt
K and ËœYt,w
K , respectively. The final augmented target set, Yt , is
constructed by concatenating the ground-truth labelsY t with both sets of pseudo-labels:
Yt =Y t âˆ¥ ËœYt
K âˆ¥ ËœYt,w
K .(2)
Although the above re-weighting mitigates bias from prior tasks, it does not address class
imbalance within the current taskt. To resolve this, the class-balanced weightsw c are ex-
tended to incorporate statistics from both previously seen and current classes, i.e., overC 1:t
rather than onlyC 1:tâˆ’1. The updated weights, denoted asw â€²
c, are used to re-weights both
pseudo-label selection and classification loss, ensuring balanced supervision across all seen
categories, including rare ones. This unified strategy reduces bias and enhances performance
across phases. The pseudo-code for constructing the supervision set in Eq. (2), which incor-
porates both PLG and CBR components, is provided in Appendix A. Final Objective.The final training objective ofCLIMB-3D, incorporating ER, PLG, and THENGANE ET AL.: CLIMB-3D7
CBR, is formulated in Eq. (1) as:
L(Dt
ER;Î¦t ) = 1
|Dt
ER| âˆ‘
(P,Yt )âˆˆDt
ER
1
|Yt | âˆ‘
(mt
j,ct
j)âˆˆYt
 
Lmask(mt
j,Ë†mt
j)+w â€²â€²
c Â·Lcls(ct
j,Ë†ct
j)

,(3)
wherew â€²â€²
c =w â€²
c Â·Î» cls denotes the scaled weight vector, andÎ» cls is a hyperparameter for bal-
ancing the loss terms. 3.3 Benchmarking Incremental Scenarios
While CI-3DIS methods offer a wide range of practical applications, they frequently rely
on the assumption of uniform sample distribution, which rarely holds in real-world settings. In practice, the number of object categories, denoted byC, is often large and characterised
by substantial variability in category frequency, shape, structure, and size. To address these
challenges, we propose three incremental learning scenarios, each designed to capture a
different facet of real-world complexity. The design of these scenarios is detailed below; for
further information and illustrations, refer to Appendix E of the supplementary material. Split-A: Frequency Scenarios.This scenario acknowledges that datasets are often la-
belled based on the frequency of categories. To accommodate this, we propose a split where
the model learns from the most frequent categories and subsequently incorporates the less
frequent ones in later stages. By prioritising the training of frequently occurring categories,
the model can establish a strong foundation before expanding to handle rare categories. Split-B: Semantic Scenarios.Beyond frequency, semantic similarity is crucial in real-
world deployments. While objects often share visual or functional traits, models may en-
counter semantically different categories in new environments. To simulate this challenge,
we introduce theSplit-Bscenario. Here, categories are grouped based on their semantic
relationships, and the model is incrementally trained on one semantic group at a time. This
setup encourages the model to generalise across semantically similar categories and adapt
more effectively when exposed to new ones. Unlike theSplit-Ascenario, which organises
learning based on category frequency, theSplit-Bscenario may contain both frequent and
infrequent categories within a single task, focusing on semantic continuity and transfer. Split-C: Random Scenarios.In some cases, data labelling is driven by the availability of
objects rather than predefined criteria. To capture this, we introduce theSplit-Cscenario,
which represents a fully random setting where any class can appear in any task, resulting in
varying levels of class imbalance. By exposing the model to such diverse and unpredictable
distributions, we aim to improve its robustness in real-world situations where labelled data
availability is inconsistent. These incremental scenarios are designed to provide a more realistic representation of
object distributions, frequencies, and dynamics encountered in the real world. 4 Experiments
4.1 Setup
Datasets.We evaluateCLIMB-3Don ScanNet200 [57], which comprises 200 object cat-
egories and exhibits significant class imbalance, which makes it ideal for simulating and 8THENGANE ET AL.: CLIMB-3D
assessing real-world scenarios. In addition, we compare our approach against existing in-
cremental learning methods using ScanNetV2 [16] in the 3DSS setting. For this evaluation,
we follow the standard training and validation splits defined in prior works [74] to ensure
consistency with existing methods. Evaluation Metrics.We evaluate our method usingMean Average Precision(mAP), a
standard metric for 3DIS which provides a comprehensive measure of segmentation quality
by accounting for both precision and recall. For comparison with existing 3DSS methods,
we report themean Intersection over Union(mIoU), which quantifies the overlap between
predicted and ground truth segments. To assess the modelâ€™s ability to mitigate catastrophic
forgetting in incremental settings, we use theForgetting Percentage Points(FPP) metric, as
defined in [47]. FPP captures performance degradation by measuring the accuracy drop on
the initially seen categories between the first and final training phases. Detailed descriptions of the incremental scenarios (Split-A,Split-B,Split-C) and im-
plementation are provided in Appendix B. 4.2 Results and Discussion
Table 1: Comparison between the proposed method and the baseline in the 3DIS setting,
evaluated using mAP25, mAP50, mAP, and FPP after training across all phase. Scenarios Methods Average Precisionâ†‘FPPâ†“
mAP25 mAP50 mAP mAP 25 mAP50
Baseline 16.46 14.29 10.44 51.30 46.82Split-A CLIMB-3D(Ours) 35.69 31.05 22.72 3.44 2.63
Baseline 17.22 15.07 10.93 46.27 42.1Split-B CLIMB-3D(Ours) 35.48 31.56 23.69 8.00 5.51
Baseline 25.65 21.08 14.85 31.68 28.84Split-C CLIMB-3D(Ours) 31.59 26.78 18.93 9.10 7.89
To evaluate our proposed approach, we construct a baseline using ER [5] for the 3DIS
setting, as no existing incremental baselines are available for this task. In contrast, for the
3DSS setting, where incremental baselines do exist, we compare our method against three
prior approaches [43, 60, 74]. Results on CI-3DIS.Tab. 1 compares CLIMB-3D against the ER baseline across the three
CI-3DIS scenarios, evaluated after all phases using mAP 25, mAP50, overall mAP, and FPP. In theSplit-Ascenario, characterised by significant distribution shifts, our method achieves
gains of +19.23%, +16.76%, and +12.28% in mAP 25, mAP 50, and overall mAP, respec-
tively, while drastically reducing forgetting with FPP scores of 3.44% (mAP 25) and 2.63%
(mAP50), both over 45 points lower than the baseline. Under theSplit-Bscenario, with
semantically related new categories, improvements of +18.26%, +16.49%, and +12.76%
are observed alongside a reduction in forgetting to 8.00% and 5.51% from baseline levels
above 42%. For theSplit-Cscenario, featuring increasing geometric complexity, CLIMB-
3D maintains solid gains of +5.94%, +5.70%, and +4.08% across mAP metrics, with forget-
ting reduced by over 22 points. Overall, these results demonstrate robust forward transfer,
minimal forgetting, and stable learning, highlighting the methodâ€™s effectiveness for scalable
continual 3D semantic understanding. THENGANE ET AL.: CLIMB-3D9
Table 2: Comparison between the proposed method and existing baselines in the 3DSS set-
ting on ScanNetV2 [16], evaluated using mIoU. Methods Phase=1 Phase=2 All
EWC [60] 17.75 13.22 16.62
LwF [43] 30.38 13.37 26.13
Yanget al. [74] 34.16 13.43 28.98
CLIMB-3D(Ours) 69.39 32.56 59.38
Results on CI-3DSS.Although our primary focus is on CI-3DIS, we also evaluate our
method under the CI-3DSS setting to enable comparisons with existing approaches. To do
so, we adapt our predictions to assign each point the label corresponding to the highest-
confidence mask and exclude background classes (e.g., floor and wall), as these are not part
of the semantic segmentation targets. Tab. 2 presents a comparison between our method and
existing baselines on the ScanNet V2 dataset [16], evaluated using mIoU across two training
phases and overall. Although originally designed for instance segmentation, our method
generalises effectively to semantic segmentation, achieving substantial gains of +35.23%
mIoU in Phase 1 and +19.1% in Phase 2. Overall, it achieves 59.38% mIoU, significantly
higher than theâˆ¼30% mIoU of prior methods, highlighting its robustness and transferability. Refer to Appendix C, D, and F of the supplementary material for detailed analyses of per-
phase performance, rare-class evaluation, and qualitative result comparisons, respectively. 4.3 Ablation
Table 3: Ablation study illustrating the impact of each component in a three-phase setup. Each split (s) and corresponding number indicates data introduced at that phase. Best results
are highlighted inbold. Row Modules p=1â†‘p=2â†‘p=3â†‘ FPPâ†“
s1 s1 s2 All s1 s2 s3 All
1. Oracle - - - - 55.14 30.77 25.30 37.68 -
2. NaÃ¯ve 56.82 0.00 28.09 14.15 0.00 0.00 19.67 5.80 56.82
3. + ER 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28 46.44
4. + PLG 56.82 50.0034.3942.13 49.78 11.41 26.47 29.28 7.04
5. + CBR 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05 2.63
We conduct an ablation study to evaluate the contribution of each component in our
framework. As an upper bound, we report the performance of anOraclemodel, which is
trained jointly on the full dataset. For the incremental setting, we follow the previously
defined splits and first train the model naively across phases. We then incrementally add
each module to isolate its impact. Tab. 3 presents results for theSplit-Ascenario using both
mAP50 and FPP metrics. NaÃ¯ve Training.When trained naÃ¯vely without any dedicated modules, the model suffers
from severe catastrophic forgetting, as evident in row 2. The model entirely forgets pre-
viously learned classes upon entering new phases, with performance dropping to zero for
earlier splits and FPP rising to 56.82. 10THENGANE ET AL.: CLIMB-3D
Effect of ER.Adding exemplar replay (row 3) partially alleviates forgetting by maintaining
a buffer of past examples. This yields notable gains fors1in Phase 2 (+18.51%) and Phase 3
(+10.38%), and also improvess2in Phase 3 (+9.43%). However, the overall forgetting
remains substantial (FPP: 46.44), showing ER alone is insufficient. Effect of PLG.The addition of the pseudo-label generator (PLG), which generates labels for
previous classes by retaining a copy of the model from an earlier phase, facilitates knowledge
retention and forward transfer. As shown in row 4, PLG significantly reduces forgetting and
enhances performance on current tasks. Fors1, it improves mAP 50 by 31.49% in Phase 2
and 39.40% in Phase 3 over exemplar replay. Overall, PLG yields a 15.00% increase in
performance and reduces forgetting by 39.40%. Effect of CBR.Finally, class-balanced re-weighting (CBR) mitigates class imbalance dur-
ing both pseudo-labelling and current task learning by adjusting each classâ€™s contribution
based on its frequency. As shown in row 5, CBR enhancess1retention over PLG in Phases 2
and 3 (+4.67% and +4.41%), and further improves performance ons2ands3. It also achieves
the lowest FPP of 2.63, reflecting strong forgetting mitigation. Overall, CBR offers the best
trade-off between retaining past knowledge and acquiring new tasks, outperforming PLG
and ER by 4.41% and 43.81%, respectively. 4.4 Limitations & Future Work
While CLIMB-3D achieves strong performance across CI-3DIS and CI-3DSS settings, sev-
eral limitations remain and warrant discussion. The experiments are currently limited to
indoor datasets (ScanNet200 and ScanNetV2), therefore evaluating performance on outdoor
scenes would help assess generalisability across diverse environments. The current setup
also considers only three incremental phases, whereas real-world applications often involve
longer sequences; incorporating such extended setups would better reflect practical chal-
lenges. Moreover, the model operates in a uni-modal setting. Integrating multi-modal cues,
such as vision-language models, could further enhance performance, as suggested by recent
2D and 3D studies [25, 65, 66]. 5 Conclusion
This work addresses the challenge of catastrophic forgetting in class-incremental 3D in-
stance segmentation by introducing a modular framework that integrates exemplar replay
(ER), a pseudo-label generator (PLG), and class-balanced re-weighting (CBR). The pro-
posed method incrementally adapts to new classes while preserving prior knowledge, all
without requiring access to the full dataset. Extensive experiments on a three-phase bench-
mark validate the individual and combined effectiveness of each component: ER enables
efficient memory-based retention; PLG facilitates knowledge preservation through pseudo-
supervision; and CBR mitigates class imbalance to enhance learning stability. Together,
these components significantly reduce forgetting and improve segmentation performance
across all phases. By achieving a strong balance between stability and plasticity, our ap-
proach advances continual 3D scene understanding and sets a new baseline for future re-
search in this area. THENGANE ET AL.: CLIMB-3D11
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne
Tuytelaars. Memory aware synapses: Learning what (not) to forget. InECCV, pages
139â€“154, 2018. [2] Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long-tailed
recognition via weight balancing. InCVPR, pages 6897â€“6907, 2022. [3] Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying
imbalanced data. InJoint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 770â€“785.

Class-imbalanced learning."
Which authors contributed to the paper on Geometry and uncertainty-aware 3d point cloud class-incremental semantic segmentation?,"Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei.","[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei.

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019."
What is the title of the paper by Xi Yin et al.?,Feature transfer learning for face recognition with under-represented data,"[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021.

It may be distributed unchanged freely in print or electronic forms. 2THENGANE ET AL.: CLIMB-3D
1 Introduction
Table Chair Couch Pillow Coffee Table Sofa Chair Recycling BinCabinet Unknown
Task-1 Task-2 Task-3
Figure 1:Overview of the CI-3DIS setting.New object categories are introduced incre-
mentally with each task. After every phase, the model must recognise both newly added
and previously learned classes. For instance, in Task 2, categories such asPillow,Coffee
Table, andSofa Chairare introduced, while the model is expected to retain recognition
of earlier classes likeTable,Chair, andCouch. 3D instance segmentation (3DIS) is a fundamental task in computer vision that involves
identifying precise object boundaries and class labels in 3D space, with broad applications in
graphics, robotics, and augmented reality [4, 39]. Traditional 3DIS methods, including top-
down [28, 70, 78], bottom-up [26, 73], and transformer-based approaches [59], perform well
under the assumption that all classes are available and balanced during training. However,
this assumption does not hold in real-world environments, where new object classes emerge
over time and exhibit natural imbalance. This gap motivates the need for Class-Incremental Learning (CIL) [17, 54], which sup-
ports learning new categories while retaining knowledge of previous ones [51]. While CIL
has seen success in 2D image tasks [1, 43, 54, 60], extensions to 3D point clouds remain
limited, often focusing on object-level classification [13, 18, 48]. Recent works on scene-
level class-incremental 3DIS (CI-3DIS) [4] and class-incremental semantic segmentation
(CI-3DSS) [74] show promise, but depend heavily on large exemplar [56] and overlook class
imbalance, limiting their practicality. To address this, we proposeCLIMB-3D, a unified framework forCLass-incremental
IMBalance-aware3DIS that jointly tackles catastrophic forgetting and class imbalance. The
framework begins with Exemplar Replay (ER), storing a small number of samples from
past classes for later replay. However, this alone does not yield promising results under the
strict memory constraints required in CI-3DIS. To address this, we introduce a Pseudo-Label
Generator (PLG), which uses a frozen model from previous task to generate supervision for
earlier classes. However, we observed that PLG tends to favour frequent classes while ig-
noring under-represented ones. To mitigate this, we propose a Class-Balanced Re-weighting
(CBR), which estimates class frequencies from the pseudo-labels to enhance the learning of
rare classes without accessing past data. Together, these components form a practical and
effective solution for real-world 3DIS. To evaluate CLIMB-3D, we designed three benchmark scenarios for CI-3DIS based on
the ScanNet200 dataset [57]. These scenarios simulate incremental learning setup under nat-
ural class imbalance, where new classes emerge based on (A) object frequency, (B) semantic
similarity, or (C) random grouping. Additionally, for comparison with existing methods,
we evaluate our approach on CI-3DSS using the ScanNetV2 dataset [16]. Experimental re-
sults show that our method significantly reduces forgetting and improves performance across
both frequent and rare categories, outperforming previous methods in both settings. Fig. 1
illustrates the CI-3DIS setup. THENGANE ET AL.: CLIMB-3D3
In summary, our contributions are: (i) a novel problem setting of imbalanced class-
incremental 3DIS with an effective method to balance learning and mitigate forgetting; (ii)
three benchmarks modelling continual object emergence with natural imbalance; and (iii)
strong experimental results, achieving up to 16.76% mAP improvement over baselines. 2 Related Work
3D Instance Segmentation.Various methods have been proposed for 3DIS. Grouping-
based approaches adopt a bottom-up pipeline that learns latent embeddings for point cluster-
ing [9, 21, 22, 29, 37, 44, 70, 78]. In contrast, proposal-based methods follow a top-down
strategy by detecting 3D boxes and segmenting objects within them [19, 26, 46, 73, 76]. Recently, transformer-based models [68] have also been applied to 3DIS and 3DSS [59, 64],
inspired by advances in 2D vision [10, 11]. However, these approaches require full anno-
tations for all classes and are not designed for progressive learning, where only new class
annotations are available and previous data is inaccessible. Another line of work aims to re-
duce annotation costs by proposing weakly supervised 3DIS methods based on sparse cues
[12, 27, 72]. While effective with limited annotations, these methods assume a fixed set of
classes and are susceptible to catastrophic forgetting in incremental settings. Incremental Learning.Continual learning involves training models sequentially to miti-
gate catastrophic forgetting. One common strategy is model regularisation, which constrains
parameter updates using techniques such as elastic weight consolidation or knowledge dis-
tillation [1, 24, 35, 43, 60]. Another approach is exemplar replay, where past data is stored
or generated to rehearse older tasks while learning new ones [5, 7, 30, 54]. A third direc-
tion involves dynamically expanding the model architecture or using modular sub-networks
to accommodate new tasks without interfering with old ones [31, 42, 53, 58, 71, 79]. Re-
cent efforts in class-incremental 3D segmentation [63, 74] have shown early promise but
often rely on basic architectures and focus primarily on semantic segmentation. Other works
[4, 36] explore continual learning in 3D settings, though they often depend on large memory
buffers. In contrast, our work introduces a tailored framework for 3D instance segmentation
that effectively transfers knowledge across tasks, even under class imbalance. Long-tailed Recognition.Imbalanced class distributions lead to poor recognition of rare
(long-tail) classes. To address this, re-sampling methods balance the data via input [8, 20, 50,
52, 61] or feature space [3, 14, 40], avoiding naÃ¯ve under-/over-sampling. Loss re-weighting
offers another direction, using class-based [6, 15, 23, 32, 33, 69] or per-example [45, 55, 62]
adjustments to ensure fair contribution. Parameter regularisation improves generalisation
through weight constraints [2], albeit requiring careful tuning. Other approaches leverage
transfer learning [77, 81], self-supervision [41, 75], or contrastive learning [34, 41, 82] to
improve rare-class representations. Long-tailed recognition is well-studied in 2D with large-
scale datasets [15, 49, 67]; in 3D, ScanNet200 [16, 57] enables similar efforts. Prior 3D
work focused on re-weighting, re-sampling, and transfer learning [57], while regularisation
remains underexplored. CeCo [80] balances class centres via auxiliary loss but overlooks
sampling and augmentation. Lahoud et al. [38] propose adaptive classifier regularisation
for 3D segmentation, outperforming prior approaches without threshold tuning. However,
neither method considers incremental learning. In contrast, we jointly address long-tail and
continual 3D semantic segmentation. 4THENGANE ET AL.: CLIMB-3D
Feature 
Backbone
Transformer 
Decoder
Feature 
Backbone
Transformer 
Decoder
(ð‘·,ð’€ð‘¡)
Current model (ð›·
ð‘¡
)
Pseudo Label Generator (PLG) 
Selection
à·¨ð‘Œð¾
ð‘¡
â„’CIâˆ’3DIS
à·¨ð‘Œð¾
ð‘¡,ð‘¤
à´¤ð‘Œð¾
ð‘¡
à· ð‘Œð‘¡
(ð·ð‘¡)
Current Dataset 
Exemplar 
Replay (ER)
+
~
Class-Balanced  
Re-weighting 
(CBR)
1
ð‘“(ð‘)
Figure 2: Overview ofCLIMB-3Dfor CI-3DIS. The model incrementally learns new classes
across sequential phases. During taskt, point cloudsPand their corresponding labelsY t are
sampled from a combination of the current training datasetD t and Exemplar Replay (ER),
which maintains a small memory of past examples. These are then passed to the current
modelÎ¦ t to produce predicted labelsY t . The Pseudo-Label Generator (PLG) selects the
top-Kpredictions from the previous modelÎ¦ tâˆ’1 . These pseudo-labels are then weighted
based on class frequencyf(c)using Class-Balanced Re-weighting (CBR), and the top-Kre-
weighted labels are selected to form the balanced pseudo-label set Â¯Yt . This pseudo-label set
is then concatenated with the ground-truth labels to form a final augmented supervision set
Yt for taskt, which is used to optimise the modelÎ¦ t using Eq. (3). 3 Methodology
3.1 Problem Formulation
3DIS.The objective of this task is to detect and segment each object instance within a point
cloud. Formally, the training dataset is denoted asD={(P i,Yi)}N
i=1, whereNis the number
of training samples. Each inputP i âˆˆR MÃ—6 is a coloured point cloud consisting ofMpoints,
where each point is represented by its 3D coordinates and RGB values. The corresponding
annotationY i ={(m i,j ,ci,j )}Ji
j=1 containsJ i object instances, wheremi,j âˆˆ {0,1}M is a binary
mask indicating which points belong to thej-th instance, andc i,j âˆˆ C={1,...,C}is the
semantic class label of that instance. GivenP i, the modelÎ¦predicts instance-level outputs
Ë†Yi ={(Ë†mi,j ,Ë†ci,j )}
Ë†Ji
j=1, where Ë†mi,j and Ë†ci,j denote the predicted mask and category label for
thej-th instance, respectively. The number of predicted instances Ë†Ji varies depending on the
modelâ€™s inference. The model is optimised using the following objective:
L3DIS(D;Î¦) = 1
|D| âˆ‘
(P,Y)âˆˆD
1
|Y| âˆ‘
(mj,cj)âˆˆY
(Lmask(mj,Ë†mj)+Î»Â·L cls(cj,Ë†cj)),(1)
whereL cls andL mask are the average classification and mask losses over all instances, andÎ»
controls their trade-off. CI-3DIS.Unlike conventional 3DIS, CI-3DIS involves sequential learning, where object
categories are introduced incrementally overTtraining tasks. Each tasktintroduces a dis-
joint set of classesC t , withC= ST
t=1 Ct andC t âˆ©C tâ€²
=/ 0 fortÌ¸=t â€². During each taskt, the THENGANE ET AL.: CLIMB-3D5
model receives a datasetD t ={(P i,Yt
i)}N
i=1, where each coloured point cloudP i âˆˆR MÃ—6
containsMpoints, and the corresponding annotationY t
i ={(m t
i,j ,ct
i,j )}
Jt
i
j=1 includes instance
masksm t
i,j âˆˆ {0,1}M and semantic class labelsc t
i,j âˆˆ Ct . The modelÎ¦ t , initialized from
Î¦tâˆ’1 , is trained onD t to predict instance-level outputs Ë†Yt
i ={(Ë†mt
i,j ,Ë†ct
i,j )}
Ë†Jt
i
j=1, where each
predicted semantic label Ë†ct
i,j belongs to the cumulative set of all classes observed so far:St
k=1 Ck. The key challenge in this setup is to learn new classes without forgetting those
encountered in previous tasks, despite limited supervision and the absence of past labels. 3.2 Method: CLIMB-3D
Overview.As shown in Fig. 2 and formulated in Sec. 3.1, the proposed framework for
CI-3DIS follows aphase-wisetraining strategy. In each phase, the model is exposed to a
carefully curated subset of the dataset, simulating real-world scenarios discussed in Sec. 3.3. Training naÃ¯vely on such phased data leads tocatastrophic forgetting[51], where the model
forgets prior knowledge when learning new tasks. To address this, we first incorporateEx-
emplar Replay(ER) [5], a strategy inspired by 2D incremental methods [1, 43, 54] and
recently extended to 3D settings [4], to mitigate forgetting by storing a small set of repre-
sentative samples from earlier phases. However, ER alone is insufficient to achieve robust
performance. To address this, two additional components are introduced: aPseudo-Label
Generator(PLG), which leverages a frozen model to generate supervision signals for previ-
ously seen classes, and aClass-Balanced Re-weighting(CBR) module that compensates for
class imbalance across phases. Each of these components is described in detail below 1. Exemplar Replay (ER).Inspired by Buzzega et al. [5], ER addresses the issue of catas-
trophic forgetting by allowing the model to retain a subset of data from earlier phases. During phaset, the model is trained not only on the current task dataD t , but also on a
set of exemplarsE 1:tâˆ’1, collected from previous phases. These exemplars are accumulated
incrementally:E 1:tâˆ’1 =E 1 âˆªÂ·Â·Â·âˆªE tâˆ’1 .Here,E t denotes the exemplar subset stored after
phaset, and|E t |denotes the exemplar replay size. The combined replay dataset is denoted
as:D t
ER =D t âˆªE 1:tâˆ’1.Epoch training consists of two stages: first, the model is trained on
the current data,D t ; then it is trained using exemplars fromE 1:tâˆ’1. While previous CI-3DIS approaches rely on large exemplar sets to mitigate forgetting [4],
such strategies become impractical under memory constraints. Consequently, although ER
aids knowledge retention, it is insufficient on its own for robust performance with a limited
exemplar budget. Therefore, PLG and CBR are introduced to better preserve past represen-
tations and enhance generalisation across phases. Pseudo-Label Generator (PLG).In the CI-3DIS setting, during phaset(wheret>1),
although ground-truth labels for previously seen classes are unavailable, the model from the
previous phaseÎ¦ tâˆ’1 , which preserves knowledge of past tasks, is retained. We use this
model to generate pseudo-labels for previously seen classes, thereby providing approximate
supervision during training. This allows the current modelÎ¦t , to retain prior knowledge and
reduce forgetting, even without access to ground-truth annotations. Given a point cloud and label pair(P,Y t ), fromD t , the previous modelÎ¦ tâˆ’1 , generates
pseudo-labels for the previously seen classes as Ë†Y1:tâˆ’1 =Î¦ tâˆ’1 (P), which we denote as ËœYt
1For clarity, modifications introduced by each component (ER, PLG, and CBR) are highlighted in green, blue,
and red, respectively. 6THENGANE ET AL.: CLIMB-3D
for brevity. However, we observe that utilising all predictions fromÎ¦tâˆ’1 introduces noisy or
incorrect labels, which degrade overall performance. To mitigate this, we select the top-K
most confident instance predictions, denoted as ËœYt
K, and concatenate them with the ground-
truth labels of the current task to obtain the supervision set: Â¯Yt =Y t âˆ¥ ËœYt
K. Following this,
the loss is computed over both the current taskâ€™s ground-truth labels and the top-Kconfi-
dent pseudo-labels from earlier phases, enabling the model to retain prior knowledge while
adapting to new classes. Class-Balanced Re-weighting (CBR).While ER and PLG contribute to preserving prior
knowledge, we observe that the modelâ€™s predictions are biased towards frequent classes,
resulting in the forgetting of rare categories. This issue is further amplified by the top-K
pseudo-labels selected by PLG, which predominantly represent dominant classes in the data. To address this, we propose aClass-Balanced Re-weighting(CBR) scheme that compensates
for class imbalance using object frequency statistics. At taskt, only the current datasetD t
and its label distributionp t (c)over classescâˆˆ Ct are available. Datasets and class distribu-
tions from earlier tasks{D i,p i(c)}tâˆ’1
i=1 are no longer accessible, which makes it challenging
to directly account for class imbalance across all previously seen tasks. Therefore, we pro-
pose to leverage the pseudo-label predictions of the frozen modelÎ¦ tâˆ’1 as a proxy for the
class distribution across previously learned categories. During each training iteration, the previous modelÎ¦ tâˆ’1 is applied to the current inputP
to generate pseudo-labels ËœYt , from which we accumulate class-wise frequency statistics for
previously learned classes. LetC1:t = St
i=1 Ci represent the union of all classes seen up to task
t. At the end of each epoch, we compute the overall class frequencyfâˆˆR |C1:t | by combining
the pseudo-label distribution Ëœpt (c)derived from ËœYt with the ground-truth label distribution
pt (c)from the current task. The resulting frequency vectorf= [f(c)] câˆˆC1:t is then used to
compute class-wise weightsw= [w(c)] câˆˆC1:t , which guide balanced pseudo-label selection
and help the model focus on rare classes. In the next epoch, the modelÎ¦ tâˆ’1 produces soft pseudo-label predictions ËœYt (as de-
scribed in PLG) , where each component ËœYt [c]denotes the confidence score for classcâˆˆ
C1:tâˆ’1. To promote balanced pseudo-label selection, class-wise re-weighting is applied using
w(c) = 1
f(c)+Îµ , whereÎµis a small constant for numerical stability. The adjusted predictions
are computed by applying the class-wise weights as: ËœYt,w [c] =w(c)Â· ËœYt [c],âˆ€câˆˆ C 1:tâˆ’1. Top-Kselection is then applied to both the original and the re-weighted scores, yielding
two pseudo-label sets: ËœYt
K and ËœYt,w
K , respectively. The final augmented target set, Yt , is
constructed by concatenating the ground-truth labelsY t with both sets of pseudo-labels:
Yt =Y t âˆ¥ ËœYt
K âˆ¥ ËœYt,w
K .(2)
Although the above re-weighting mitigates bias from prior tasks, it does not address class
imbalance within the current taskt. To resolve this, the class-balanced weightsw c are ex-
tended to incorporate statistics from both previously seen and current classes, i.e., overC 1:t
rather than onlyC 1:tâˆ’1. The updated weights, denoted asw â€²
c, are used to re-weights both
pseudo-label selection and classification loss, ensuring balanced supervision across all seen
categories, including rare ones. This unified strategy reduces bias and enhances performance
across phases. The pseudo-code for constructing the supervision set in Eq. (2), which incor-
porates both PLG and CBR components, is provided in Appendix A. Final Objective.The final training objective ofCLIMB-3D, incorporating ER, PLG, and THENGANE ET AL.: CLIMB-3D7
CBR, is formulated in Eq. (1) as:
L(Dt
ER;Î¦t ) = 1
|Dt
ER| âˆ‘
(P,Yt )âˆˆDt
ER
1
|Yt | âˆ‘
(mt
j,ct
j)âˆˆYt
 
Lmask(mt
j,Ë†mt
j)+w â€²â€²
c Â·Lcls(ct
j,Ë†ct
j)

,(3)
wherew â€²â€²
c =w â€²
c Â·Î» cls denotes the scaled weight vector, andÎ» cls is a hyperparameter for bal-
ancing the loss terms. 3.3 Benchmarking Incremental Scenarios
While CI-3DIS methods offer a wide range of practical applications, they frequently rely
on the assumption of uniform sample distribution, which rarely holds in real-world settings. In practice, the number of object categories, denoted byC, is often large and characterised
by substantial variability in category frequency, shape, structure, and size. To address these
challenges, we propose three incremental learning scenarios, each designed to capture a
different facet of real-world complexity. The design of these scenarios is detailed below; for
further information and illustrations, refer to Appendix E of the supplementary material. Split-A: Frequency Scenarios.This scenario acknowledges that datasets are often la-
belled based on the frequency of categories. To accommodate this, we propose a split where
the model learns from the most frequent categories and subsequently incorporates the less
frequent ones in later stages. By prioritising the training of frequently occurring categories,
the model can establish a strong foundation before expanding to handle rare categories. Split-B: Semantic Scenarios.Beyond frequency, semantic similarity is crucial in real-
world deployments. While objects often share visual or functional traits, models may en-
counter semantically different categories in new environments. To simulate this challenge,
we introduce theSplit-Bscenario. Here, categories are grouped based on their semantic
relationships, and the model is incrementally trained on one semantic group at a time. This
setup encourages the model to generalise across semantically similar categories and adapt
more effectively when exposed to new ones. Unlike theSplit-Ascenario, which organises
learning based on category frequency, theSplit-Bscenario may contain both frequent and
infrequent categories within a single task, focusing on semantic continuity and transfer. Split-C: Random Scenarios.In some cases, data labelling is driven by the availability of
objects rather than predefined criteria. To capture this, we introduce theSplit-Cscenario,
which represents a fully random setting where any class can appear in any task, resulting in
varying levels of class imbalance. By exposing the model to such diverse and unpredictable
distributions, we aim to improve its robustness in real-world situations where labelled data
availability is inconsistent. These incremental scenarios are designed to provide a more realistic representation of
object distributions, frequencies, and dynamics encountered in the real world. 4 Experiments
4.1 Setup
Datasets.We evaluateCLIMB-3Don ScanNet200 [57], which comprises 200 object cat-
egories and exhibits significant class imbalance, which makes it ideal for simulating and 8THENGANE ET AL.: CLIMB-3D
assessing real-world scenarios. In addition, we compare our approach against existing in-
cremental learning methods using ScanNetV2 [16] in the 3DSS setting. For this evaluation,
we follow the standard training and validation splits defined in prior works [74] to ensure
consistency with existing methods. Evaluation Metrics.We evaluate our method usingMean Average Precision(mAP), a
standard metric for 3DIS which provides a comprehensive measure of segmentation quality
by accounting for both precision and recall. For comparison with existing 3DSS methods,
we report themean Intersection over Union(mIoU), which quantifies the overlap between
predicted and ground truth segments. To assess the modelâ€™s ability to mitigate catastrophic
forgetting in incremental settings, we use theForgetting Percentage Points(FPP) metric, as
defined in [47]. FPP captures performance degradation by measuring the accuracy drop on
the initially seen categories between the first and final training phases. Detailed descriptions of the incremental scenarios (Split-A,Split-B,Split-C) and im-
plementation are provided in Appendix B. 4.2 Results and Discussion
Table 1: Comparison between the proposed method and the baseline in the 3DIS setting,
evaluated using mAP25, mAP50, mAP, and FPP after training across all phase. Scenarios Methods Average Precisionâ†‘FPPâ†“
mAP25 mAP50 mAP mAP 25 mAP50
Baseline 16.46 14.29 10.44 51.30 46.82Split-A CLIMB-3D(Ours) 35.69 31.05 22.72 3.44 2.63
Baseline 17.22 15.07 10.93 46.27 42.1Split-B CLIMB-3D(Ours) 35.48 31.56 23.69 8.00 5.51
Baseline 25.65 21.08 14.85 31.68 28.84Split-C CLIMB-3D(Ours) 31.59 26.78 18.93 9.10 7.89
To evaluate our proposed approach, we construct a baseline using ER [5] for the 3DIS
setting, as no existing incremental baselines are available for this task. In contrast, for the
3DSS setting, where incremental baselines do exist, we compare our method against three
prior approaches [43, 60, 74]. Results on CI-3DIS.Tab. 1 compares CLIMB-3D against the ER baseline across the three
CI-3DIS scenarios, evaluated after all phases using mAP 25, mAP50, overall mAP, and FPP. In theSplit-Ascenario, characterised by significant distribution shifts, our method achieves
gains of +19.23%, +16.76%, and +12.28% in mAP 25, mAP 50, and overall mAP, respec-
tively, while drastically reducing forgetting with FPP scores of 3.44% (mAP 25) and 2.63%
(mAP50), both over 45 points lower than the baseline. Under theSplit-Bscenario, with
semantically related new categories, improvements of +18.26%, +16.49%, and +12.76%
are observed alongside a reduction in forgetting to 8.00% and 5.51% from baseline levels
above 42%. For theSplit-Cscenario, featuring increasing geometric complexity, CLIMB-
3D maintains solid gains of +5.94%, +5.70%, and +4.08% across mAP metrics, with forget-
ting reduced by over 22 points. Overall, these results demonstrate robust forward transfer,
minimal forgetting, and stable learning, highlighting the methodâ€™s effectiveness for scalable
continual 3D semantic understanding. THENGANE ET AL.: CLIMB-3D9
Table 2: Comparison between the proposed method and existing baselines in the 3DSS set-
ting on ScanNetV2 [16], evaluated using mIoU. Methods Phase=1 Phase=2 All
EWC [60] 17.75 13.22 16.62
LwF [43] 30.38 13.37 26.13
Yanget al. [74] 34.16 13.43 28.98
CLIMB-3D(Ours) 69.39 32.56 59.38
Results on CI-3DSS.Although our primary focus is on CI-3DIS, we also evaluate our
method under the CI-3DSS setting to enable comparisons with existing approaches. To do
so, we adapt our predictions to assign each point the label corresponding to the highest-
confidence mask and exclude background classes (e.g., floor and wall), as these are not part
of the semantic segmentation targets. Tab. 2 presents a comparison between our method and
existing baselines on the ScanNet V2 dataset [16], evaluated using mIoU across two training
phases and overall. Although originally designed for instance segmentation, our method
generalises effectively to semantic segmentation, achieving substantial gains of +35.23%
mIoU in Phase 1 and +19.1% in Phase 2. Overall, it achieves 59.38% mIoU, significantly
higher than theâˆ¼30% mIoU of prior methods, highlighting its robustness and transferability. Refer to Appendix C, D, and F of the supplementary material for detailed analyses of per-
phase performance, rare-class evaluation, and qualitative result comparisons, respectively. 4.3 Ablation
Table 3: Ablation study illustrating the impact of each component in a three-phase setup. Each split (s) and corresponding number indicates data introduced at that phase. Best results
are highlighted inbold. Row Modules p=1â†‘p=2â†‘p=3â†‘ FPPâ†“
s1 s1 s2 All s1 s2 s3 All
1. Oracle - - - - 55.14 30.77 25.30 37.68 -
2. NaÃ¯ve 56.82 0.00 28.09 14.15 0.00 0.00 19.67 5.80 56.82
3. + ER 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28 46.44
4. + PLG 56.82 50.0034.3942.13 49.78 11.41 26.47 29.28 7.04
5. + CBR 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05 2.63
We conduct an ablation study to evaluate the contribution of each component in our
framework. As an upper bound, we report the performance of anOraclemodel, which is
trained jointly on the full dataset. For the incremental setting, we follow the previously
defined splits and first train the model naively across phases. We then incrementally add
each module to isolate its impact. Tab. 3 presents results for theSplit-Ascenario using both
mAP50 and FPP metrics. NaÃ¯ve Training.When trained naÃ¯vely without any dedicated modules, the model suffers
from severe catastrophic forgetting, as evident in row 2. The model entirely forgets pre-
viously learned classes upon entering new phases, with performance dropping to zero for
earlier splits and FPP rising to 56.82. 10THENGANE ET AL.: CLIMB-3D
Effect of ER.Adding exemplar replay (row 3) partially alleviates forgetting by maintaining
a buffer of past examples. This yields notable gains fors1in Phase 2 (+18.51%) and Phase 3
(+10.38%), and also improvess2in Phase 3 (+9.43%). However, the overall forgetting
remains substantial (FPP: 46.44), showing ER alone is insufficient. Effect of PLG.The addition of the pseudo-label generator (PLG), which generates labels for
previous classes by retaining a copy of the model from an earlier phase, facilitates knowledge
retention and forward transfer. As shown in row 4, PLG significantly reduces forgetting and
enhances performance on current tasks. Fors1, it improves mAP 50 by 31.49% in Phase 2
and 39.40% in Phase 3 over exemplar replay. Overall, PLG yields a 15.00% increase in
performance and reduces forgetting by 39.40%. Effect of CBR.Finally, class-balanced re-weighting (CBR) mitigates class imbalance dur-
ing both pseudo-labelling and current task learning by adjusting each classâ€™s contribution
based on its frequency. As shown in row 5, CBR enhancess1retention over PLG in Phases 2
and 3 (+4.67% and +4.41%), and further improves performance ons2ands3. It also achieves
the lowest FPP of 2.63, reflecting strong forgetting mitigation. Overall, CBR offers the best
trade-off between retaining past knowledge and acquiring new tasks, outperforming PLG
and ER by 4.41% and 43.81%, respectively. 4.4 Limitations & Future Work
While CLIMB-3D achieves strong performance across CI-3DIS and CI-3DSS settings, sev-
eral limitations remain and warrant discussion. The experiments are currently limited to
indoor datasets (ScanNet200 and ScanNetV2), therefore evaluating performance on outdoor
scenes would help assess generalisability across diverse environments. The current setup
also considers only three incremental phases, whereas real-world applications often involve
longer sequences; incorporating such extended setups would better reflect practical chal-
lenges. Moreover, the model operates in a uni-modal setting. Integrating multi-modal cues,
such as vision-language models, could further enhance performance, as suggested by recent
2D and 3D studies [25, 65, 66]. 5 Conclusion
This work addresses the challenge of catastrophic forgetting in class-incremental 3D in-
stance segmentation by introducing a modular framework that integrates exemplar replay
(ER), a pseudo-label generator (PLG), and class-balanced re-weighting (CBR). The pro-
posed method incrementally adapts to new classes while preserving prior knowledge, all
without requiring access to the full dataset. Extensive experiments on a three-phase bench-
mark validate the individual and combined effectiveness of each component: ER enables
efficient memory-based retention; PLG facilitates knowledge preservation through pseudo-
supervision; and CBR mitigates class imbalance to enhance learning stability. Together,
these components significantly reduce forgetting and improve segmentation performance
across all phases. By achieving a strong balance between stability and plasticity, our ap-
proach advances continual 3D scene understanding and sets a new baseline for future re-
search in this area. THENGANE ET AL.: CLIMB-3D11
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne
Tuytelaars. Memory aware synapses: Learning what (not) to forget. InECCV, pages
139â€“154, 2018. [2] Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long-tailed
recognition via weight balancing. InCVPR, pages 6897â€“6907, 2022. [3] Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying
imbalanced data. InJoint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 770â€“785.

Feature transfer learning for face recognition with under-represented data"
In which conference was the paper by Xi Yin et al. published?,InCVPR,"IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

arXiv:2502.17429v3  [cs.CV]  21 Nov 2025
THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced
3D Instance Segmentation
Vishal Thengane1
v.thengane@surrey.ac.uk
Jean Lahoud2
jean.lahoud@mbzuai.ac.ae
Hisham Cholakkal2
hisham.cholakkal@mbzuai.ac.ae
Rao Muhammad Anwer2
rao.anwer@mbzuai.ac.ae
Lu Yin1
l.yin@surrey.ac.uk
Xiatian Zhu1
xiatian.zhu@surrey.ac.uk
Salman Khan2, 3
salman.khan@mbzuai.ac.ae
1 University of Surrey,
Guildford, UK
2 Mohamed bin Zayed University of
Artificial Intelligence,
Abu Dhabi, UAE
3 Australian National University,
Canberra, Australia
Abstract
While 3D instance segmentation (3DIS) has advanced significantly, most existing
methods assume that all object classes are known in advance and uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new
classes emerge gradually and exhibit natural imbalance. Although some approaches ad-
dress the emergence of new classes, they often overlook class imbalance, which leads
to suboptimal performance, particularly on rare categories. To tackle this, we propose
CLIMB-3D, a unified framework forCLass-incrementalImbalance-aware3DIS. Build-
ing upon established exemplar replay (ER) strategies, we show that ER alone is insuf-
ficient to achieve robust performance under memory constraints. To mitigate this, we
introduce a novel pseudo-label generator (PLG) that extends supervision to previously
learned categories by leveraging predictions from a frozen model trained on prior tasks. Despite its promise, PLG tends to be biased towards frequent classes. Therefore, we
propose a class-balanced re-weighting (CBR) scheme that estimates object frequencies
from pseudo-labels and dynamically adjusts training bias, without requiring access to
past data. We design and evaluate three incremental scenarios for 3DIS on the challeng-
ing ScanNet200 dataset and additionally validate our method for semantic segmentation
on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by
up to 16.76% mAP for instance segmentation and approximately 30% mIoU for semantic
segmentation, demonstrating strong generalisation across both frequent and rare classes. Code is available at:https://github.com/vgthengane/CLIMB3D. Â© 2025.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916.

InCVPR"
What are the page numbers of the paper by Xi Yin et al.?,pages 5704â€“5713,"Springer, 2017.

pages 5704â€“5713

Springer,
2016.

Elsevier, 1989."
Who are the authors of the paper on face recognition with under-represented data?,"Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker","[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017.

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker"
What is the main focus of the paper by Xi Yin et al.?,Feature transfer learning for face recognition with under-represented data,"Feature transfer learning for face recognition with under-represented data

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021.

It may be distributed unchanged freely in print or electronic forms. 2THENGANE ET AL.: CLIMB-3D
1 Introduction
Table Chair Couch Pillow Coffee Table Sofa Chair Recycling BinCabinet Unknown
Task-1 Task-2 Task-3
Figure 1:Overview of the CI-3DIS setting.New object categories are introduced incre-
mentally with each task. After every phase, the model must recognise both newly added
and previously learned classes. For instance, in Task 2, categories such asPillow,Coffee
Table, andSofa Chairare introduced, while the model is expected to retain recognition
of earlier classes likeTable,Chair, andCouch. 3D instance segmentation (3DIS) is a fundamental task in computer vision that involves
identifying precise object boundaries and class labels in 3D space, with broad applications in
graphics, robotics, and augmented reality [4, 39]. Traditional 3DIS methods, including top-
down [28, 70, 78], bottom-up [26, 73], and transformer-based approaches [59], perform well
under the assumption that all classes are available and balanced during training. However,
this assumption does not hold in real-world environments, where new object classes emerge
over time and exhibit natural imbalance. This gap motivates the need for Class-Incremental Learning (CIL) [17, 54], which sup-
ports learning new categories while retaining knowledge of previous ones [51]. While CIL
has seen success in 2D image tasks [1, 43, 54, 60], extensions to 3D point clouds remain
limited, often focusing on object-level classification [13, 18, 48]. Recent works on scene-
level class-incremental 3DIS (CI-3DIS) [4] and class-incremental semantic segmentation
(CI-3DSS) [74] show promise, but depend heavily on large exemplar [56] and overlook class
imbalance, limiting their practicality. To address this, we proposeCLIMB-3D, a unified framework forCLass-incremental
IMBalance-aware3DIS that jointly tackles catastrophic forgetting and class imbalance. The
framework begins with Exemplar Replay (ER), storing a small number of samples from
past classes for later replay. However, this alone does not yield promising results under the
strict memory constraints required in CI-3DIS. To address this, we introduce a Pseudo-Label
Generator (PLG), which uses a frozen model from previous task to generate supervision for
earlier classes. However, we observed that PLG tends to favour frequent classes while ig-
noring under-represented ones. To mitigate this, we propose a Class-Balanced Re-weighting
(CBR), which estimates class frequencies from the pseudo-labels to enhance the learning of
rare classes without accessing past data. Together, these components form a practical and
effective solution for real-world 3DIS. To evaluate CLIMB-3D, we designed three benchmark scenarios for CI-3DIS based on
the ScanNet200 dataset [57]. These scenarios simulate incremental learning setup under nat-
ural class imbalance, where new classes emerge based on (A) object frequency, (B) semantic
similarity, or (C) random grouping. Additionally, for comparison with existing methods,
we evaluate our approach on CI-3DSS using the ScanNetV2 dataset [16]. Experimental re-
sults show that our method significantly reduces forgetting and improves performance across
both frequent and rare categories, outperforming previous methods in both settings. Fig. 1
illustrates the CI-3DIS setup. THENGANE ET AL.: CLIMB-3D3
In summary, our contributions are: (i) a novel problem setting of imbalanced class-
incremental 3DIS with an effective method to balance learning and mitigate forgetting; (ii)
three benchmarks modelling continual object emergence with natural imbalance; and (iii)
strong experimental results, achieving up to 16.76% mAP improvement over baselines. 2 Related Work
3D Instance Segmentation.Various methods have been proposed for 3DIS. Grouping-
based approaches adopt a bottom-up pipeline that learns latent embeddings for point cluster-
ing [9, 21, 22, 29, 37, 44, 70, 78]. In contrast, proposal-based methods follow a top-down
strategy by detecting 3D boxes and segmenting objects within them [19, 26, 46, 73, 76]. Recently, transformer-based models [68] have also been applied to 3DIS and 3DSS [59, 64],
inspired by advances in 2D vision [10, 11]. However, these approaches require full anno-
tations for all classes and are not designed for progressive learning, where only new class
annotations are available and previous data is inaccessible. Another line of work aims to re-
duce annotation costs by proposing weakly supervised 3DIS methods based on sparse cues
[12, 27, 72]. While effective with limited annotations, these methods assume a fixed set of
classes and are susceptible to catastrophic forgetting in incremental settings. Incremental Learning.Continual learning involves training models sequentially to miti-
gate catastrophic forgetting. One common strategy is model regularisation, which constrains
parameter updates using techniques such as elastic weight consolidation or knowledge dis-
tillation [1, 24, 35, 43, 60]. Another approach is exemplar replay, where past data is stored
or generated to rehearse older tasks while learning new ones [5, 7, 30, 54]. A third direc-
tion involves dynamically expanding the model architecture or using modular sub-networks
to accommodate new tasks without interfering with old ones [31, 42, 53, 58, 71, 79]. Re-
cent efforts in class-incremental 3D segmentation [63, 74] have shown early promise but
often rely on basic architectures and focus primarily on semantic segmentation. Other works
[4, 36] explore continual learning in 3D settings, though they often depend on large memory
buffers. In contrast, our work introduces a tailored framework for 3D instance segmentation
that effectively transfers knowledge across tasks, even under class imbalance. Long-tailed Recognition.Imbalanced class distributions lead to poor recognition of rare
(long-tail) classes. To address this, re-sampling methods balance the data via input [8, 20, 50,
52, 61] or feature space [3, 14, 40], avoiding naÃ¯ve under-/over-sampling. Loss re-weighting
offers another direction, using class-based [6, 15, 23, 32, 33, 69] or per-example [45, 55, 62]
adjustments to ensure fair contribution. Parameter regularisation improves generalisation
through weight constraints [2], albeit requiring careful tuning. Other approaches leverage
transfer learning [77, 81], self-supervision [41, 75], or contrastive learning [34, 41, 82] to
improve rare-class representations. Long-tailed recognition is well-studied in 2D with large-
scale datasets [15, 49, 67]; in 3D, ScanNet200 [16, 57] enables similar efforts. Prior 3D
work focused on re-weighting, re-sampling, and transfer learning [57], while regularisation
remains underexplored. CeCo [80] balances class centres via auxiliary loss but overlooks
sampling and augmentation. Lahoud et al. [38] propose adaptive classifier regularisation
for 3D segmentation, outperforming prior approaches without threshold tuning. However,
neither method considers incremental learning. In contrast, we jointly address long-tail and
continual 3D semantic segmentation. 4THENGANE ET AL.: CLIMB-3D
Feature 
Backbone
Transformer 
Decoder
Feature 
Backbone
Transformer 
Decoder
(ð‘·,ð’€ð‘¡)
Current model (ð›·
ð‘¡
)
Pseudo Label Generator (PLG) 
Selection
à·¨ð‘Œð¾
ð‘¡
â„’CIâˆ’3DIS
à·¨ð‘Œð¾
ð‘¡,ð‘¤
à´¤ð‘Œð¾
ð‘¡
à· ð‘Œð‘¡
(ð·ð‘¡)
Current Dataset 
Exemplar 
Replay (ER)
+
~
Class-Balanced  
Re-weighting 
(CBR)
1
ð‘“(ð‘)
Figure 2: Overview ofCLIMB-3Dfor CI-3DIS. The model incrementally learns new classes
across sequential phases. During taskt, point cloudsPand their corresponding labelsY t are
sampled from a combination of the current training datasetD t and Exemplar Replay (ER),
which maintains a small memory of past examples. These are then passed to the current
modelÎ¦ t to produce predicted labelsY t . The Pseudo-Label Generator (PLG) selects the
top-Kpredictions from the previous modelÎ¦ tâˆ’1 . These pseudo-labels are then weighted
based on class frequencyf(c)using Class-Balanced Re-weighting (CBR), and the top-Kre-
weighted labels are selected to form the balanced pseudo-label set Â¯Yt . This pseudo-label set
is then concatenated with the ground-truth labels to form a final augmented supervision set
Yt for taskt, which is used to optimise the modelÎ¦ t using Eq. (3). 3 Methodology
3.1 Problem Formulation
3DIS.The objective of this task is to detect and segment each object instance within a point
cloud. Formally, the training dataset is denoted asD={(P i,Yi)}N
i=1, whereNis the number
of training samples. Each inputP i âˆˆR MÃ—6 is a coloured point cloud consisting ofMpoints,
where each point is represented by its 3D coordinates and RGB values. The corresponding
annotationY i ={(m i,j ,ci,j )}Ji
j=1 containsJ i object instances, wheremi,j âˆˆ {0,1}M is a binary
mask indicating which points belong to thej-th instance, andc i,j âˆˆ C={1,...,C}is the
semantic class label of that instance. GivenP i, the modelÎ¦predicts instance-level outputs
Ë†Yi ={(Ë†mi,j ,Ë†ci,j )}
Ë†Ji
j=1, where Ë†mi,j and Ë†ci,j denote the predicted mask and category label for
thej-th instance, respectively. The number of predicted instances Ë†Ji varies depending on the
modelâ€™s inference. The model is optimised using the following objective:
L3DIS(D;Î¦) = 1
|D| âˆ‘
(P,Y)âˆˆD
1
|Y| âˆ‘
(mj,cj)âˆˆY
(Lmask(mj,Ë†mj)+Î»Â·L cls(cj,Ë†cj)),(1)
whereL cls andL mask are the average classification and mask losses over all instances, andÎ»
controls their trade-off. CI-3DIS.Unlike conventional 3DIS, CI-3DIS involves sequential learning, where object
categories are introduced incrementally overTtraining tasks. Each tasktintroduces a dis-
joint set of classesC t , withC= ST
t=1 Ct andC t âˆ©C tâ€²
=/ 0 fortÌ¸=t â€². During each taskt, the THENGANE ET AL.: CLIMB-3D5
model receives a datasetD t ={(P i,Yt
i)}N
i=1, where each coloured point cloudP i âˆˆR MÃ—6
containsMpoints, and the corresponding annotationY t
i ={(m t
i,j ,ct
i,j )}
Jt
i
j=1 includes instance
masksm t
i,j âˆˆ {0,1}M and semantic class labelsc t
i,j âˆˆ Ct . The modelÎ¦ t , initialized from
Î¦tâˆ’1 , is trained onD t to predict instance-level outputs Ë†Yt
i ={(Ë†mt
i,j ,Ë†ct
i,j )}
Ë†Jt
i
j=1, where each
predicted semantic label Ë†ct
i,j belongs to the cumulative set of all classes observed so far:St
k=1 Ck. The key challenge in this setup is to learn new classes without forgetting those
encountered in previous tasks, despite limited supervision and the absence of past labels. 3.2 Method: CLIMB-3D
Overview.As shown in Fig. 2 and formulated in Sec. 3.1, the proposed framework for
CI-3DIS follows aphase-wisetraining strategy. In each phase, the model is exposed to a
carefully curated subset of the dataset, simulating real-world scenarios discussed in Sec. 3.3. Training naÃ¯vely on such phased data leads tocatastrophic forgetting[51], where the model
forgets prior knowledge when learning new tasks. To address this, we first incorporateEx-
emplar Replay(ER) [5], a strategy inspired by 2D incremental methods [1, 43, 54] and
recently extended to 3D settings [4], to mitigate forgetting by storing a small set of repre-
sentative samples from earlier phases. However, ER alone is insufficient to achieve robust
performance. To address this, two additional components are introduced: aPseudo-Label
Generator(PLG), which leverages a frozen model to generate supervision signals for previ-
ously seen classes, and aClass-Balanced Re-weighting(CBR) module that compensates for
class imbalance across phases. Each of these components is described in detail below 1. Exemplar Replay (ER).Inspired by Buzzega et al. [5], ER addresses the issue of catas-
trophic forgetting by allowing the model to retain a subset of data from earlier phases. During phaset, the model is trained not only on the current task dataD t , but also on a
set of exemplarsE 1:tâˆ’1, collected from previous phases. These exemplars are accumulated
incrementally:E 1:tâˆ’1 =E 1 âˆªÂ·Â·Â·âˆªE tâˆ’1 .Here,E t denotes the exemplar subset stored after
phaset, and|E t |denotes the exemplar replay size. The combined replay dataset is denoted
as:D t
ER =D t âˆªE 1:tâˆ’1.Epoch training consists of two stages: first, the model is trained on
the current data,D t ; then it is trained using exemplars fromE 1:tâˆ’1. While previous CI-3DIS approaches rely on large exemplar sets to mitigate forgetting [4],
such strategies become impractical under memory constraints. Consequently, although ER
aids knowledge retention, it is insufficient on its own for robust performance with a limited
exemplar budget. Therefore, PLG and CBR are introduced to better preserve past represen-
tations and enhance generalisation across phases. Pseudo-Label Generator (PLG).In the CI-3DIS setting, during phaset(wheret>1),
although ground-truth labels for previously seen classes are unavailable, the model from the
previous phaseÎ¦ tâˆ’1 , which preserves knowledge of past tasks, is retained. We use this
model to generate pseudo-labels for previously seen classes, thereby providing approximate
supervision during training. This allows the current modelÎ¦t , to retain prior knowledge and
reduce forgetting, even without access to ground-truth annotations. Given a point cloud and label pair(P,Y t ), fromD t , the previous modelÎ¦ tâˆ’1 , generates
pseudo-labels for the previously seen classes as Ë†Y1:tâˆ’1 =Î¦ tâˆ’1 (P), which we denote as ËœYt
1For clarity, modifications introduced by each component (ER, PLG, and CBR) are highlighted in green, blue,
and red, respectively. 6THENGANE ET AL.: CLIMB-3D
for brevity. However, we observe that utilising all predictions fromÎ¦tâˆ’1 introduces noisy or
incorrect labels, which degrade overall performance. To mitigate this, we select the top-K
most confident instance predictions, denoted as ËœYt
K, and concatenate them with the ground-
truth labels of the current task to obtain the supervision set: Â¯Yt =Y t âˆ¥ ËœYt
K. Following this,
the loss is computed over both the current taskâ€™s ground-truth labels and the top-Kconfi-
dent pseudo-labels from earlier phases, enabling the model to retain prior knowledge while
adapting to new classes. Class-Balanced Re-weighting (CBR).While ER and PLG contribute to preserving prior
knowledge, we observe that the modelâ€™s predictions are biased towards frequent classes,
resulting in the forgetting of rare categories. This issue is further amplified by the top-K
pseudo-labels selected by PLG, which predominantly represent dominant classes in the data. To address this, we propose aClass-Balanced Re-weighting(CBR) scheme that compensates
for class imbalance using object frequency statistics. At taskt, only the current datasetD t
and its label distributionp t (c)over classescâˆˆ Ct are available. Datasets and class distribu-
tions from earlier tasks{D i,p i(c)}tâˆ’1
i=1 are no longer accessible, which makes it challenging
to directly account for class imbalance across all previously seen tasks. Therefore, we pro-
pose to leverage the pseudo-label predictions of the frozen modelÎ¦ tâˆ’1 as a proxy for the
class distribution across previously learned categories. During each training iteration, the previous modelÎ¦ tâˆ’1 is applied to the current inputP
to generate pseudo-labels ËœYt , from which we accumulate class-wise frequency statistics for
previously learned classes. LetC1:t = St
i=1 Ci represent the union of all classes seen up to task
t. At the end of each epoch, we compute the overall class frequencyfâˆˆR |C1:t | by combining
the pseudo-label distribution Ëœpt (c)derived from ËœYt with the ground-truth label distribution
pt (c)from the current task. The resulting frequency vectorf= [f(c)] câˆˆC1:t is then used to
compute class-wise weightsw= [w(c)] câˆˆC1:t , which guide balanced pseudo-label selection
and help the model focus on rare classes. In the next epoch, the modelÎ¦ tâˆ’1 produces soft pseudo-label predictions ËœYt (as de-
scribed in PLG) , where each component ËœYt [c]denotes the confidence score for classcâˆˆ
C1:tâˆ’1. To promote balanced pseudo-label selection, class-wise re-weighting is applied using
w(c) = 1
f(c)+Îµ , whereÎµis a small constant for numerical stability. The adjusted predictions
are computed by applying the class-wise weights as: ËœYt,w [c] =w(c)Â· ËœYt [c],âˆ€câˆˆ C 1:tâˆ’1. Top-Kselection is then applied to both the original and the re-weighted scores, yielding
two pseudo-label sets: ËœYt
K and ËœYt,w
K , respectively. The final augmented target set, Yt , is
constructed by concatenating the ground-truth labelsY t with both sets of pseudo-labels:
Yt =Y t âˆ¥ ËœYt
K âˆ¥ ËœYt,w
K .(2)
Although the above re-weighting mitigates bias from prior tasks, it does not address class
imbalance within the current taskt. To resolve this, the class-balanced weightsw c are ex-
tended to incorporate statistics from both previously seen and current classes, i.e., overC 1:t
rather than onlyC 1:tâˆ’1. The updated weights, denoted asw â€²
c, are used to re-weights both
pseudo-label selection and classification loss, ensuring balanced supervision across all seen
categories, including rare ones. This unified strategy reduces bias and enhances performance
across phases. The pseudo-code for constructing the supervision set in Eq. (2), which incor-
porates both PLG and CBR components, is provided in Appendix A. Final Objective.The final training objective ofCLIMB-3D, incorporating ER, PLG, and THENGANE ET AL.: CLIMB-3D7
CBR, is formulated in Eq. (1) as:
L(Dt
ER;Î¦t ) = 1
|Dt
ER| âˆ‘
(P,Yt )âˆˆDt
ER
1
|Yt | âˆ‘
(mt
j,ct
j)âˆˆYt
 
Lmask(mt
j,Ë†mt
j)+w â€²â€²
c Â·Lcls(ct
j,Ë†ct
j)

,(3)
wherew â€²â€²
c =w â€²
c Â·Î» cls denotes the scaled weight vector, andÎ» cls is a hyperparameter for bal-
ancing the loss terms. 3.3 Benchmarking Incremental Scenarios
While CI-3DIS methods offer a wide range of practical applications, they frequently rely
on the assumption of uniform sample distribution, which rarely holds in real-world settings. In practice, the number of object categories, denoted byC, is often large and characterised
by substantial variability in category frequency, shape, structure, and size. To address these
challenges, we propose three incremental learning scenarios, each designed to capture a
different facet of real-world complexity. The design of these scenarios is detailed below; for
further information and illustrations, refer to Appendix E of the supplementary material. Split-A: Frequency Scenarios.This scenario acknowledges that datasets are often la-
belled based on the frequency of categories. To accommodate this, we propose a split where
the model learns from the most frequent categories and subsequently incorporates the less
frequent ones in later stages. By prioritising the training of frequently occurring categories,
the model can establish a strong foundation before expanding to handle rare categories. Split-B: Semantic Scenarios.Beyond frequency, semantic similarity is crucial in real-
world deployments. While objects often share visual or functional traits, models may en-
counter semantically different categories in new environments. To simulate this challenge,
we introduce theSplit-Bscenario. Here, categories are grouped based on their semantic
relationships, and the model is incrementally trained on one semantic group at a time. This
setup encourages the model to generalise across semantically similar categories and adapt
more effectively when exposed to new ones. Unlike theSplit-Ascenario, which organises
learning based on category frequency, theSplit-Bscenario may contain both frequent and
infrequent categories within a single task, focusing on semantic continuity and transfer. Split-C: Random Scenarios.In some cases, data labelling is driven by the availability of
objects rather than predefined criteria. To capture this, we introduce theSplit-Cscenario,
which represents a fully random setting where any class can appear in any task, resulting in
varying levels of class imbalance. By exposing the model to such diverse and unpredictable
distributions, we aim to improve its robustness in real-world situations where labelled data
availability is inconsistent. These incremental scenarios are designed to provide a more realistic representation of
object distributions, frequencies, and dynamics encountered in the real world. 4 Experiments
4.1 Setup
Datasets.We evaluateCLIMB-3Don ScanNet200 [57], which comprises 200 object cat-
egories and exhibits significant class imbalance, which makes it ideal for simulating and 8THENGANE ET AL.: CLIMB-3D
assessing real-world scenarios. In addition, we compare our approach against existing in-
cremental learning methods using ScanNetV2 [16] in the 3DSS setting. For this evaluation,
we follow the standard training and validation splits defined in prior works [74] to ensure
consistency with existing methods. Evaluation Metrics.We evaluate our method usingMean Average Precision(mAP), a
standard metric for 3DIS which provides a comprehensive measure of segmentation quality
by accounting for both precision and recall. For comparison with existing 3DSS methods,
we report themean Intersection over Union(mIoU), which quantifies the overlap between
predicted and ground truth segments. To assess the modelâ€™s ability to mitigate catastrophic
forgetting in incremental settings, we use theForgetting Percentage Points(FPP) metric, as
defined in [47]. FPP captures performance degradation by measuring the accuracy drop on
the initially seen categories between the first and final training phases. Detailed descriptions of the incremental scenarios (Split-A,Split-B,Split-C) and im-
plementation are provided in Appendix B. 4.2 Results and Discussion
Table 1: Comparison between the proposed method and the baseline in the 3DIS setting,
evaluated using mAP25, mAP50, mAP, and FPP after training across all phase. Scenarios Methods Average Precisionâ†‘FPPâ†“
mAP25 mAP50 mAP mAP 25 mAP50
Baseline 16.46 14.29 10.44 51.30 46.82Split-A CLIMB-3D(Ours) 35.69 31.05 22.72 3.44 2.63
Baseline 17.22 15.07 10.93 46.27 42.1Split-B CLIMB-3D(Ours) 35.48 31.56 23.69 8.00 5.51
Baseline 25.65 21.08 14.85 31.68 28.84Split-C CLIMB-3D(Ours) 31.59 26.78 18.93 9.10 7.89
To evaluate our proposed approach, we construct a baseline using ER [5] for the 3DIS
setting, as no existing incremental baselines are available for this task. In contrast, for the
3DSS setting, where incremental baselines do exist, we compare our method against three
prior approaches [43, 60, 74]. Results on CI-3DIS.Tab. 1 compares CLIMB-3D against the ER baseline across the three
CI-3DIS scenarios, evaluated after all phases using mAP 25, mAP50, overall mAP, and FPP. In theSplit-Ascenario, characterised by significant distribution shifts, our method achieves
gains of +19.23%, +16.76%, and +12.28% in mAP 25, mAP 50, and overall mAP, respec-
tively, while drastically reducing forgetting with FPP scores of 3.44% (mAP 25) and 2.63%
(mAP50), both over 45 points lower than the baseline. Under theSplit-Bscenario, with
semantically related new categories, improvements of +18.26%, +16.49%, and +12.76%
are observed alongside a reduction in forgetting to 8.00% and 5.51% from baseline levels
above 42%. For theSplit-Cscenario, featuring increasing geometric complexity, CLIMB-
3D maintains solid gains of +5.94%, +5.70%, and +4.08% across mAP metrics, with forget-
ting reduced by over 22 points. Overall, these results demonstrate robust forward transfer,
minimal forgetting, and stable learning, highlighting the methodâ€™s effectiveness for scalable
continual 3D semantic understanding. THENGANE ET AL.: CLIMB-3D9
Table 2: Comparison between the proposed method and existing baselines in the 3DSS set-
ting on ScanNetV2 [16], evaluated using mIoU. Methods Phase=1 Phase=2 All
EWC [60] 17.75 13.22 16.62
LwF [43] 30.38 13.37 26.13
Yanget al. [74] 34.16 13.43 28.98
CLIMB-3D(Ours) 69.39 32.56 59.38
Results on CI-3DSS.Although our primary focus is on CI-3DIS, we also evaluate our
method under the CI-3DSS setting to enable comparisons with existing approaches. To do
so, we adapt our predictions to assign each point the label corresponding to the highest-
confidence mask and exclude background classes (e.g., floor and wall), as these are not part
of the semantic segmentation targets. Tab. 2 presents a comparison between our method and
existing baselines on the ScanNet V2 dataset [16], evaluated using mIoU across two training
phases and overall. Although originally designed for instance segmentation, our method
generalises effectively to semantic segmentation, achieving substantial gains of +35.23%
mIoU in Phase 1 and +19.1% in Phase 2. Overall, it achieves 59.38% mIoU, significantly
higher than theâˆ¼30% mIoU of prior methods, highlighting its robustness and transferability. Refer to Appendix C, D, and F of the supplementary material for detailed analyses of per-
phase performance, rare-class evaluation, and qualitative result comparisons, respectively. 4.3 Ablation
Table 3: Ablation study illustrating the impact of each component in a three-phase setup. Each split (s) and corresponding number indicates data introduced at that phase. Best results
are highlighted inbold. Row Modules p=1â†‘p=2â†‘p=3â†‘ FPPâ†“
s1 s1 s2 All s1 s2 s3 All
1. Oracle - - - - 55.14 30.77 25.30 37.68 -
2. NaÃ¯ve 56.82 0.00 28.09 14.15 0.00 0.00 19.67 5.80 56.82
3. + ER 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28 46.44
4. + PLG 56.82 50.0034.3942.13 49.78 11.41 26.47 29.28 7.04
5. + CBR 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05 2.63
We conduct an ablation study to evaluate the contribution of each component in our
framework. As an upper bound, we report the performance of anOraclemodel, which is
trained jointly on the full dataset. For the incremental setting, we follow the previously
defined splits and first train the model naively across phases. We then incrementally add
each module to isolate its impact. Tab. 3 presents results for theSplit-Ascenario using both
mAP50 and FPP metrics. NaÃ¯ve Training.When trained naÃ¯vely without any dedicated modules, the model suffers
from severe catastrophic forgetting, as evident in row 2. The model entirely forgets pre-
viously learned classes upon entering new phases, with performance dropping to zero for
earlier splits and FPP rising to 56.82. 10THENGANE ET AL.: CLIMB-3D
Effect of ER.Adding exemplar replay (row 3) partially alleviates forgetting by maintaining
a buffer of past examples. This yields notable gains fors1in Phase 2 (+18.51%) and Phase 3
(+10.38%), and also improvess2in Phase 3 (+9.43%). However, the overall forgetting
remains substantial (FPP: 46.44), showing ER alone is insufficient. Effect of PLG.The addition of the pseudo-label generator (PLG), which generates labels for
previous classes by retaining a copy of the model from an earlier phase, facilitates knowledge
retention and forward transfer. As shown in row 4, PLG significantly reduces forgetting and
enhances performance on current tasks. Fors1, it improves mAP 50 by 31.49% in Phase 2
and 39.40% in Phase 3 over exemplar replay. Overall, PLG yields a 15.00% increase in
performance and reduces forgetting by 39.40%. Effect of CBR.Finally, class-balanced re-weighting (CBR) mitigates class imbalance dur-
ing both pseudo-labelling and current task learning by adjusting each classâ€™s contribution
based on its frequency. As shown in row 5, CBR enhancess1retention over PLG in Phases 2
and 3 (+4.67% and +4.41%), and further improves performance ons2ands3. It also achieves
the lowest FPP of 2.63, reflecting strong forgetting mitigation. Overall, CBR offers the best
trade-off between retaining past knowledge and acquiring new tasks, outperforming PLG
and ER by 4.41% and 43.81%, respectively. 4.4 Limitations & Future Work
While CLIMB-3D achieves strong performance across CI-3DIS and CI-3DSS settings, sev-
eral limitations remain and warrant discussion. The experiments are currently limited to
indoor datasets (ScanNet200 and ScanNetV2), therefore evaluating performance on outdoor
scenes would help assess generalisability across diverse environments. The current setup
also considers only three incremental phases, whereas real-world applications often involve
longer sequences; incorporating such extended setups would better reflect practical chal-
lenges. Moreover, the model operates in a uni-modal setting. Integrating multi-modal cues,
such as vision-language models, could further enhance performance, as suggested by recent
2D and 3D studies [25, 65, 66]. 5 Conclusion
This work addresses the challenge of catastrophic forgetting in class-incremental 3D in-
stance segmentation by introducing a modular framework that integrates exemplar replay
(ER), a pseudo-label generator (PLG), and class-balanced re-weighting (CBR). The pro-
posed method incrementally adapts to new classes while preserving prior knowledge, all
without requiring access to the full dataset. Extensive experiments on a three-phase bench-
mark validate the individual and combined effectiveness of each component: ER enables
efficient memory-based retention; PLG facilitates knowledge preservation through pseudo-
supervision; and CBR mitigates class imbalance to enhance learning stability. Together,
these components significantly reduce forgetting and improve segmentation performance
across all phases. By achieving a strong balance between stability and plasticity, our ap-
proach advances continual 3D scene understanding and sets a new baseline for future re-
search in this area. THENGANE ET AL.: CLIMB-3D11
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne
Tuytelaars. Memory aware synapses: Learning what (not) to forget. InECCV, pages
139â€“154, 2018. [2] Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long-tailed
recognition via weight balancing. InCVPR, pages 6897â€“6907, 2022. [3] Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying
imbalanced data. InJoint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 770â€“785."
What year was the paper by Xi Yin et al. published?,2019,"Springer,
2016.

Springer, 2017.

Elsevier, 1989.

2019"
How many authors are listed for the paper on face recognition with under-represented data?,"Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker","[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker"
What type of learning is emphasized in the paper by Xi Yin et al.?,Feature transfer learning,"Feature transfer learning

[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021.

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

[30] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for
continual learning.arXiv preprint arXiv:1710.10368, 2017. [31] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence
of similar and dissimilar tasks.NeurIPS, 33:18493â€“18504, 2020. [32] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019. [33] Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and
Roberto Togneri. Cost-sensitive learning of deep feature representations from im-
balanced data.IEEE transactions on neural networks and learning systems, 29(8):
3573â€“3587, 2017. [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 33:18661â€“18673, 2020. [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-
jardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Pro-
ceedings of the national academy of sciences, 114(13):3521â€“3526, 2017. [36] Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual
learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024. [37] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3d instance
segmentation via multi-task metric learning. InICCV, pages 9256â€“9266, 2019. [38] Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and
Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint
and sampling. InInternational Conference on Robotics and Automation (ICRA), pages
5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029. [39] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,
and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022. [40] Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing
Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In
CVPR, pages 5212â€“5221, 2021. [41] Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-
tailed visual recognition. InICCV, pages 630â€“639, 2021. [42] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In
ICML, pages 3925â€“3934."
What specific field does the paper by Xi Yin et al. focus on?,face recognition,"face recognition

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In
CVPR, 2017."
What is the research area addressed in the paper by Xi Yin et al.?,face recognition with under-represented data,"Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

face recognition with under-represented data

It may be distributed unchanged freely in print or electronic forms. 2THENGANE ET AL.: CLIMB-3D
1 Introduction
Table Chair Couch Pillow Coffee Table Sofa Chair Recycling BinCabinet Unknown
Task-1 Task-2 Task-3
Figure 1:Overview of the CI-3DIS setting.New object categories are introduced incre-
mentally with each task. After every phase, the model must recognise both newly added
and previously learned classes. For instance, in Task 2, categories such asPillow,Coffee
Table, andSofa Chairare introduced, while the model is expected to retain recognition
of earlier classes likeTable,Chair, andCouch. 3D instance segmentation (3DIS) is a fundamental task in computer vision that involves
identifying precise object boundaries and class labels in 3D space, with broad applications in
graphics, robotics, and augmented reality [4, 39]. Traditional 3DIS methods, including top-
down [28, 70, 78], bottom-up [26, 73], and transformer-based approaches [59], perform well
under the assumption that all classes are available and balanced during training. However,
this assumption does not hold in real-world environments, where new object classes emerge
over time and exhibit natural imbalance. This gap motivates the need for Class-Incremental Learning (CIL) [17, 54], which sup-
ports learning new categories while retaining knowledge of previous ones [51]. While CIL
has seen success in 2D image tasks [1, 43, 54, 60], extensions to 3D point clouds remain
limited, often focusing on object-level classification [13, 18, 48]. Recent works on scene-
level class-incremental 3DIS (CI-3DIS) [4] and class-incremental semantic segmentation
(CI-3DSS) [74] show promise, but depend heavily on large exemplar [56] and overlook class
imbalance, limiting their practicality. To address this, we proposeCLIMB-3D, a unified framework forCLass-incremental
IMBalance-aware3DIS that jointly tackles catastrophic forgetting and class imbalance. The
framework begins with Exemplar Replay (ER), storing a small number of samples from
past classes for later replay. However, this alone does not yield promising results under the
strict memory constraints required in CI-3DIS. To address this, we introduce a Pseudo-Label
Generator (PLG), which uses a frozen model from previous task to generate supervision for
earlier classes. However, we observed that PLG tends to favour frequent classes while ig-
noring under-represented ones. To mitigate this, we propose a Class-Balanced Re-weighting
(CBR), which estimates class frequencies from the pseudo-labels to enhance the learning of
rare classes without accessing past data. Together, these components form a practical and
effective solution for real-world 3DIS. To evaluate CLIMB-3D, we designed three benchmark scenarios for CI-3DIS based on
the ScanNet200 dataset [57]. These scenarios simulate incremental learning setup under nat-
ural class imbalance, where new classes emerge based on (A) object frequency, (B) semantic
similarity, or (C) random grouping. Additionally, for comparison with existing methods,
we evaluate our approach on CI-3DSS using the ScanNetV2 dataset [16]. Experimental re-
sults show that our method significantly reduces forgetting and improves performance across
both frequent and rare categories, outperforming previous methods in both settings. Fig. 1
illustrates the CI-3DIS setup. THENGANE ET AL.: CLIMB-3D3
In summary, our contributions are: (i) a novel problem setting of imbalanced class-
incremental 3DIS with an effective method to balance learning and mitigate forgetting; (ii)
three benchmarks modelling continual object emergence with natural imbalance; and (iii)
strong experimental results, achieving up to 16.76% mAP improvement over baselines. 2 Related Work
3D Instance Segmentation.Various methods have been proposed for 3DIS. Grouping-
based approaches adopt a bottom-up pipeline that learns latent embeddings for point cluster-
ing [9, 21, 22, 29, 37, 44, 70, 78]. In contrast, proposal-based methods follow a top-down
strategy by detecting 3D boxes and segmenting objects within them [19, 26, 46, 73, 76]. Recently, transformer-based models [68] have also been applied to 3DIS and 3DSS [59, 64],
inspired by advances in 2D vision [10, 11]. However, these approaches require full anno-
tations for all classes and are not designed for progressive learning, where only new class
annotations are available and previous data is inaccessible. Another line of work aims to re-
duce annotation costs by proposing weakly supervised 3DIS methods based on sparse cues
[12, 27, 72]. While effective with limited annotations, these methods assume a fixed set of
classes and are susceptible to catastrophic forgetting in incremental settings. Incremental Learning.Continual learning involves training models sequentially to miti-
gate catastrophic forgetting. One common strategy is model regularisation, which constrains
parameter updates using techniques such as elastic weight consolidation or knowledge dis-
tillation [1, 24, 35, 43, 60]. Another approach is exemplar replay, where past data is stored
or generated to rehearse older tasks while learning new ones [5, 7, 30, 54]. A third direc-
tion involves dynamically expanding the model architecture or using modular sub-networks
to accommodate new tasks without interfering with old ones [31, 42, 53, 58, 71, 79]. Re-
cent efforts in class-incremental 3D segmentation [63, 74] have shown early promise but
often rely on basic architectures and focus primarily on semantic segmentation. Other works
[4, 36] explore continual learning in 3D settings, though they often depend on large memory
buffers. In contrast, our work introduces a tailored framework for 3D instance segmentation
that effectively transfers knowledge across tasks, even under class imbalance. Long-tailed Recognition.Imbalanced class distributions lead to poor recognition of rare
(long-tail) classes. To address this, re-sampling methods balance the data via input [8, 20, 50,
52, 61] or feature space [3, 14, 40], avoiding naÃ¯ve under-/over-sampling. Loss re-weighting
offers another direction, using class-based [6, 15, 23, 32, 33, 69] or per-example [45, 55, 62]
adjustments to ensure fair contribution. Parameter regularisation improves generalisation
through weight constraints [2], albeit requiring careful tuning. Other approaches leverage
transfer learning [77, 81], self-supervision [41, 75], or contrastive learning [34, 41, 82] to
improve rare-class representations. Long-tailed recognition is well-studied in 2D with large-
scale datasets [15, 49, 67]; in 3D, ScanNet200 [16, 57] enables similar efforts. Prior 3D
work focused on re-weighting, re-sampling, and transfer learning [57], while regularisation
remains underexplored. CeCo [80] balances class centres via auxiliary loss but overlooks
sampling and augmentation. Lahoud et al. [38] propose adaptive classifier regularisation
for 3D segmentation, outperforming prior approaches without threshold tuning. However,
neither method considers incremental learning. In contrast, we jointly address long-tail and
continual 3D semantic segmentation. 4THENGANE ET AL.: CLIMB-3D
Feature 
Backbone
Transformer 
Decoder
Feature 
Backbone
Transformer 
Decoder
(ð‘·,ð’€ð‘¡)
Current model (ð›·
ð‘¡
)
Pseudo Label Generator (PLG) 
Selection
à·¨ð‘Œð¾
ð‘¡
â„’CIâˆ’3DIS
à·¨ð‘Œð¾
ð‘¡,ð‘¤
à´¤ð‘Œð¾
ð‘¡
à· ð‘Œð‘¡
(ð·ð‘¡)
Current Dataset 
Exemplar 
Replay (ER)
+
~
Class-Balanced  
Re-weighting 
(CBR)
1
ð‘“(ð‘)
Figure 2: Overview ofCLIMB-3Dfor CI-3DIS. The model incrementally learns new classes
across sequential phases. During taskt, point cloudsPand their corresponding labelsY t are
sampled from a combination of the current training datasetD t and Exemplar Replay (ER),
which maintains a small memory of past examples. These are then passed to the current
modelÎ¦ t to produce predicted labelsY t . The Pseudo-Label Generator (PLG) selects the
top-Kpredictions from the previous modelÎ¦ tâˆ’1 . These pseudo-labels are then weighted
based on class frequencyf(c)using Class-Balanced Re-weighting (CBR), and the top-Kre-
weighted labels are selected to form the balanced pseudo-label set Â¯Yt . This pseudo-label set
is then concatenated with the ground-truth labels to form a final augmented supervision set
Yt for taskt, which is used to optimise the modelÎ¦ t using Eq. (3). 3 Methodology
3.1 Problem Formulation
3DIS.The objective of this task is to detect and segment each object instance within a point
cloud. Formally, the training dataset is denoted asD={(P i,Yi)}N
i=1, whereNis the number
of training samples. Each inputP i âˆˆR MÃ—6 is a coloured point cloud consisting ofMpoints,
where each point is represented by its 3D coordinates and RGB values. The corresponding
annotationY i ={(m i,j ,ci,j )}Ji
j=1 containsJ i object instances, wheremi,j âˆˆ {0,1}M is a binary
mask indicating which points belong to thej-th instance, andc i,j âˆˆ C={1,...,C}is the
semantic class label of that instance. GivenP i, the modelÎ¦predicts instance-level outputs
Ë†Yi ={(Ë†mi,j ,Ë†ci,j )}
Ë†Ji
j=1, where Ë†mi,j and Ë†ci,j denote the predicted mask and category label for
thej-th instance, respectively. The number of predicted instances Ë†Ji varies depending on the
modelâ€™s inference. The model is optimised using the following objective:
L3DIS(D;Î¦) = 1
|D| âˆ‘
(P,Y)âˆˆD
1
|Y| âˆ‘
(mj,cj)âˆˆY
(Lmask(mj,Ë†mj)+Î»Â·L cls(cj,Ë†cj)),(1)
whereL cls andL mask are the average classification and mask losses over all instances, andÎ»
controls their trade-off. CI-3DIS.Unlike conventional 3DIS, CI-3DIS involves sequential learning, where object
categories are introduced incrementally overTtraining tasks. Each tasktintroduces a dis-
joint set of classesC t , withC= ST
t=1 Ct andC t âˆ©C tâ€²
=/ 0 fortÌ¸=t â€². During each taskt, the THENGANE ET AL.: CLIMB-3D5
model receives a datasetD t ={(P i,Yt
i)}N
i=1, where each coloured point cloudP i âˆˆR MÃ—6
containsMpoints, and the corresponding annotationY t
i ={(m t
i,j ,ct
i,j )}
Jt
i
j=1 includes instance
masksm t
i,j âˆˆ {0,1}M and semantic class labelsc t
i,j âˆˆ Ct . The modelÎ¦ t , initialized from
Î¦tâˆ’1 , is trained onD t to predict instance-level outputs Ë†Yt
i ={(Ë†mt
i,j ,Ë†ct
i,j )}
Ë†Jt
i
j=1, where each
predicted semantic label Ë†ct
i,j belongs to the cumulative set of all classes observed so far:St
k=1 Ck. The key challenge in this setup is to learn new classes without forgetting those
encountered in previous tasks, despite limited supervision and the absence of past labels. 3.2 Method: CLIMB-3D
Overview.As shown in Fig. 2 and formulated in Sec. 3.1, the proposed framework for
CI-3DIS follows aphase-wisetraining strategy. In each phase, the model is exposed to a
carefully curated subset of the dataset, simulating real-world scenarios discussed in Sec. 3.3. Training naÃ¯vely on such phased data leads tocatastrophic forgetting[51], where the model
forgets prior knowledge when learning new tasks. To address this, we first incorporateEx-
emplar Replay(ER) [5], a strategy inspired by 2D incremental methods [1, 43, 54] and
recently extended to 3D settings [4], to mitigate forgetting by storing a small set of repre-
sentative samples from earlier phases. However, ER alone is insufficient to achieve robust
performance. To address this, two additional components are introduced: aPseudo-Label
Generator(PLG), which leverages a frozen model to generate supervision signals for previ-
ously seen classes, and aClass-Balanced Re-weighting(CBR) module that compensates for
class imbalance across phases. Each of these components is described in detail below 1. Exemplar Replay (ER).Inspired by Buzzega et al. [5], ER addresses the issue of catas-
trophic forgetting by allowing the model to retain a subset of data from earlier phases. During phaset, the model is trained not only on the current task dataD t , but also on a
set of exemplarsE 1:tâˆ’1, collected from previous phases. These exemplars are accumulated
incrementally:E 1:tâˆ’1 =E 1 âˆªÂ·Â·Â·âˆªE tâˆ’1 .Here,E t denotes the exemplar subset stored after
phaset, and|E t |denotes the exemplar replay size. The combined replay dataset is denoted
as:D t
ER =D t âˆªE 1:tâˆ’1.Epoch training consists of two stages: first, the model is trained on
the current data,D t ; then it is trained using exemplars fromE 1:tâˆ’1. While previous CI-3DIS approaches rely on large exemplar sets to mitigate forgetting [4],
such strategies become impractical under memory constraints. Consequently, although ER
aids knowledge retention, it is insufficient on its own for robust performance with a limited
exemplar budget. Therefore, PLG and CBR are introduced to better preserve past represen-
tations and enhance generalisation across phases. Pseudo-Label Generator (PLG).In the CI-3DIS setting, during phaset(wheret>1),
although ground-truth labels for previously seen classes are unavailable, the model from the
previous phaseÎ¦ tâˆ’1 , which preserves knowledge of past tasks, is retained. We use this
model to generate pseudo-labels for previously seen classes, thereby providing approximate
supervision during training. This allows the current modelÎ¦t , to retain prior knowledge and
reduce forgetting, even without access to ground-truth annotations. Given a point cloud and label pair(P,Y t ), fromD t , the previous modelÎ¦ tâˆ’1 , generates
pseudo-labels for the previously seen classes as Ë†Y1:tâˆ’1 =Î¦ tâˆ’1 (P), which we denote as ËœYt
1For clarity, modifications introduced by each component (ER, PLG, and CBR) are highlighted in green, blue,
and red, respectively. 6THENGANE ET AL.: CLIMB-3D
for brevity. However, we observe that utilising all predictions fromÎ¦tâˆ’1 introduces noisy or
incorrect labels, which degrade overall performance. To mitigate this, we select the top-K
most confident instance predictions, denoted as ËœYt
K, and concatenate them with the ground-
truth labels of the current task to obtain the supervision set: Â¯Yt =Y t âˆ¥ ËœYt
K. Following this,
the loss is computed over both the current taskâ€™s ground-truth labels and the top-Kconfi-
dent pseudo-labels from earlier phases, enabling the model to retain prior knowledge while
adapting to new classes. Class-Balanced Re-weighting (CBR).While ER and PLG contribute to preserving prior
knowledge, we observe that the modelâ€™s predictions are biased towards frequent classes,
resulting in the forgetting of rare categories. This issue is further amplified by the top-K
pseudo-labels selected by PLG, which predominantly represent dominant classes in the data. To address this, we propose aClass-Balanced Re-weighting(CBR) scheme that compensates
for class imbalance using object frequency statistics. At taskt, only the current datasetD t
and its label distributionp t (c)over classescâˆˆ Ct are available. Datasets and class distribu-
tions from earlier tasks{D i,p i(c)}tâˆ’1
i=1 are no longer accessible, which makes it challenging
to directly account for class imbalance across all previously seen tasks. Therefore, we pro-
pose to leverage the pseudo-label predictions of the frozen modelÎ¦ tâˆ’1 as a proxy for the
class distribution across previously learned categories. During each training iteration, the previous modelÎ¦ tâˆ’1 is applied to the current inputP
to generate pseudo-labels ËœYt , from which we accumulate class-wise frequency statistics for
previously learned classes. LetC1:t = St
i=1 Ci represent the union of all classes seen up to task
t. At the end of each epoch, we compute the overall class frequencyfâˆˆR |C1:t | by combining
the pseudo-label distribution Ëœpt (c)derived from ËœYt with the ground-truth label distribution
pt (c)from the current task. The resulting frequency vectorf= [f(c)] câˆˆC1:t is then used to
compute class-wise weightsw= [w(c)] câˆˆC1:t , which guide balanced pseudo-label selection
and help the model focus on rare classes. In the next epoch, the modelÎ¦ tâˆ’1 produces soft pseudo-label predictions ËœYt (as de-
scribed in PLG) , where each component ËœYt [c]denotes the confidence score for classcâˆˆ
C1:tâˆ’1. To promote balanced pseudo-label selection, class-wise re-weighting is applied using
w(c) = 1
f(c)+Îµ , whereÎµis a small constant for numerical stability. The adjusted predictions
are computed by applying the class-wise weights as: ËœYt,w [c] =w(c)Â· ËœYt [c],âˆ€câˆˆ C 1:tâˆ’1. Top-Kselection is then applied to both the original and the re-weighted scores, yielding
two pseudo-label sets: ËœYt
K and ËœYt,w
K , respectively. The final augmented target set, Yt , is
constructed by concatenating the ground-truth labelsY t with both sets of pseudo-labels:
Yt =Y t âˆ¥ ËœYt
K âˆ¥ ËœYt,w
K .(2)
Although the above re-weighting mitigates bias from prior tasks, it does not address class
imbalance within the current taskt. To resolve this, the class-balanced weightsw c are ex-
tended to incorporate statistics from both previously seen and current classes, i.e., overC 1:t
rather than onlyC 1:tâˆ’1. The updated weights, denoted asw â€²
c, are used to re-weights both
pseudo-label selection and classification loss, ensuring balanced supervision across all seen
categories, including rare ones. This unified strategy reduces bias and enhances performance
across phases. The pseudo-code for constructing the supervision set in Eq. (2), which incor-
porates both PLG and CBR components, is provided in Appendix A. Final Objective.The final training objective ofCLIMB-3D, incorporating ER, PLG, and THENGANE ET AL.: CLIMB-3D7
CBR, is formulated in Eq. (1) as:
L(Dt
ER;Î¦t ) = 1
|Dt
ER| âˆ‘
(P,Yt )âˆˆDt
ER
1
|Yt | âˆ‘
(mt
j,ct
j)âˆˆYt
 
Lmask(mt
j,Ë†mt
j)+w â€²â€²
c Â·Lcls(ct
j,Ë†ct
j)

,(3)
wherew â€²â€²
c =w â€²
c Â·Î» cls denotes the scaled weight vector, andÎ» cls is a hyperparameter for bal-
ancing the loss terms. 3.3 Benchmarking Incremental Scenarios
While CI-3DIS methods offer a wide range of practical applications, they frequently rely
on the assumption of uniform sample distribution, which rarely holds in real-world settings. In practice, the number of object categories, denoted byC, is often large and characterised
by substantial variability in category frequency, shape, structure, and size. To address these
challenges, we propose three incremental learning scenarios, each designed to capture a
different facet of real-world complexity. The design of these scenarios is detailed below; for
further information and illustrations, refer to Appendix E of the supplementary material. Split-A: Frequency Scenarios.This scenario acknowledges that datasets are often la-
belled based on the frequency of categories. To accommodate this, we propose a split where
the model learns from the most frequent categories and subsequently incorporates the less
frequent ones in later stages. By prioritising the training of frequently occurring categories,
the model can establish a strong foundation before expanding to handle rare categories. Split-B: Semantic Scenarios.Beyond frequency, semantic similarity is crucial in real-
world deployments. While objects often share visual or functional traits, models may en-
counter semantically different categories in new environments. To simulate this challenge,
we introduce theSplit-Bscenario. Here, categories are grouped based on their semantic
relationships, and the model is incrementally trained on one semantic group at a time. This
setup encourages the model to generalise across semantically similar categories and adapt
more effectively when exposed to new ones. Unlike theSplit-Ascenario, which organises
learning based on category frequency, theSplit-Bscenario may contain both frequent and
infrequent categories within a single task, focusing on semantic continuity and transfer. Split-C: Random Scenarios.In some cases, data labelling is driven by the availability of
objects rather than predefined criteria. To capture this, we introduce theSplit-Cscenario,
which represents a fully random setting where any class can appear in any task, resulting in
varying levels of class imbalance. By exposing the model to such diverse and unpredictable
distributions, we aim to improve its robustness in real-world situations where labelled data
availability is inconsistent. These incremental scenarios are designed to provide a more realistic representation of
object distributions, frequencies, and dynamics encountered in the real world. 4 Experiments
4.1 Setup
Datasets.We evaluateCLIMB-3Don ScanNet200 [57], which comprises 200 object cat-
egories and exhibits significant class imbalance, which makes it ideal for simulating and 8THENGANE ET AL.: CLIMB-3D
assessing real-world scenarios. In addition, we compare our approach against existing in-
cremental learning methods using ScanNetV2 [16] in the 3DSS setting. For this evaluation,
we follow the standard training and validation splits defined in prior works [74] to ensure
consistency with existing methods. Evaluation Metrics.We evaluate our method usingMean Average Precision(mAP), a
standard metric for 3DIS which provides a comprehensive measure of segmentation quality
by accounting for both precision and recall. For comparison with existing 3DSS methods,
we report themean Intersection over Union(mIoU), which quantifies the overlap between
predicted and ground truth segments. To assess the modelâ€™s ability to mitigate catastrophic
forgetting in incremental settings, we use theForgetting Percentage Points(FPP) metric, as
defined in [47]. FPP captures performance degradation by measuring the accuracy drop on
the initially seen categories between the first and final training phases. Detailed descriptions of the incremental scenarios (Split-A,Split-B,Split-C) and im-
plementation are provided in Appendix B. 4.2 Results and Discussion
Table 1: Comparison between the proposed method and the baseline in the 3DIS setting,
evaluated using mAP25, mAP50, mAP, and FPP after training across all phase. Scenarios Methods Average Precisionâ†‘FPPâ†“
mAP25 mAP50 mAP mAP 25 mAP50
Baseline 16.46 14.29 10.44 51.30 46.82Split-A CLIMB-3D(Ours) 35.69 31.05 22.72 3.44 2.63
Baseline 17.22 15.07 10.93 46.27 42.1Split-B CLIMB-3D(Ours) 35.48 31.56 23.69 8.00 5.51
Baseline 25.65 21.08 14.85 31.68 28.84Split-C CLIMB-3D(Ours) 31.59 26.78 18.93 9.10 7.89
To evaluate our proposed approach, we construct a baseline using ER [5] for the 3DIS
setting, as no existing incremental baselines are available for this task. In contrast, for the
3DSS setting, where incremental baselines do exist, we compare our method against three
prior approaches [43, 60, 74]. Results on CI-3DIS.Tab. 1 compares CLIMB-3D against the ER baseline across the three
CI-3DIS scenarios, evaluated after all phases using mAP 25, mAP50, overall mAP, and FPP. In theSplit-Ascenario, characterised by significant distribution shifts, our method achieves
gains of +19.23%, +16.76%, and +12.28% in mAP 25, mAP 50, and overall mAP, respec-
tively, while drastically reducing forgetting with FPP scores of 3.44% (mAP 25) and 2.63%
(mAP50), both over 45 points lower than the baseline. Under theSplit-Bscenario, with
semantically related new categories, improvements of +18.26%, +16.49%, and +12.76%
are observed alongside a reduction in forgetting to 8.00% and 5.51% from baseline levels
above 42%. For theSplit-Cscenario, featuring increasing geometric complexity, CLIMB-
3D maintains solid gains of +5.94%, +5.70%, and +4.08% across mAP metrics, with forget-
ting reduced by over 22 points. Overall, these results demonstrate robust forward transfer,
minimal forgetting, and stable learning, highlighting the methodâ€™s effectiveness for scalable
continual 3D semantic understanding. THENGANE ET AL.: CLIMB-3D9
Table 2: Comparison between the proposed method and existing baselines in the 3DSS set-
ting on ScanNetV2 [16], evaluated using mIoU. Methods Phase=1 Phase=2 All
EWC [60] 17.75 13.22 16.62
LwF [43] 30.38 13.37 26.13
Yanget al. [74] 34.16 13.43 28.98
CLIMB-3D(Ours) 69.39 32.56 59.38
Results on CI-3DSS.Although our primary focus is on CI-3DIS, we also evaluate our
method under the CI-3DSS setting to enable comparisons with existing approaches. To do
so, we adapt our predictions to assign each point the label corresponding to the highest-
confidence mask and exclude background classes (e.g., floor and wall), as these are not part
of the semantic segmentation targets. Tab. 2 presents a comparison between our method and
existing baselines on the ScanNet V2 dataset [16], evaluated using mIoU across two training
phases and overall. Although originally designed for instance segmentation, our method
generalises effectively to semantic segmentation, achieving substantial gains of +35.23%
mIoU in Phase 1 and +19.1% in Phase 2. Overall, it achieves 59.38% mIoU, significantly
higher than theâˆ¼30% mIoU of prior methods, highlighting its robustness and transferability. Refer to Appendix C, D, and F of the supplementary material for detailed analyses of per-
phase performance, rare-class evaluation, and qualitative result comparisons, respectively. 4.3 Ablation
Table 3: Ablation study illustrating the impact of each component in a three-phase setup. Each split (s) and corresponding number indicates data introduced at that phase. Best results
are highlighted inbold. Row Modules p=1â†‘p=2â†‘p=3â†‘ FPPâ†“
s1 s1 s2 All s1 s2 s3 All
1. Oracle - - - - 55.14 30.77 25.30 37.68 -
2. NaÃ¯ve 56.82 0.00 28.09 14.15 0.00 0.00 19.67 5.80 56.82
3. + ER 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28 46.44
4. + PLG 56.82 50.0034.3942.13 49.78 11.41 26.47 29.28 7.04
5. + CBR 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05 2.63
We conduct an ablation study to evaluate the contribution of each component in our
framework. As an upper bound, we report the performance of anOraclemodel, which is
trained jointly on the full dataset. For the incremental setting, we follow the previously
defined splits and first train the model naively across phases. We then incrementally add
each module to isolate its impact. Tab. 3 presents results for theSplit-Ascenario using both
mAP50 and FPP metrics. NaÃ¯ve Training.When trained naÃ¯vely without any dedicated modules, the model suffers
from severe catastrophic forgetting, as evident in row 2. The model entirely forgets pre-
viously learned classes upon entering new phases, with performance dropping to zero for
earlier splits and FPP rising to 56.82. 10THENGANE ET AL.: CLIMB-3D
Effect of ER.Adding exemplar replay (row 3) partially alleviates forgetting by maintaining
a buffer of past examples. This yields notable gains fors1in Phase 2 (+18.51%) and Phase 3
(+10.38%), and also improvess2in Phase 3 (+9.43%). However, the overall forgetting
remains substantial (FPP: 46.44), showing ER alone is insufficient. Effect of PLG.The addition of the pseudo-label generator (PLG), which generates labels for
previous classes by retaining a copy of the model from an earlier phase, facilitates knowledge
retention and forward transfer. As shown in row 4, PLG significantly reduces forgetting and
enhances performance on current tasks. Fors1, it improves mAP 50 by 31.49% in Phase 2
and 39.40% in Phase 3 over exemplar replay. Overall, PLG yields a 15.00% increase in
performance and reduces forgetting by 39.40%. Effect of CBR.Finally, class-balanced re-weighting (CBR) mitigates class imbalance dur-
ing both pseudo-labelling and current task learning by adjusting each classâ€™s contribution
based on its frequency. As shown in row 5, CBR enhancess1retention over PLG in Phases 2
and 3 (+4.67% and +4.41%), and further improves performance ons2ands3. It also achieves
the lowest FPP of 2.63, reflecting strong forgetting mitigation. Overall, CBR offers the best
trade-off between retaining past knowledge and acquiring new tasks, outperforming PLG
and ER by 4.41% and 43.81%, respectively. 4.4 Limitations & Future Work
While CLIMB-3D achieves strong performance across CI-3DIS and CI-3DSS settings, sev-
eral limitations remain and warrant discussion. The experiments are currently limited to
indoor datasets (ScanNet200 and ScanNetV2), therefore evaluating performance on outdoor
scenes would help assess generalisability across diverse environments. The current setup
also considers only three incremental phases, whereas real-world applications often involve
longer sequences; incorporating such extended setups would better reflect practical chal-
lenges. Moreover, the model operates in a uni-modal setting. Integrating multi-modal cues,
such as vision-language models, could further enhance performance, as suggested by recent
2D and 3D studies [25, 65, 66]. 5 Conclusion
This work addresses the challenge of catastrophic forgetting in class-incremental 3D in-
stance segmentation by introducing a modular framework that integrates exemplar replay
(ER), a pseudo-label generator (PLG), and class-balanced re-weighting (CBR). The pro-
posed method incrementally adapts to new classes while preserving prior knowledge, all
without requiring access to the full dataset. Extensive experiments on a three-phase bench-
mark validate the individual and combined effectiveness of each component: ER enables
efficient memory-based retention; PLG facilitates knowledge preservation through pseudo-
supervision; and CBR mitigates class imbalance to enhance learning stability. Together,
these components significantly reduce forgetting and improve segmentation performance
across all phases. By achieving a strong balance between stability and plasticity, our ap-
proach advances continual 3D scene understanding and sets a new baseline for future re-
search in this area. THENGANE ET AL.: CLIMB-3D11
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne
Tuytelaars. Memory aware synapses: Learning what (not) to forget. InECCV, pages
139â€“154, 2018. [2] Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long-tailed
recognition via weight balancing. InCVPR, pages 6897â€“6907, 2022. [3] Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying
imbalanced data. InJoint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 770â€“785.

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019."
What is the title of the paper by Jianggang Zhu et al.?,"Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang Jiang. Balanced contrastive learning for long-tailed visual recognition. InCVPR, pages 6908â€“6917, 2022.","[30] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for
continual learning.arXiv preprint arXiv:1710.10368, 2017. [31] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence
of similar and dissimilar tasks.NeurIPS, 33:18493â€“18504, 2020. [32] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019. [33] Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and
Roberto Togneri. Cost-sensitive learning of deep feature representations from im-
balanced data.IEEE transactions on neural networks and learning systems, 29(8):
3573â€“3587, 2017. [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 33:18661â€“18673, 2020. [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-
jardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Pro-
ceedings of the national academy of sciences, 114(13):3521â€“3526, 2017. [36] Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual
learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024. [37] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3d instance
segmentation via multi-task metric learning. InICCV, pages 9256â€“9266, 2019. [38] Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and
Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint
and sampling. InInternational Conference on Robotics and Automation (ICRA), pages
5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029. [39] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,
and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022. [40] Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing
Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In
CVPR, pages 5212â€“5221, 2021. [41] Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-
tailed visual recognition. InICCV, pages 630â€“639, 2021. [42] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In
ICML, pages 3925â€“3934.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang Jiang. Balanced contrastive learning for long-tailed visual recognition. InCVPR, pages 6908â€“6917, 2022.

[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021."
In which conference was the paper by Boyan Zhou et al. published?,"Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition. InCVPR, pages 9719â€“9728, 2020.","[30] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for
continual learning.arXiv preprint arXiv:1710.10368, 2017. [31] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence
of similar and dissimilar tasks.NeurIPS, 33:18493â€“18504, 2020. [32] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019. [33] Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and
Roberto Togneri. Cost-sensitive learning of deep feature representations from im-
balanced data.IEEE transactions on neural networks and learning systems, 29(8):
3573â€“3587, 2017. [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 33:18661â€“18673, 2020. [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-
jardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Pro-
ceedings of the national academy of sciences, 114(13):3521â€“3526, 2017. [36] Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual
learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024. [37] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3d instance
segmentation via multi-task metric learning. InICCV, pages 9256â€“9266, 2019. [38] Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and
Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint
and sampling. InInternational Conference on Robotics and Automation (ICRA), pages
5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029. [39] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,
and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022. [40] Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing
Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In
CVPR, pages 5212â€“5221, 2021. [41] Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-
tailed visual recognition. InICCV, pages 630â€“639, 2021. [42] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In
ICML, pages 3925â€“3934.

[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021.

Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition. InCVPR, pages 9719â€“9728, 2020.

[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019."
What is the topic of the paper by Tingting Zhao et al.?,"Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsupervised lifelong learning. Neural Networks, 149:95â€“106, 2022.","[30] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for
continual learning.arXiv preprint arXiv:1710.10368, 2017. [31] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence
of similar and dissimilar tasks.NeurIPS, 33:18493â€“18504, 2020. [32] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019. [33] Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and
Roberto Togneri. Cost-sensitive learning of deep feature representations from im-
balanced data.IEEE transactions on neural networks and learning systems, 29(8):
3573â€“3587, 2017. [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 33:18661â€“18673, 2020. [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-
jardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Pro-
ceedings of the national academy of sciences, 114(13):3521â€“3526, 2017. [36] Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual
learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024. [37] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3d instance
segmentation via multi-task metric learning. InICCV, pages 9256â€“9266, 2019. [38] Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and
Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint
and sampling. InInternational Conference on Robotics and Automation (ICRA), pages
5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029. [39] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,
and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022. [40] Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing
Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In
CVPR, pages 5212â€“5221, 2021. [41] Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-
tailed visual recognition. InICCV, pages 630â€“639, 2021. [42] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In
ICML, pages 3925â€“3934.

Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsupervised lifelong learning. Neural Networks, 149:95â€“106, 2022.

[70] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity
group proposal network for 3d point cloud instance segmentation. InProceedings of
the IEEE conference on computer vision and pattern recognition, pages 2569â€“2578,
2018. 16THENGANE ET AL.: CLIMB-3D
[71] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis
Ioannidis. Learn-prune-share for lifelong learning. In2020 IEEE International Con-
ference on Data Mining (ICDM), pages 641â€“650.

[46] Shih-Hung Liu, Shang-Yi Yu, Shao-Chi Wu, Hwann-Tzong Chen, and Tyng-Luh
Liu. Learning gaussian instance segmentation in point clouds.arXiv preprint
arXiv:2007.09860, 2020. [47] Yaoyao Liu, Bernt Schiele, Andrea Vedaldi, and Christian Rupprecht. Continual de-
tection transformer for incremental object detection. InCVPR, pages 23799â€“23808,
2023. [48] Yuyang Liu, Yang Cong, Gan Sun, Tao Zhang, Jiahua Dong, and Hongsen Liu. L3doc:
Lifelong 3d object classification.ICIP, 30:7486â€“7498, 2021. [49] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X
Yu. Large-scale long-tailed recognition in an open world. InCVPR, pages 2537â€“2546,
2019. [50] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of
weakly supervised pretraining. InECCV, pages 181â€“196, 2018. [51] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist net-
works: The sequential learning problem. InPsychology of learning and motivation,
volume 24, pages 109â€“165."
Which year was the paper by Biao Zhang and Peter Wonka published?,"Biao Zhang and Peter Wonka. Point cloud instance segmentation using probabilistic embeddings. InCVPR, pages 8883â€“8892, 2021.","Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

IEEE, 2020. [72] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. InECCV,
pages 574â€“591.

Biao Zhang and Peter Wonka. Point cloud instance segmentation using probabilistic embeddings. InCVPR, pages 8883â€“8892, 2021."
What is the focus of the paper by Zhisheng Zhong et al.?,"Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang, and Jiaya Jia. Understanding imbalanced semantic segmentation through neural collapse. InCVPR, pages 19550â€“19560, 2023.","[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021.

Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang, and Jiaya Jia. Understanding imbalanced semantic segmentation through neural collapse. InCVPR, pages 19550â€“19560, 2023.

Springer, 2020. [73] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham,
and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on
point clouds.NeurIPS, 32, 2019. [74] Yuwei Yang, Munawar Hayat, Zhao Jin, Chao Ren, and Yinjie Lei. Geometry and
uncertainty-aware 3d point cloud class-incremental semantic segmentation. InCVPR,
pages 21759â€“21768, June 2023. [75] Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced
learning.NeurIPS, 33:19290â€“19301, 2020. [76] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative
shape proposal network for 3d instance segmentation in point cloud. InCVPR, pages
3947â€“3956, 2019.

[17] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: De-
fying forgetting in classification tasks.IEEE PAMI, 44(7):3366â€“3385, 2021. [18] Jiahua Dong, Yang Cong, Gan Sun, Bingtao Ma, and Lichen Wang. I3dol: Incremental
3d object learning without catastrophic forgetting. InAAAI, volume 35, pages 6066â€“
6074, 2021. [19] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias
NieÃŸner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. InCVPR, pages 9031â€“9040, 2020. [20] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary
instance segmentation. InCVPR, pages 5356â€“5364, 2019. [21] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance
segmentation. InCVPR, pages 2940â€“2949, 2020. [22] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3d: Robust instance seg-
mentation of 3d point clouds through dynamic convolution. InCVPR, pages 354â€“363,
2021. [23] Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, and Jian Sun. Relieving
long-tailed instance segmentation via pairwise class balance. InCVPR, pages 7000â€“
7009, 2022. [24] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural
network.arXiv preprint arXiv:1503.02531, 2(7), 2015. [25] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen,
and Chuang Gan. 3d-llm: Injecting the 3d world into large language models.NeurIPS,
36:20482â€“20494, 2023. [26] Ji Hou, Angela Dai, and Matthias NieÃŸner. 3d-sis: 3d semantic instance segmentation
of rgb-d scans. InCVPR, pages 4421â€“4430, 2019. [27] Ji Hou, Benjamin Graham, Matthias NieÃŸner, and Saining Xie. Exploring data-efficient
3d scene understanding with contrastive scene contexts. InCVPR, pages 15587â€“15597,
2021. [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language rep-
resentation learning with noisy text supervision. InICML, pages 4904â€“4916."
Who are the authors of the paper on Class-Incremental Imbalanced 3D Instance Segmentation?,THENGANE ET AL.: CLIMB-3D1 CLIMB-3D: Class-Incremental Imbalanced 3D Instance Segmentation Supplementary Material,"[78] Biao Zhang and Peter Wonka. Point cloud instance segmentation using probabilistic
embeddings. InCVPR, pages 8883â€“8892, 2021. [79] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsuper-
vised lifelong learning.Neural Networks, 149:95â€“106, 2022. [80] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang,
and Jiaya Jia. Understanding imbalanced semantic segmentation through neural col-
lapse. InCVPR, pages 19550â€“19560, 2023. [81] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch
network with cumulative learning for long-tailed visual recognition. InCVPR, pages
9719â€“9728, 2020. [82] Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang
Jiang. Balanced contrastive learning for long-tailed visual recognition. InCVPR, pages
6908â€“6917, 2022. THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced 3D Instance
Segmentation
Supplementary Material
Appendix A Illustrating Incremental scenarios
Frequent Scenario Semantic Scenario
background
frequent common
rare semantic 
cluster 1
semantic 
cluster 2
semantic 
cluster 3
background
random 
cluster 1
random 
cluster 2
random 
cluster 3
background
Random Scenario
Figure 3: Tasks are grouped into incremental scenarios based on object frequency, semantic
similarity, and random assignment. , , and denote different tasks; shapes indicate
object categories; marks the background.Left:Grouped by category frequency.Middle:
Grouped by semantic similarity (e.g., similar shapes).Right:Randomly grouped, mixing
semantic and frequency variations. Appendix B Per Phase Analysis
We extend the analysis from Tab.

THENGANE ET AL.: CLIMB-3D1 CLIMB-3D: Class-Incremental Imbalanced 3D Instance Segmentation Supplementary Material

arXiv:2502.17429v3  [cs.CV]  21 Nov 2025
THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced
3D Instance Segmentation
Vishal Thengane1
v.thengane@surrey.ac.uk
Jean Lahoud2
jean.lahoud@mbzuai.ac.ae
Hisham Cholakkal2
hisham.cholakkal@mbzuai.ac.ae
Rao Muhammad Anwer2
rao.anwer@mbzuai.ac.ae
Lu Yin1
l.yin@surrey.ac.uk
Xiatian Zhu1
xiatian.zhu@surrey.ac.uk
Salman Khan2, 3
salman.khan@mbzuai.ac.ae
1 University of Surrey,
Guildford, UK
2 Mohamed bin Zayed University of
Artificial Intelligence,
Abu Dhabi, UAE
3 Australian National University,
Canberra, Australia
Abstract
While 3D instance segmentation (3DIS) has advanced significantly, most existing
methods assume that all object classes are known in advance and uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new
classes emerge gradually and exhibit natural imbalance. Although some approaches ad-
dress the emergence of new classes, they often overlook class imbalance, which leads
to suboptimal performance, particularly on rare categories. To tackle this, we propose
CLIMB-3D, a unified framework forCLass-incrementalImbalance-aware3DIS. Build-
ing upon established exemplar replay (ER) strategies, we show that ER alone is insuf-
ficient to achieve robust performance under memory constraints. To mitigate this, we
introduce a novel pseudo-label generator (PLG) that extends supervision to previously
learned categories by leveraging predictions from a frozen model trained on prior tasks. Despite its promise, PLG tends to be biased towards frequent classes. Therefore, we
propose a class-balanced re-weighting (CBR) scheme that estimates object frequencies
from pseudo-labels and dynamically adjusts training bias, without requiring access to
past data. We design and evaluate three incremental scenarios for 3DIS on the challeng-
ing ScanNet200 dataset and additionally validate our method for semantic segmentation
on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by
up to 16.76% mAP for instance segmentation and approximately 30% mIoU for semantic
segmentation, demonstrating strong generalisation across both frequent and rare classes. Code is available at:https://github.com/vgthengane/CLIMB3D. Â© 2025.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020."
What is illustrated in Appendix A of the document chunk?,Appendix A Illustrating Incremental scenarios Frequent Scenario Semantic Scenario background frequent common rare semantic cluster 1 semantic cluster 2 semantic cluster 3 background random cluster 1 random cluster 2 random cluster 3 background Random Scenario,"Appendix A Illustrating Incremental scenarios Frequent Scenario Semantic Scenario background frequent common rare semantic cluster 1 semantic cluster 2 semantic cluster 3 background random cluster 1 random cluster 2 random cluster 3 background Random Scenario

1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

[78] Biao Zhang and Peter Wonka. Point cloud instance segmentation using probabilistic
embeddings. InCVPR, pages 8883â€“8892, 2021. [79] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsuper-
vised lifelong learning.Neural Networks, 149:95â€“106, 2022. [80] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang,
and Jiaya Jia. Understanding imbalanced semantic segmentation through neural col-
lapse. InCVPR, pages 19550â€“19560, 2023. [81] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch
network with cumulative learning for long-tailed visual recognition. InCVPR, pages
9719â€“9728, 2020. [82] Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang
Jiang. Balanced contrastive learning for long-tailed visual recognition. InCVPR, pages
6908â€“6917, 2022. THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced 3D Instance
Segmentation
Supplementary Material
Appendix A Illustrating Incremental scenarios
Frequent Scenario Semantic Scenario
background
frequent common
rare semantic 
cluster 1
semantic 
cluster 2
semantic 
cluster 3
background
random 
cluster 1
random 
cluster 2
random 
cluster 3
background
Random Scenario
Figure 3: Tasks are grouped into incremental scenarios based on object frequency, semantic
similarity, and random assignment. , , and denote different tasks; shapes indicate
object categories; marks the background.Left:Grouped by category frequency.Middle:
Grouped by semantic similarity (e.g., similar shapes).Right:Randomly grouped, mixing
semantic and frequency variations. Appendix B Per Phase Analysis
We extend the analysis from Tab."
How are tasks grouped in Figure 3 based on semantic similarity?,"Figure 3: Tasks are grouped into incremental scenarios based on object frequency, semantic similarity, and random assignment. , , and denote different tasks; shapes indicate object categories; marks the background. Left:Grouped by category frequency.Middle:Grouped by semantic similarity (e.g., similar shapes).Right:Randomly grouped, mixing semantic and frequency variations.","5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

[78] Biao Zhang and Peter Wonka. Point cloud instance segmentation using probabilistic
embeddings. InCVPR, pages 8883â€“8892, 2021. [79] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsuper-
vised lifelong learning.Neural Networks, 149:95â€“106, 2022. [80] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang,
and Jiaya Jia. Understanding imbalanced semantic segmentation through neural col-
lapse. InCVPR, pages 19550â€“19560, 2023. [81] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch
network with cumulative learning for long-tailed visual recognition. InCVPR, pages
9719â€“9728, 2020. [82] Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang
Jiang. Balanced contrastive learning for long-tailed visual recognition. InCVPR, pages
6908â€“6917, 2022. THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced 3D Instance
Segmentation
Supplementary Material
Appendix A Illustrating Incremental scenarios
Frequent Scenario Semantic Scenario
background
frequent common
rare semantic 
cluster 1
semantic 
cluster 2
semantic 
cluster 3
background
random 
cluster 1
random 
cluster 2
random 
cluster 3
background
Random Scenario
Figure 3: Tasks are grouped into incremental scenarios based on object frequency, semantic
similarity, and random assignment. , , and denote different tasks; shapes indicate
object categories; marks the background.Left:Grouped by category frequency.Middle:
Grouped by semantic similarity (e.g., similar shapes).Right:Randomly grouped, mixing
semantic and frequency variations. Appendix B Per Phase Analysis
We extend the analysis from Tab.

5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks.

Figure 3: Tasks are grouped into incremental scenarios based on object frequency, semantic similarity, and random assignment. , , and denote different tasks; shapes indicate object categories; marks the background. Left:Grouped by category frequency.Middle:Grouped by semantic similarity (e.g., similar shapes).Right:Randomly grouped, mixing semantic and frequency variations."
What does the term 'CLIMB-3D' stand for in the document chunk?,THENGANE ET AL.: CLIMB-3D1 CLIMB-3D: Class-Incremental Imbalanced 3D Instance Segmentation Supplementary Material,"PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020.

THENGANE ET AL.: CLIMB-3D1 CLIMB-3D: Class-Incremental Imbalanced 3D Instance Segmentation Supplementary Material

arXiv:2502.17429v3  [cs.CV]  21 Nov 2025
THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced
3D Instance Segmentation
Vishal Thengane1
v.thengane@surrey.ac.uk
Jean Lahoud2
jean.lahoud@mbzuai.ac.ae
Hisham Cholakkal2
hisham.cholakkal@mbzuai.ac.ae
Rao Muhammad Anwer2
rao.anwer@mbzuai.ac.ae
Lu Yin1
l.yin@surrey.ac.uk
Xiatian Zhu1
xiatian.zhu@surrey.ac.uk
Salman Khan2, 3
salman.khan@mbzuai.ac.ae
1 University of Surrey,
Guildford, UK
2 Mohamed bin Zayed University of
Artificial Intelligence,
Abu Dhabi, UAE
3 Australian National University,
Canberra, Australia
Abstract
While 3D instance segmentation (3DIS) has advanced significantly, most existing
methods assume that all object classes are known in advance and uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new
classes emerge gradually and exhibit natural imbalance. Although some approaches ad-
dress the emergence of new classes, they often overlook class imbalance, which leads
to suboptimal performance, particularly on rare categories. To tackle this, we propose
CLIMB-3D, a unified framework forCLass-incrementalImbalance-aware3DIS. Build-
ing upon established exemplar replay (ER) strategies, we show that ER alone is insuf-
ficient to achieve robust performance under memory constraints. To mitigate this, we
introduce a novel pseudo-label generator (PLG) that extends supervision to previously
learned categories by leveraging predictions from a frozen model trained on prior tasks. Despite its promise, PLG tends to be biased towards frequent classes. Therefore, we
propose a class-balanced re-weighting (CBR) scheme that estimates object frequencies
from pseudo-labels and dynamically adjusts training bias, without requiring access to
past data. We design and evaluate three incremental scenarios for 3DIS on the challeng-
ing ScanNet200 dataset and additionally validate our method for semantic segmentation
on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by
up to 16.76% mAP for instance segmentation and approximately 30% mIoU for semantic
segmentation, demonstrating strong generalisation across both frequent and rare classes. Code is available at:https://github.com/vgthengane/CLIMB3D. Â© 2025.

[78] Biao Zhang and Peter Wonka. Point cloud instance segmentation using probabilistic
embeddings. InCVPR, pages 8883â€“8892, 2021. [79] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsuper-
vised lifelong learning.Neural Networks, 149:95â€“106, 2022. [80] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang,
and Jiaya Jia. Understanding imbalanced semantic segmentation through neural col-
lapse. InCVPR, pages 19550â€“19560, 2023. [81] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch
network with cumulative learning for long-tailed visual recognition. InCVPR, pages
9719â€“9728, 2020. [82] Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang
Jiang. Balanced contrastive learning for long-tailed visual recognition. InCVPR, pages
6908â€“6917, 2022. THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced 3D Instance
Segmentation
Supplementary Material
Appendix A Illustrating Incremental scenarios
Frequent Scenario Semantic Scenario
background
frequent common
rare semantic 
cluster 1
semantic 
cluster 2
semantic 
cluster 3
background
random 
cluster 1
random 
cluster 2
random 
cluster 3
background
Random Scenario
Figure 3: Tasks are grouped into incremental scenarios based on object frequency, semantic
similarity, and random assignment. , , and denote different tasks; shapes indicate
object categories; marks the background.Left:Grouped by category frequency.Middle:
Grouped by semantic similarity (e.g., similar shapes).Right:Randomly grouped, mixing
semantic and frequency variations. Appendix B Per Phase Analysis
We extend the analysis from Tab."
What type of learning is discussed in the paper by Boyan Zhou et al.?,"Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition. InCVPR, pages 9719â€“9728, 2020.","[77] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature
transfer learning for face recognition with under-represented data. InCVPR, pages
5704â€“5713, 2019.

[62] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting.NeurIPS, 32,
2019. [63] Yuanzhi Su, Siyuan Chen, and Yuan-Gen Wang. Balanced residual distillation learn-
ing for 3d point cloud class-incremental semantic segmentation.arXiv preprint
arXiv:2408.01356, 2024. [64] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer
for 3d scene instance segmentation.arXiv preprint arXiv:2211.15766, 2022. [65] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an
efficient continual learner.arXiv preprint arXiv:2210.03114, 2022. [66] Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, and Yunpeng
Li. Foundational models for 3d point clouds: A survey and outlook.arXiv preprint
arXiv:2501.18594, 2025. [67] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classifica-
tion and detection dataset. InCVPR, pages 8769â€“8778, 2018. [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.NeurIPS, 30,
2017. [69] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, and Ming Tang. Adaptive class suppression loss for long-tail object detection. InCVPR, pages 3103â€“
3112, 2021.

Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition. InCVPR, pages 9719â€“9728, 2020.

[30] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for
continual learning.arXiv preprint arXiv:1710.10368, 2017. [31] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence
of similar and dissimilar tasks.NeurIPS, 33:18493â€“18504, 2020. [32] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. InCVPR, pages 103â€“112, 2019. [33] Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and
Roberto Togneri. Cost-sensitive learning of deep feature representations from im-
balanced data.IEEE transactions on neural networks and learning systems, 29(8):
3573â€“3587, 2017. [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 33:18661â€“18673, 2020. [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Des-
jardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks.Pro-
ceedings of the national academy of sciences, 114(13):3521â€“3526, 2017. [36] Theodora Kontogianni, Yuanwen Yue, Siyu Tang, and Konrad Schindler. Is continual
learning ready for real-world challenges?arXiv preprint arXiv:2402.10130, 2024. [37] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3d instance
segmentation via multi-task metric learning. InICCV, pages 9256â€“9266, 2019. [38] Jean Lahoud, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and
Salman Khan. Long-tailed 3d semantic segmentation with adaptive weight constraint
and sampling. InInternational Conference on Robotics and Automation (ICRA), pages
5037â€“5044, 2024. doi: 10.1109/ICRA57147.2024.10610029. [39] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi,
and Jiaya Jia. Stratified transformer for 3d point cloud segmentation, 2022. [40] Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao, and Xinjing
Cheng. Metasaug: Meta semantic augmentation for long-tailed visual recognition. In
CVPR, pages 5212â€“5221, 2021. [41] Tianhao Li, Limin Wang, and Gangshan Wu. Self supervision to distillation for long-
tailed visual recognition. InICCV, pages 630â€“639, 2021. [42] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In
ICML, pages 3925â€“3934."
What is the overall mAP50 performance of the proposed CLIMB-3D method in Split-A?,Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05,"5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks.

Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05

arXiv:2502.17429v3  [cs.CV]  21 Nov 2025
THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced
3D Instance Segmentation
Vishal Thengane1
v.thengane@surrey.ac.uk
Jean Lahoud2
jean.lahoud@mbzuai.ac.ae
Hisham Cholakkal2
hisham.cholakkal@mbzuai.ac.ae
Rao Muhammad Anwer2
rao.anwer@mbzuai.ac.ae
Lu Yin1
l.yin@surrey.ac.uk
Xiatian Zhu1
xiatian.zhu@surrey.ac.uk
Salman Khan2, 3
salman.khan@mbzuai.ac.ae
1 University of Surrey,
Guildford, UK
2 Mohamed bin Zayed University of
Artificial Intelligence,
Abu Dhabi, UAE
3 Australian National University,
Canberra, Australia
Abstract
While 3D instance segmentation (3DIS) has advanced significantly, most existing
methods assume that all object classes are known in advance and uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new
classes emerge gradually and exhibit natural imbalance. Although some approaches ad-
dress the emergence of new classes, they often overlook class imbalance, which leads
to suboptimal performance, particularly on rare categories. To tackle this, we propose
CLIMB-3D, a unified framework forCLass-incrementalImbalance-aware3DIS. Build-
ing upon established exemplar replay (ER) strategies, we show that ER alone is insuf-
ficient to achieve robust performance under memory constraints. To mitigate this, we
introduce a novel pseudo-label generator (PLG) that extends supervision to previously
learned categories by leveraging predictions from a frozen model trained on prior tasks. Despite its promise, PLG tends to be biased towards frequent classes. Therefore, we
propose a class-balanced re-weighting (CBR) scheme that estimates object frequencies
from pseudo-labels and dynamically adjusts training bias, without requiring access to
past data. We design and evaluate three incremental scenarios for 3DIS on the challeng-
ing ScanNet200 dataset and additionally validate our method for semantic segmentation
on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by
up to 16.76% mAP for instance segmentation and approximately 30% mIoU for semantic
segmentation, demonstrating strong generalisation across both frequent and rare classes. Code is available at:https://github.com/vgthengane/CLIMB3D. Â© 2025.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig."
How does the proposed method in Split-B compare to the baseline in terms of mAP50 performance in Phase 3?,Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56,"Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56

5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks.

arXiv:2502.17429v3  [cs.CV]  21 Nov 2025
THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced
3D Instance Segmentation
Vishal Thengane1
v.thengane@surrey.ac.uk
Jean Lahoud2
jean.lahoud@mbzuai.ac.ae
Hisham Cholakkal2
hisham.cholakkal@mbzuai.ac.ae
Rao Muhammad Anwer2
rao.anwer@mbzuai.ac.ae
Lu Yin1
l.yin@surrey.ac.uk
Xiatian Zhu1
xiatian.zhu@surrey.ac.uk
Salman Khan2, 3
salman.khan@mbzuai.ac.ae
1 University of Surrey,
Guildford, UK
2 Mohamed bin Zayed University of
Artificial Intelligence,
Abu Dhabi, UAE
3 Australian National University,
Canberra, Australia
Abstract
While 3D instance segmentation (3DIS) has advanced significantly, most existing
methods assume that all object classes are known in advance and uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new
classes emerge gradually and exhibit natural imbalance. Although some approaches ad-
dress the emergence of new classes, they often overlook class imbalance, which leads
to suboptimal performance, particularly on rare categories. To tackle this, we propose
CLIMB-3D, a unified framework forCLass-incrementalImbalance-aware3DIS. Build-
ing upon established exemplar replay (ER) strategies, we show that ER alone is insuf-
ficient to achieve robust performance under memory constraints. To mitigate this, we
introduce a novel pseudo-label generator (PLG) that extends supervision to previously
learned categories by leveraging predictions from a frozen model trained on prior tasks. Despite its promise, PLG tends to be biased towards frequent classes. Therefore, we
propose a class-balanced re-weighting (CBR) scheme that estimates object frequencies
from pseudo-labels and dynamically adjusts training bias, without requiring access to
past data. We design and evaluate three incremental scenarios for 3DIS on the challeng-
ing ScanNet200 dataset and additionally validate our method for semantic segmentation
on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by
up to 16.76% mAP for instance segmentation and approximately 30% mIoU for semantic
segmentation, demonstrating strong generalisation across both frequent and rare classes. Code is available at:https://github.com/vgthengane/CLIMB3D. Â© 2025.

1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab."
What is the mAP50 performance of the baseline method in Phase 2 of Split-C?,Baseline 36.40 7.74 37.62 22.32 7.55 15.96 40.41 21.08,"Baseline 36.40 7.74 37.62 22.32 7.55 15.96 40.41 21.08

1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks.

[78] Biao Zhang and Peter Wonka. Point cloud instance segmentation using probabilistic
embeddings. InCVPR, pages 8883â€“8892, 2021. [79] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsuper-
vised lifelong learning.Neural Networks, 149:95â€“106, 2022. [80] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang,
and Jiaya Jia. Understanding imbalanced semantic segmentation through neural col-
lapse. InCVPR, pages 19550â€“19560, 2023. [81] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch
network with cumulative learning for long-tailed visual recognition. InCVPR, pages
9719â€“9728, 2020. [82] Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang
Jiang. Balanced contrastive learning for long-tailed visual recognition. InCVPR, pages
6908â€“6917, 2022. THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced 3D Instance
Segmentation
Supplementary Material
Appendix A Illustrating Incremental scenarios
Frequent Scenario Semantic Scenario
background
frequent common
rare semantic 
cluster 1
semantic 
cluster 2
semantic 
cluster 3
background
random 
cluster 1
random 
cluster 2
random 
cluster 3
background
Random Scenario
Figure 3: Tasks are grouped into incremental scenarios based on object frequency, semantic
similarity, and random assignment. , , and denote different tasks; shapes indicate
object categories; marks the background.Left:Grouped by category frequency.Middle:
Grouped by semantic similarity (e.g., similar shapes).Right:Randomly grouped, mixing
semantic and frequency variations. Appendix B Per Phase Analysis
We extend the analysis from Tab."
How does the proposed method in Split-C address the increased complexity introduced by random grouping?,"In theSplit-Cscenario, the first-stage model struggles due to the increased complexity introduced by random grouping. In Phase 2, while the baseline focuses on learning the current task, it suffers from severe forgetting of prior knowledge. Conversely, our method balances new task learning with the retention of earlier information.","1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

In theSplit-Cscenario, the first-stage model struggles due to the increased complexity introduced by random grouping. In Phase 2, while the baseline focuses on learning the current task, it suffers from severe forgetting of prior knowledge. Conversely, our method balances new task learning with the retention of earlier information.

5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks.

[78] Biao Zhang and Peter Wonka. Point cloud instance segmentation using probabilistic
embeddings. InCVPR, pages 8883â€“8892, 2021. [79] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsuper-
vised lifelong learning.Neural Networks, 149:95â€“106, 2022. [80] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang,
and Jiaya Jia. Understanding imbalanced semantic segmentation through neural col-
lapse. InCVPR, pages 19550â€“19560, 2023. [81] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch
network with cumulative learning for long-tailed visual recognition. InCVPR, pages
9719â€“9728, 2020. [82] Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang
Jiang. Balanced contrastive learning for long-tailed visual recognition. InCVPR, pages
6908â€“6917, 2022. THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced 3D Instance
Segmentation
Supplementary Material
Appendix A Illustrating Incremental scenarios
Frequent Scenario Semantic Scenario
background
frequent common
rare semantic 
cluster 1
semantic 
cluster 2
semantic 
cluster 3
background
random 
cluster 1
random 
cluster 2
random 
cluster 3
background
Random Scenario
Figure 3: Tasks are grouped into incremental scenarios based on object frequency, semantic
similarity, and random assignment. , , and denote different tasks; shapes indicate
object categories; marks the background.Left:Grouped by category frequency.Middle:
Grouped by semantic similarity (e.g., similar shapes).Right:Randomly grouped, mixing
semantic and frequency variations. Appendix B Per Phase Analysis
We extend the analysis from Tab."
What is the improvement percentage in mAP achieved by the proposed method compared to the baseline across all task splits?,"solidatess1and maintains strong performance across all task splits. Overall, our proposed method improves mAP by 5.6%.","1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

solidatess1and maintains strong performance across all task splits. Overall, our proposed method improves mAP by 5.6%."
What is the purpose of the imbalance correction (IC) module in the proposed method?,"The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to address the performance gap for rare classes.","Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to address the performance gap for rare classes.

1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

It may be distributed unchanged freely in print or electronic forms. 2THENGANE ET AL.: CLIMB-3D
1 Introduction
Table Chair Couch Pillow Coffee Table Sofa Chair Recycling BinCabinet Unknown
Task-1 Task-2 Task-3
Figure 1:Overview of the CI-3DIS setting.New object categories are introduced incre-
mentally with each task. After every phase, the model must recognise both newly added
and previously learned classes. For instance, in Task 2, categories such asPillow,Coffee
Table, andSofa Chairare introduced, while the model is expected to retain recognition
of earlier classes likeTable,Chair, andCouch. 3D instance segmentation (3DIS) is a fundamental task in computer vision that involves
identifying precise object boundaries and class labels in 3D space, with broad applications in
graphics, robotics, and augmented reality [4, 39]. Traditional 3DIS methods, including top-
down [28, 70, 78], bottom-up [26, 73], and transformer-based approaches [59], perform well
under the assumption that all classes are available and balanced during training. However,
this assumption does not hold in real-world environments, where new object classes emerge
over time and exhibit natural imbalance. This gap motivates the need for Class-Incremental Learning (CIL) [17, 54], which sup-
ports learning new categories while retaining knowledge of previous ones [51]. While CIL
has seen success in 2D image tasks [1, 43, 54, 60], extensions to 3D point clouds remain
limited, often focusing on object-level classification [13, 18, 48]. Recent works on scene-
level class-incremental 3DIS (CI-3DIS) [4] and class-incremental semantic segmentation
(CI-3DSS) [74] show promise, but depend heavily on large exemplar [56] and overlook class
imbalance, limiting their practicality. To address this, we proposeCLIMB-3D, a unified framework forCLass-incremental
IMBalance-aware3DIS that jointly tackles catastrophic forgetting and class imbalance. The
framework begins with Exemplar Replay (ER), storing a small number of samples from
past classes for later replay. However, this alone does not yield promising results under the
strict memory constraints required in CI-3DIS. To address this, we introduce a Pseudo-Label
Generator (PLG), which uses a frozen model from previous task to generate supervision for
earlier classes. However, we observed that PLG tends to favour frequent classes while ig-
noring under-represented ones. To mitigate this, we propose a Class-Balanced Re-weighting
(CBR), which estimates class frequencies from the pseudo-labels to enhance the learning of
rare classes without accessing past data. Together, these components form a practical and
effective solution for real-world 3DIS. To evaluate CLIMB-3D, we designed three benchmark scenarios for CI-3DIS based on
the ScanNet200 dataset [57]. These scenarios simulate incremental learning setup under nat-
ural class imbalance, where new classes emerge based on (A) object frequency, (B) semantic
similarity, or (C) random grouping. Additionally, for comparison with existing methods,
we evaluate our approach on CI-3DSS using the ScanNetV2 dataset [16]. Experimental re-
sults show that our method significantly reduces forgetting and improves performance across
both frequent and rare categories, outperforming previous methods in both settings. Fig. 1
illustrates the CI-3DIS setup. THENGANE ET AL.: CLIMB-3D3
In summary, our contributions are: (i) a novel problem setting of imbalanced class-
incremental 3DIS with an effective method to balance learning and mitigate forgetting; (ii)
three benchmarks modelling continual object emergence with natural imbalance; and (iii)
strong experimental results, achieving up to 16.76% mAP improvement over baselines. 2 Related Work
3D Instance Segmentation.Various methods have been proposed for 3DIS. Grouping-
based approaches adopt a bottom-up pipeline that learns latent embeddings for point cluster-
ing [9, 21, 22, 29, 37, 44, 70, 78]. In contrast, proposal-based methods follow a top-down
strategy by detecting 3D boxes and segmenting objects within them [19, 26, 46, 73, 76]. Recently, transformer-based models [68] have also been applied to 3DIS and 3DSS [59, 64],
inspired by advances in 2D vision [10, 11]. However, these approaches require full anno-
tations for all classes and are not designed for progressive learning, where only new class
annotations are available and previous data is inaccessible. Another line of work aims to re-
duce annotation costs by proposing weakly supervised 3DIS methods based on sparse cues
[12, 27, 72]. While effective with limited annotations, these methods assume a fixed set of
classes and are susceptible to catastrophic forgetting in incremental settings. Incremental Learning.Continual learning involves training models sequentially to miti-
gate catastrophic forgetting. One common strategy is model regularisation, which constrains
parameter updates using techniques such as elastic weight consolidation or knowledge dis-
tillation [1, 24, 35, 43, 60]. Another approach is exemplar replay, where past data is stored
or generated to rehearse older tasks while learning new ones [5, 7, 30, 54]. A third direc-
tion involves dynamically expanding the model architecture or using modular sub-networks
to accommodate new tasks without interfering with old ones [31, 42, 53, 58, 71, 79]. Re-
cent efforts in class-incremental 3D segmentation [63, 74] have shown early promise but
often rely on basic architectures and focus primarily on semantic segmentation. Other works
[4, 36] explore continual learning in 3D settings, though they often depend on large memory
buffers. In contrast, our work introduces a tailored framework for 3D instance segmentation
that effectively transfers knowledge across tasks, even under class imbalance. Long-tailed Recognition.Imbalanced class distributions lead to poor recognition of rare
(long-tail) classes. To address this, re-sampling methods balance the data via input [8, 20, 50,
52, 61] or feature space [3, 14, 40], avoiding naÃ¯ve under-/over-sampling. Loss re-weighting
offers another direction, using class-based [6, 15, 23, 32, 33, 69] or per-example [45, 55, 62]
adjustments to ensure fair contribution. Parameter regularisation improves generalisation
through weight constraints [2], albeit requiring careful tuning. Other approaches leverage
transfer learning [77, 81], self-supervision [41, 75], or contrastive learning [34, 41, 82] to
improve rare-class representations. Long-tailed recognition is well-studied in 2D with large-
scale datasets [15, 49, 67]; in 3D, ScanNet200 [16, 57] enables similar efforts. Prior 3D
work focused on re-weighting, re-sampling, and transfer learning [57], while regularisation
remains underexplored. CeCo [80] balances class centres via auxiliary loss but overlooks
sampling and augmentation. Lahoud et al. [38] propose adaptive classifier regularisation
for 3D segmentation, outperforming prior approaches without threshold tuning. However,
neither method considers incremental learning. In contrast, we jointly address long-tail and
continual 3D semantic segmentation. 4THENGANE ET AL.: CLIMB-3D
Feature 
Backbone
Transformer 
Decoder
Feature 
Backbone
Transformer 
Decoder
(ð‘·,ð’€ð‘¡)
Current model (ð›·
ð‘¡
)
Pseudo Label Generator (PLG) 
Selection
à·¨ð‘Œð¾
ð‘¡
â„’CIâˆ’3DIS
à·¨ð‘Œð¾
ð‘¡,ð‘¤
à´¤ð‘Œð¾
ð‘¡
à· ð‘Œð‘¡
(ð·ð‘¡)
Current Dataset 
Exemplar 
Replay (ER)
+
~
Class-Balanced  
Re-weighting 
(CBR)
1
ð‘“(ð‘)
Figure 2: Overview ofCLIMB-3Dfor CI-3DIS. The model incrementally learns new classes
across sequential phases. During taskt, point cloudsPand their corresponding labelsY t are
sampled from a combination of the current training datasetD t and Exemplar Replay (ER),
which maintains a small memory of past examples. These are then passed to the current
modelÎ¦ t to produce predicted labelsY t . The Pseudo-Label Generator (PLG) selects the
top-Kpredictions from the previous modelÎ¦ tâˆ’1 . These pseudo-labels are then weighted
based on class frequencyf(c)using Class-Balanced Re-weighting (CBR), and the top-Kre-
weighted labels are selected to form the balanced pseudo-label set Â¯Yt . This pseudo-label set
is then concatenated with the ground-truth labels to form a final augmented supervision set
Yt for taskt, which is used to optimise the modelÎ¦ t using Eq. (3). 3 Methodology
3.1 Problem Formulation
3DIS.The objective of this task is to detect and segment each object instance within a point
cloud. Formally, the training dataset is denoted asD={(P i,Yi)}N
i=1, whereNis the number
of training samples. Each inputP i âˆˆR MÃ—6 is a coloured point cloud consisting ofMpoints,
where each point is represented by its 3D coordinates and RGB values. The corresponding
annotationY i ={(m i,j ,ci,j )}Ji
j=1 containsJ i object instances, wheremi,j âˆˆ {0,1}M is a binary
mask indicating which points belong to thej-th instance, andc i,j âˆˆ C={1,...,C}is the
semantic class label of that instance. GivenP i, the modelÎ¦predicts instance-level outputs
Ë†Yi ={(Ë†mi,j ,Ë†ci,j )}
Ë†Ji
j=1, where Ë†mi,j and Ë†ci,j denote the predicted mask and category label for
thej-th instance, respectively. The number of predicted instances Ë†Ji varies depending on the
modelâ€™s inference. The model is optimised using the following objective:
L3DIS(D;Î¦) = 1
|D| âˆ‘
(P,Y)âˆˆD
1
|Y| âˆ‘
(mj,cj)âˆˆY
(Lmask(mj,Ë†mj)+Î»Â·L cls(cj,Ë†cj)),(1)
whereL cls andL mask are the average classification and mask losses over all instances, andÎ»
controls their trade-off. CI-3DIS.Unlike conventional 3DIS, CI-3DIS involves sequential learning, where object
categories are introduced incrementally overTtraining tasks. Each tasktintroduces a dis-
joint set of classesC t , withC= ST
t=1 Ct andC t âˆ©C tâ€²
=/ 0 fortÌ¸=t â€². During each taskt, the THENGANE ET AL.: CLIMB-3D5
model receives a datasetD t ={(P i,Yt
i)}N
i=1, where each coloured point cloudP i âˆˆR MÃ—6
containsMpoints, and the corresponding annotationY t
i ={(m t
i,j ,ct
i,j )}
Jt
i
j=1 includes instance
masksm t
i,j âˆˆ {0,1}M and semantic class labelsc t
i,j âˆˆ Ct . The modelÎ¦ t , initialized from
Î¦tâˆ’1 , is trained onD t to predict instance-level outputs Ë†Yt
i ={(Ë†mt
i,j ,Ë†ct
i,j )}
Ë†Jt
i
j=1, where each
predicted semantic label Ë†ct
i,j belongs to the cumulative set of all classes observed so far:St
k=1 Ck. The key challenge in this setup is to learn new classes without forgetting those
encountered in previous tasks, despite limited supervision and the absence of past labels. 3.2 Method: CLIMB-3D
Overview.As shown in Fig. 2 and formulated in Sec. 3.1, the proposed framework for
CI-3DIS follows aphase-wisetraining strategy. In each phase, the model is exposed to a
carefully curated subset of the dataset, simulating real-world scenarios discussed in Sec. 3.3. Training naÃ¯vely on such phased data leads tocatastrophic forgetting[51], where the model
forgets prior knowledge when learning new tasks. To address this, we first incorporateEx-
emplar Replay(ER) [5], a strategy inspired by 2D incremental methods [1, 43, 54] and
recently extended to 3D settings [4], to mitigate forgetting by storing a small set of repre-
sentative samples from earlier phases. However, ER alone is insufficient to achieve robust
performance. To address this, two additional components are introduced: aPseudo-Label
Generator(PLG), which leverages a frozen model to generate supervision signals for previ-
ously seen classes, and aClass-Balanced Re-weighting(CBR) module that compensates for
class imbalance across phases. Each of these components is described in detail below 1. Exemplar Replay (ER).Inspired by Buzzega et al. [5], ER addresses the issue of catas-
trophic forgetting by allowing the model to retain a subset of data from earlier phases. During phaset, the model is trained not only on the current task dataD t , but also on a
set of exemplarsE 1:tâˆ’1, collected from previous phases. These exemplars are accumulated
incrementally:E 1:tâˆ’1 =E 1 âˆªÂ·Â·Â·âˆªE tâˆ’1 .Here,E t denotes the exemplar subset stored after
phaset, and|E t |denotes the exemplar replay size. The combined replay dataset is denoted
as:D t
ER =D t âˆªE 1:tâˆ’1.Epoch training consists of two stages: first, the model is trained on
the current data,D t ; then it is trained using exemplars fromE 1:tâˆ’1. While previous CI-3DIS approaches rely on large exemplar sets to mitigate forgetting [4],
such strategies become impractical under memory constraints. Consequently, although ER
aids knowledge retention, it is insufficient on its own for robust performance with a limited
exemplar budget. Therefore, PLG and CBR are introduced to better preserve past represen-
tations and enhance generalisation across phases. Pseudo-Label Generator (PLG).In the CI-3DIS setting, during phaset(wheret>1),
although ground-truth labels for previously seen classes are unavailable, the model from the
previous phaseÎ¦ tâˆ’1 , which preserves knowledge of past tasks, is retained. We use this
model to generate pseudo-labels for previously seen classes, thereby providing approximate
supervision during training. This allows the current modelÎ¦t , to retain prior knowledge and
reduce forgetting, even without access to ground-truth annotations. Given a point cloud and label pair(P,Y t ), fromD t , the previous modelÎ¦ tâˆ’1 , generates
pseudo-labels for the previously seen classes as Ë†Y1:tâˆ’1 =Î¦ tâˆ’1 (P), which we denote as ËœYt
1For clarity, modifications introduced by each component (ER, PLG, and CBR) are highlighted in green, blue,
and red, respectively. 6THENGANE ET AL.: CLIMB-3D
for brevity. However, we observe that utilising all predictions fromÎ¦tâˆ’1 introduces noisy or
incorrect labels, which degrade overall performance. To mitigate this, we select the top-K
most confident instance predictions, denoted as ËœYt
K, and concatenate them with the ground-
truth labels of the current task to obtain the supervision set: Â¯Yt =Y t âˆ¥ ËœYt
K. Following this,
the loss is computed over both the current taskâ€™s ground-truth labels and the top-Kconfi-
dent pseudo-labels from earlier phases, enabling the model to retain prior knowledge while
adapting to new classes. Class-Balanced Re-weighting (CBR).While ER and PLG contribute to preserving prior
knowledge, we observe that the modelâ€™s predictions are biased towards frequent classes,
resulting in the forgetting of rare categories. This issue is further amplified by the top-K
pseudo-labels selected by PLG, which predominantly represent dominant classes in the data. To address this, we propose aClass-Balanced Re-weighting(CBR) scheme that compensates
for class imbalance using object frequency statistics. At taskt, only the current datasetD t
and its label distributionp t (c)over classescâˆˆ Ct are available. Datasets and class distribu-
tions from earlier tasks{D i,p i(c)}tâˆ’1
i=1 are no longer accessible, which makes it challenging
to directly account for class imbalance across all previously seen tasks. Therefore, we pro-
pose to leverage the pseudo-label predictions of the frozen modelÎ¦ tâˆ’1 as a proxy for the
class distribution across previously learned categories. During each training iteration, the previous modelÎ¦ tâˆ’1 is applied to the current inputP
to generate pseudo-labels ËœYt , from which we accumulate class-wise frequency statistics for
previously learned classes. LetC1:t = St
i=1 Ci represent the union of all classes seen up to task
t. At the end of each epoch, we compute the overall class frequencyfâˆˆR |C1:t | by combining
the pseudo-label distribution Ëœpt (c)derived from ËœYt with the ground-truth label distribution
pt (c)from the current task. The resulting frequency vectorf= [f(c)] câˆˆC1:t is then used to
compute class-wise weightsw= [w(c)] câˆˆC1:t , which guide balanced pseudo-label selection
and help the model focus on rare classes. In the next epoch, the modelÎ¦ tâˆ’1 produces soft pseudo-label predictions ËœYt (as de-
scribed in PLG) , where each component ËœYt [c]denotes the confidence score for classcâˆˆ
C1:tâˆ’1. To promote balanced pseudo-label selection, class-wise re-weighting is applied using
w(c) = 1
f(c)+Îµ , whereÎµis a small constant for numerical stability. The adjusted predictions
are computed by applying the class-wise weights as: ËœYt,w [c] =w(c)Â· ËœYt [c],âˆ€câˆˆ C 1:tâˆ’1. Top-Kselection is then applied to both the original and the re-weighted scores, yielding
two pseudo-label sets: ËœYt
K and ËœYt,w
K , respectively. The final augmented target set, Yt , is
constructed by concatenating the ground-truth labelsY t with both sets of pseudo-labels:
Yt =Y t âˆ¥ ËœYt
K âˆ¥ ËœYt,w
K .(2)
Although the above re-weighting mitigates bias from prior tasks, it does not address class
imbalance within the current taskt. To resolve this, the class-balanced weightsw c are ex-
tended to incorporate statistics from both previously seen and current classes, i.e., overC 1:t
rather than onlyC 1:tâˆ’1. The updated weights, denoted asw â€²
c, are used to re-weights both
pseudo-label selection and classification loss, ensuring balanced supervision across all seen
categories, including rare ones. This unified strategy reduces bias and enhances performance
across phases. The pseudo-code for constructing the supervision set in Eq. (2), which incor-
porates both PLG and CBR components, is provided in Appendix A. Final Objective.The final training objective ofCLIMB-3D, incorporating ER, PLG, and THENGANE ET AL.: CLIMB-3D7
CBR, is formulated in Eq. (1) as:
L(Dt
ER;Î¦t ) = 1
|Dt
ER| âˆ‘
(P,Yt )âˆˆDt
ER
1
|Yt | âˆ‘
(mt
j,ct
j)âˆˆYt
 
Lmask(mt
j,Ë†mt
j)+w â€²â€²
c Â·Lcls(ct
j,Ë†ct
j)

,(3)
wherew â€²â€²
c =w â€²
c Â·Î» cls denotes the scaled weight vector, andÎ» cls is a hyperparameter for bal-
ancing the loss terms. 3.3 Benchmarking Incremental Scenarios
While CI-3DIS methods offer a wide range of practical applications, they frequently rely
on the assumption of uniform sample distribution, which rarely holds in real-world settings. In practice, the number of object categories, denoted byC, is often large and characterised
by substantial variability in category frequency, shape, structure, and size. To address these
challenges, we propose three incremental learning scenarios, each designed to capture a
different facet of real-world complexity. The design of these scenarios is detailed below; for
further information and illustrations, refer to Appendix E of the supplementary material. Split-A: Frequency Scenarios.This scenario acknowledges that datasets are often la-
belled based on the frequency of categories. To accommodate this, we propose a split where
the model learns from the most frequent categories and subsequently incorporates the less
frequent ones in later stages. By prioritising the training of frequently occurring categories,
the model can establish a strong foundation before expanding to handle rare categories. Split-B: Semantic Scenarios.Beyond frequency, semantic similarity is crucial in real-
world deployments. While objects often share visual or functional traits, models may en-
counter semantically different categories in new environments. To simulate this challenge,
we introduce theSplit-Bscenario. Here, categories are grouped based on their semantic
relationships, and the model is incrementally trained on one semantic group at a time. This
setup encourages the model to generalise across semantically similar categories and adapt
more effectively when exposed to new ones. Unlike theSplit-Ascenario, which organises
learning based on category frequency, theSplit-Bscenario may contain both frequent and
infrequent categories within a single task, focusing on semantic continuity and transfer. Split-C: Random Scenarios.In some cases, data labelling is driven by the availability of
objects rather than predefined criteria. To capture this, we introduce theSplit-Cscenario,
which represents a fully random setting where any class can appear in any task, resulting in
varying levels of class imbalance. By exposing the model to such diverse and unpredictable
distributions, we aim to improve its robustness in real-world situations where labelled data
availability is inconsistent. These incremental scenarios are designed to provide a more realistic representation of
object distributions, frequencies, and dynamics encountered in the real world. 4 Experiments
4.1 Setup
Datasets.We evaluateCLIMB-3Don ScanNet200 [57], which comprises 200 object cat-
egories and exhibits significant class imbalance, which makes it ideal for simulating and 8THENGANE ET AL.: CLIMB-3D
assessing real-world scenarios. In addition, we compare our approach against existing in-
cremental learning methods using ScanNetV2 [16] in the 3DSS setting. For this evaluation,
we follow the standard training and validation splits defined in prior works [74] to ensure
consistency with existing methods. Evaluation Metrics.We evaluate our method usingMean Average Precision(mAP), a
standard metric for 3DIS which provides a comprehensive measure of segmentation quality
by accounting for both precision and recall. For comparison with existing 3DSS methods,
we report themean Intersection over Union(mIoU), which quantifies the overlap between
predicted and ground truth segments. To assess the modelâ€™s ability to mitigate catastrophic
forgetting in incremental settings, we use theForgetting Percentage Points(FPP) metric, as
defined in [47]. FPP captures performance degradation by measuring the accuracy drop on
the initially seen categories between the first and final training phases. Detailed descriptions of the incremental scenarios (Split-A,Split-B,Split-C) and im-
plementation are provided in Appendix B. 4.2 Results and Discussion
Table 1: Comparison between the proposed method and the baseline in the 3DIS setting,
evaluated using mAP25, mAP50, mAP, and FPP after training across all phase. Scenarios Methods Average Precisionâ†‘FPPâ†“
mAP25 mAP50 mAP mAP 25 mAP50
Baseline 16.46 14.29 10.44 51.30 46.82Split-A CLIMB-3D(Ours) 35.69 31.05 22.72 3.44 2.63
Baseline 17.22 15.07 10.93 46.27 42.1Split-B CLIMB-3D(Ours) 35.48 31.56 23.69 8.00 5.51
Baseline 25.65 21.08 14.85 31.68 28.84Split-C CLIMB-3D(Ours) 31.59 26.78 18.93 9.10 7.89
To evaluate our proposed approach, we construct a baseline using ER [5] for the 3DIS
setting, as no existing incremental baselines are available for this task. In contrast, for the
3DSS setting, where incremental baselines do exist, we compare our method against three
prior approaches [43, 60, 74]. Results on CI-3DIS.Tab. 1 compares CLIMB-3D against the ER baseline across the three
CI-3DIS scenarios, evaluated after all phases using mAP 25, mAP50, overall mAP, and FPP. In theSplit-Ascenario, characterised by significant distribution shifts, our method achieves
gains of +19.23%, +16.76%, and +12.28% in mAP 25, mAP 50, and overall mAP, respec-
tively, while drastically reducing forgetting with FPP scores of 3.44% (mAP 25) and 2.63%
(mAP50), both over 45 points lower than the baseline. Under theSplit-Bscenario, with
semantically related new categories, improvements of +18.26%, +16.49%, and +12.76%
are observed alongside a reduction in forgetting to 8.00% and 5.51% from baseline levels
above 42%. For theSplit-Cscenario, featuring increasing geometric complexity, CLIMB-
3D maintains solid gains of +5.94%, +5.70%, and +4.08% across mAP metrics, with forget-
ting reduced by over 22 points. Overall, these results demonstrate robust forward transfer,
minimal forgetting, and stable learning, highlighting the methodâ€™s effectiveness for scalable
continual 3D semantic understanding. THENGANE ET AL.: CLIMB-3D9
Table 2: Comparison between the proposed method and existing baselines in the 3DSS set-
ting on ScanNetV2 [16], evaluated using mIoU. Methods Phase=1 Phase=2 All
EWC [60] 17.75 13.22 16.62
LwF [43] 30.38 13.37 26.13
Yanget al. [74] 34.16 13.43 28.98
CLIMB-3D(Ours) 69.39 32.56 59.38
Results on CI-3DSS.Although our primary focus is on CI-3DIS, we also evaluate our
method under the CI-3DSS setting to enable comparisons with existing approaches. To do
so, we adapt our predictions to assign each point the label corresponding to the highest-
confidence mask and exclude background classes (e.g., floor and wall), as these are not part
of the semantic segmentation targets. Tab. 2 presents a comparison between our method and
existing baselines on the ScanNet V2 dataset [16], evaluated using mIoU across two training
phases and overall. Although originally designed for instance segmentation, our method
generalises effectively to semantic segmentation, achieving substantial gains of +35.23%
mIoU in Phase 1 and +19.1% in Phase 2. Overall, it achieves 59.38% mIoU, significantly
higher than theâˆ¼30% mIoU of prior methods, highlighting its robustness and transferability. Refer to Appendix C, D, and F of the supplementary material for detailed analyses of per-
phase performance, rare-class evaluation, and qualitative result comparisons, respectively. 4.3 Ablation
Table 3: Ablation study illustrating the impact of each component in a three-phase setup. Each split (s) and corresponding number indicates data introduced at that phase. Best results
are highlighted inbold. Row Modules p=1â†‘p=2â†‘p=3â†‘ FPPâ†“
s1 s1 s2 All s1 s2 s3 All
1. Oracle - - - - 55.14 30.77 25.30 37.68 -
2. NaÃ¯ve 56.82 0.00 28.09 14.15 0.00 0.00 19.67 5.80 56.82
3. + ER 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28 46.44
4. + PLG 56.82 50.0034.3942.13 49.78 11.41 26.47 29.28 7.04
5. + CBR 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05 2.63
We conduct an ablation study to evaluate the contribution of each component in our
framework. As an upper bound, we report the performance of anOraclemodel, which is
trained jointly on the full dataset. For the incremental setting, we follow the previously
defined splits and first train the model naively across phases. We then incrementally add
each module to isolate its impact. Tab. 3 presents results for theSplit-Ascenario using both
mAP50 and FPP metrics. NaÃ¯ve Training.When trained naÃ¯vely without any dedicated modules, the model suffers
from severe catastrophic forgetting, as evident in row 2. The model entirely forgets pre-
viously learned classes upon entering new phases, with performance dropping to zero for
earlier splits and FPP rising to 56.82. 10THENGANE ET AL.: CLIMB-3D
Effect of ER.Adding exemplar replay (row 3) partially alleviates forgetting by maintaining
a buffer of past examples. This yields notable gains fors1in Phase 2 (+18.51%) and Phase 3
(+10.38%), and also improvess2in Phase 3 (+9.43%). However, the overall forgetting
remains substantial (FPP: 46.44), showing ER alone is insufficient. Effect of PLG.The addition of the pseudo-label generator (PLG), which generates labels for
previous classes by retaining a copy of the model from an earlier phase, facilitates knowledge
retention and forward transfer. As shown in row 4, PLG significantly reduces forgetting and
enhances performance on current tasks. Fors1, it improves mAP 50 by 31.49% in Phase 2
and 39.40% in Phase 3 over exemplar replay. Overall, PLG yields a 15.00% increase in
performance and reduces forgetting by 39.40%. Effect of CBR.Finally, class-balanced re-weighting (CBR) mitigates class imbalance dur-
ing both pseudo-labelling and current task learning by adjusting each classâ€™s contribution
based on its frequency. As shown in row 5, CBR enhancess1retention over PLG in Phases 2
and 3 (+4.67% and +4.41%), and further improves performance ons2ands3. It also achieves
the lowest FPP of 2.63, reflecting strong forgetting mitigation. Overall, CBR offers the best
trade-off between retaining past knowledge and acquiring new tasks, outperforming PLG
and ER by 4.41% and 43.81%, respectively. 4.4 Limitations & Future Work
While CLIMB-3D achieves strong performance across CI-3DIS and CI-3DSS settings, sev-
eral limitations remain and warrant discussion. The experiments are currently limited to
indoor datasets (ScanNet200 and ScanNetV2), therefore evaluating performance on outdoor
scenes would help assess generalisability across diverse environments. The current setup
also considers only three incremental phases, whereas real-world applications often involve
longer sequences; incorporating such extended setups would better reflect practical chal-
lenges. Moreover, the model operates in a uni-modal setting. Integrating multi-modal cues,
such as vision-language models, could further enhance performance, as suggested by recent
2D and 3D studies [25, 65, 66]. 5 Conclusion
This work addresses the challenge of catastrophic forgetting in class-incremental 3D in-
stance segmentation by introducing a modular framework that integrates exemplar replay
(ER), a pseudo-label generator (PLG), and class-balanced re-weighting (CBR). The pro-
posed method incrementally adapts to new classes while preserving prior knowledge, all
without requiring access to the full dataset. Extensive experiments on a three-phase bench-
mark validate the individual and combined effectiveness of each component: ER enables
efficient memory-based retention; PLG facilitates knowledge preservation through pseudo-
supervision; and CBR mitigates class imbalance to enhance learning stability. Together,
these components significantly reduce forgetting and improve segmentation performance
across all phases. By achieving a strong balance between stability and plasticity, our ap-
proach advances continual 3D scene understanding and sets a new baseline for future re-
search in this area. THENGANE ET AL.: CLIMB-3D11
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne
Tuytelaars. Memory aware synapses: Learning what (not) to forget. InECCV, pages
139â€“154, 2018. [2] Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long-tailed
recognition via weight balancing. InCVPR, pages 6897â€“6907, 2022. [3] Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying
imbalanced data. InJoint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 770â€“785."
What are the methods compared to the imbalance correction (IC) module for performance evaluation?,"To assess its impact, we compare its performance with the framework which has exemplar replay (ER) and knowledge distillation (KD).","To assess its impact, we compare its performance with the framework which has exemplar replay (ER) and knowledge distillation (KD).

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks."
What type of classes does the imbalance correction (IC) module aim to improve performance for?,"Specifically, we focus on its ability to improve performance for rare classes, which the model encounters infrequently compared to more common classes.","Specifically, we focus on its ability to improve performance for rare classes, which the model encounters infrequently compared to more common classes.

1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

It may be distributed unchanged freely in print or electronic forms. 2THENGANE ET AL.: CLIMB-3D
1 Introduction
Table Chair Couch Pillow Coffee Table Sofa Chair Recycling BinCabinet Unknown
Task-1 Task-2 Task-3
Figure 1:Overview of the CI-3DIS setting.New object categories are introduced incre-
mentally with each task. After every phase, the model must recognise both newly added
and previously learned classes. For instance, in Task 2, categories such asPillow,Coffee
Table, andSofa Chairare introduced, while the model is expected to retain recognition
of earlier classes likeTable,Chair, andCouch. 3D instance segmentation (3DIS) is a fundamental task in computer vision that involves
identifying precise object boundaries and class labels in 3D space, with broad applications in
graphics, robotics, and augmented reality [4, 39]. Traditional 3DIS methods, including top-
down [28, 70, 78], bottom-up [26, 73], and transformer-based approaches [59], perform well
under the assumption that all classes are available and balanced during training. However,
this assumption does not hold in real-world environments, where new object classes emerge
over time and exhibit natural imbalance. This gap motivates the need for Class-Incremental Learning (CIL) [17, 54], which sup-
ports learning new categories while retaining knowledge of previous ones [51]. While CIL
has seen success in 2D image tasks [1, 43, 54, 60], extensions to 3D point clouds remain
limited, often focusing on object-level classification [13, 18, 48]. Recent works on scene-
level class-incremental 3DIS (CI-3DIS) [4] and class-incremental semantic segmentation
(CI-3DSS) [74] show promise, but depend heavily on large exemplar [56] and overlook class
imbalance, limiting their practicality. To address this, we proposeCLIMB-3D, a unified framework forCLass-incremental
IMBalance-aware3DIS that jointly tackles catastrophic forgetting and class imbalance. The
framework begins with Exemplar Replay (ER), storing a small number of samples from
past classes for later replay. However, this alone does not yield promising results under the
strict memory constraints required in CI-3DIS. To address this, we introduce a Pseudo-Label
Generator (PLG), which uses a frozen model from previous task to generate supervision for
earlier classes. However, we observed that PLG tends to favour frequent classes while ig-
noring under-represented ones. To mitigate this, we propose a Class-Balanced Re-weighting
(CBR), which estimates class frequencies from the pseudo-labels to enhance the learning of
rare classes without accessing past data. Together, these components form a practical and
effective solution for real-world 3DIS. To evaluate CLIMB-3D, we designed three benchmark scenarios for CI-3DIS based on
the ScanNet200 dataset [57]. These scenarios simulate incremental learning setup under nat-
ural class imbalance, where new classes emerge based on (A) object frequency, (B) semantic
similarity, or (C) random grouping. Additionally, for comparison with existing methods,
we evaluate our approach on CI-3DSS using the ScanNetV2 dataset [16]. Experimental re-
sults show that our method significantly reduces forgetting and improves performance across
both frequent and rare categories, outperforming previous methods in both settings. Fig. 1
illustrates the CI-3DIS setup. THENGANE ET AL.: CLIMB-3D3
In summary, our contributions are: (i) a novel problem setting of imbalanced class-
incremental 3DIS with an effective method to balance learning and mitigate forgetting; (ii)
three benchmarks modelling continual object emergence with natural imbalance; and (iii)
strong experimental results, achieving up to 16.76% mAP improvement over baselines. 2 Related Work
3D Instance Segmentation.Various methods have been proposed for 3DIS. Grouping-
based approaches adopt a bottom-up pipeline that learns latent embeddings for point cluster-
ing [9, 21, 22, 29, 37, 44, 70, 78]. In contrast, proposal-based methods follow a top-down
strategy by detecting 3D boxes and segmenting objects within them [19, 26, 46, 73, 76]. Recently, transformer-based models [68] have also been applied to 3DIS and 3DSS [59, 64],
inspired by advances in 2D vision [10, 11]. However, these approaches require full anno-
tations for all classes and are not designed for progressive learning, where only new class
annotations are available and previous data is inaccessible. Another line of work aims to re-
duce annotation costs by proposing weakly supervised 3DIS methods based on sparse cues
[12, 27, 72]. While effective with limited annotations, these methods assume a fixed set of
classes and are susceptible to catastrophic forgetting in incremental settings. Incremental Learning.Continual learning involves training models sequentially to miti-
gate catastrophic forgetting. One common strategy is model regularisation, which constrains
parameter updates using techniques such as elastic weight consolidation or knowledge dis-
tillation [1, 24, 35, 43, 60]. Another approach is exemplar replay, where past data is stored
or generated to rehearse older tasks while learning new ones [5, 7, 30, 54]. A third direc-
tion involves dynamically expanding the model architecture or using modular sub-networks
to accommodate new tasks without interfering with old ones [31, 42, 53, 58, 71, 79]. Re-
cent efforts in class-incremental 3D segmentation [63, 74] have shown early promise but
often rely on basic architectures and focus primarily on semantic segmentation. Other works
[4, 36] explore continual learning in 3D settings, though they often depend on large memory
buffers. In contrast, our work introduces a tailored framework for 3D instance segmentation
that effectively transfers knowledge across tasks, even under class imbalance. Long-tailed Recognition.Imbalanced class distributions lead to poor recognition of rare
(long-tail) classes. To address this, re-sampling methods balance the data via input [8, 20, 50,
52, 61] or feature space [3, 14, 40], avoiding naÃ¯ve under-/over-sampling. Loss re-weighting
offers another direction, using class-based [6, 15, 23, 32, 33, 69] or per-example [45, 55, 62]
adjustments to ensure fair contribution. Parameter regularisation improves generalisation
through weight constraints [2], albeit requiring careful tuning. Other approaches leverage
transfer learning [77, 81], self-supervision [41, 75], or contrastive learning [34, 41, 82] to
improve rare-class representations. Long-tailed recognition is well-studied in 2D with large-
scale datasets [15, 49, 67]; in 3D, ScanNet200 [16, 57] enables similar efforts. Prior 3D
work focused on re-weighting, re-sampling, and transfer learning [57], while regularisation
remains underexplored. CeCo [80] balances class centres via auxiliary loss but overlooks
sampling and augmentation. Lahoud et al. [38] propose adaptive classifier regularisation
for 3D segmentation, outperforming prior approaches without threshold tuning. However,
neither method considers incremental learning. In contrast, we jointly address long-tail and
continual 3D semantic segmentation. 4THENGANE ET AL.: CLIMB-3D
Feature 
Backbone
Transformer 
Decoder
Feature 
Backbone
Transformer 
Decoder
(ð‘·,ð’€ð‘¡)
Current model (ð›·
ð‘¡
)
Pseudo Label Generator (PLG) 
Selection
à·¨ð‘Œð¾
ð‘¡
â„’CIâˆ’3DIS
à·¨ð‘Œð¾
ð‘¡,ð‘¤
à´¤ð‘Œð¾
ð‘¡
à· ð‘Œð‘¡
(ð·ð‘¡)
Current Dataset 
Exemplar 
Replay (ER)
+
~
Class-Balanced  
Re-weighting 
(CBR)
1
ð‘“(ð‘)
Figure 2: Overview ofCLIMB-3Dfor CI-3DIS. The model incrementally learns new classes
across sequential phases. During taskt, point cloudsPand their corresponding labelsY t are
sampled from a combination of the current training datasetD t and Exemplar Replay (ER),
which maintains a small memory of past examples. These are then passed to the current
modelÎ¦ t to produce predicted labelsY t . The Pseudo-Label Generator (PLG) selects the
top-Kpredictions from the previous modelÎ¦ tâˆ’1 . These pseudo-labels are then weighted
based on class frequencyf(c)using Class-Balanced Re-weighting (CBR), and the top-Kre-
weighted labels are selected to form the balanced pseudo-label set Â¯Yt . This pseudo-label set
is then concatenated with the ground-truth labels to form a final augmented supervision set
Yt for taskt, which is used to optimise the modelÎ¦ t using Eq. (3). 3 Methodology
3.1 Problem Formulation
3DIS.The objective of this task is to detect and segment each object instance within a point
cloud. Formally, the training dataset is denoted asD={(P i,Yi)}N
i=1, whereNis the number
of training samples. Each inputP i âˆˆR MÃ—6 is a coloured point cloud consisting ofMpoints,
where each point is represented by its 3D coordinates and RGB values. The corresponding
annotationY i ={(m i,j ,ci,j )}Ji
j=1 containsJ i object instances, wheremi,j âˆˆ {0,1}M is a binary
mask indicating which points belong to thej-th instance, andc i,j âˆˆ C={1,...,C}is the
semantic class label of that instance. GivenP i, the modelÎ¦predicts instance-level outputs
Ë†Yi ={(Ë†mi,j ,Ë†ci,j )}
Ë†Ji
j=1, where Ë†mi,j and Ë†ci,j denote the predicted mask and category label for
thej-th instance, respectively. The number of predicted instances Ë†Ji varies depending on the
modelâ€™s inference. The model is optimised using the following objective:
L3DIS(D;Î¦) = 1
|D| âˆ‘
(P,Y)âˆˆD
1
|Y| âˆ‘
(mj,cj)âˆˆY
(Lmask(mj,Ë†mj)+Î»Â·L cls(cj,Ë†cj)),(1)
whereL cls andL mask are the average classification and mask losses over all instances, andÎ»
controls their trade-off. CI-3DIS.Unlike conventional 3DIS, CI-3DIS involves sequential learning, where object
categories are introduced incrementally overTtraining tasks. Each tasktintroduces a dis-
joint set of classesC t , withC= ST
t=1 Ct andC t âˆ©C tâ€²
=/ 0 fortÌ¸=t â€². During each taskt, the THENGANE ET AL.: CLIMB-3D5
model receives a datasetD t ={(P i,Yt
i)}N
i=1, where each coloured point cloudP i âˆˆR MÃ—6
containsMpoints, and the corresponding annotationY t
i ={(m t
i,j ,ct
i,j )}
Jt
i
j=1 includes instance
masksm t
i,j âˆˆ {0,1}M and semantic class labelsc t
i,j âˆˆ Ct . The modelÎ¦ t , initialized from
Î¦tâˆ’1 , is trained onD t to predict instance-level outputs Ë†Yt
i ={(Ë†mt
i,j ,Ë†ct
i,j )}
Ë†Jt
i
j=1, where each
predicted semantic label Ë†ct
i,j belongs to the cumulative set of all classes observed so far:St
k=1 Ck. The key challenge in this setup is to learn new classes without forgetting those
encountered in previous tasks, despite limited supervision and the absence of past labels. 3.2 Method: CLIMB-3D
Overview.As shown in Fig. 2 and formulated in Sec. 3.1, the proposed framework for
CI-3DIS follows aphase-wisetraining strategy. In each phase, the model is exposed to a
carefully curated subset of the dataset, simulating real-world scenarios discussed in Sec. 3.3. Training naÃ¯vely on such phased data leads tocatastrophic forgetting[51], where the model
forgets prior knowledge when learning new tasks. To address this, we first incorporateEx-
emplar Replay(ER) [5], a strategy inspired by 2D incremental methods [1, 43, 54] and
recently extended to 3D settings [4], to mitigate forgetting by storing a small set of repre-
sentative samples from earlier phases. However, ER alone is insufficient to achieve robust
performance. To address this, two additional components are introduced: aPseudo-Label
Generator(PLG), which leverages a frozen model to generate supervision signals for previ-
ously seen classes, and aClass-Balanced Re-weighting(CBR) module that compensates for
class imbalance across phases. Each of these components is described in detail below 1. Exemplar Replay (ER).Inspired by Buzzega et al. [5], ER addresses the issue of catas-
trophic forgetting by allowing the model to retain a subset of data from earlier phases. During phaset, the model is trained not only on the current task dataD t , but also on a
set of exemplarsE 1:tâˆ’1, collected from previous phases. These exemplars are accumulated
incrementally:E 1:tâˆ’1 =E 1 âˆªÂ·Â·Â·âˆªE tâˆ’1 .Here,E t denotes the exemplar subset stored after
phaset, and|E t |denotes the exemplar replay size. The combined replay dataset is denoted
as:D t
ER =D t âˆªE 1:tâˆ’1.Epoch training consists of two stages: first, the model is trained on
the current data,D t ; then it is trained using exemplars fromE 1:tâˆ’1. While previous CI-3DIS approaches rely on large exemplar sets to mitigate forgetting [4],
such strategies become impractical under memory constraints. Consequently, although ER
aids knowledge retention, it is insufficient on its own for robust performance with a limited
exemplar budget. Therefore, PLG and CBR are introduced to better preserve past represen-
tations and enhance generalisation across phases. Pseudo-Label Generator (PLG).In the CI-3DIS setting, during phaset(wheret>1),
although ground-truth labels for previously seen classes are unavailable, the model from the
previous phaseÎ¦ tâˆ’1 , which preserves knowledge of past tasks, is retained. We use this
model to generate pseudo-labels for previously seen classes, thereby providing approximate
supervision during training. This allows the current modelÎ¦t , to retain prior knowledge and
reduce forgetting, even without access to ground-truth annotations. Given a point cloud and label pair(P,Y t ), fromD t , the previous modelÎ¦ tâˆ’1 , generates
pseudo-labels for the previously seen classes as Ë†Y1:tâˆ’1 =Î¦ tâˆ’1 (P), which we denote as ËœYt
1For clarity, modifications introduced by each component (ER, PLG, and CBR) are highlighted in green, blue,
and red, respectively. 6THENGANE ET AL.: CLIMB-3D
for brevity. However, we observe that utilising all predictions fromÎ¦tâˆ’1 introduces noisy or
incorrect labels, which degrade overall performance. To mitigate this, we select the top-K
most confident instance predictions, denoted as ËœYt
K, and concatenate them with the ground-
truth labels of the current task to obtain the supervision set: Â¯Yt =Y t âˆ¥ ËœYt
K. Following this,
the loss is computed over both the current taskâ€™s ground-truth labels and the top-Kconfi-
dent pseudo-labels from earlier phases, enabling the model to retain prior knowledge while
adapting to new classes. Class-Balanced Re-weighting (CBR).While ER and PLG contribute to preserving prior
knowledge, we observe that the modelâ€™s predictions are biased towards frequent classes,
resulting in the forgetting of rare categories. This issue is further amplified by the top-K
pseudo-labels selected by PLG, which predominantly represent dominant classes in the data. To address this, we propose aClass-Balanced Re-weighting(CBR) scheme that compensates
for class imbalance using object frequency statistics. At taskt, only the current datasetD t
and its label distributionp t (c)over classescâˆˆ Ct are available. Datasets and class distribu-
tions from earlier tasks{D i,p i(c)}tâˆ’1
i=1 are no longer accessible, which makes it challenging
to directly account for class imbalance across all previously seen tasks. Therefore, we pro-
pose to leverage the pseudo-label predictions of the frozen modelÎ¦ tâˆ’1 as a proxy for the
class distribution across previously learned categories. During each training iteration, the previous modelÎ¦ tâˆ’1 is applied to the current inputP
to generate pseudo-labels ËœYt , from which we accumulate class-wise frequency statistics for
previously learned classes. LetC1:t = St
i=1 Ci represent the union of all classes seen up to task
t. At the end of each epoch, we compute the overall class frequencyfâˆˆR |C1:t | by combining
the pseudo-label distribution Ëœpt (c)derived from ËœYt with the ground-truth label distribution
pt (c)from the current task. The resulting frequency vectorf= [f(c)] câˆˆC1:t is then used to
compute class-wise weightsw= [w(c)] câˆˆC1:t , which guide balanced pseudo-label selection
and help the model focus on rare classes. In the next epoch, the modelÎ¦ tâˆ’1 produces soft pseudo-label predictions ËœYt (as de-
scribed in PLG) , where each component ËœYt [c]denotes the confidence score for classcâˆˆ
C1:tâˆ’1. To promote balanced pseudo-label selection, class-wise re-weighting is applied using
w(c) = 1
f(c)+Îµ , whereÎµis a small constant for numerical stability. The adjusted predictions
are computed by applying the class-wise weights as: ËœYt,w [c] =w(c)Â· ËœYt [c],âˆ€câˆˆ C 1:tâˆ’1. Top-Kselection is then applied to both the original and the re-weighted scores, yielding
two pseudo-label sets: ËœYt
K and ËœYt,w
K , respectively. The final augmented target set, Yt , is
constructed by concatenating the ground-truth labelsY t with both sets of pseudo-labels:
Yt =Y t âˆ¥ ËœYt
K âˆ¥ ËœYt,w
K .(2)
Although the above re-weighting mitigates bias from prior tasks, it does not address class
imbalance within the current taskt. To resolve this, the class-balanced weightsw c are ex-
tended to incorporate statistics from both previously seen and current classes, i.e., overC 1:t
rather than onlyC 1:tâˆ’1. The updated weights, denoted asw â€²
c, are used to re-weights both
pseudo-label selection and classification loss, ensuring balanced supervision across all seen
categories, including rare ones. This unified strategy reduces bias and enhances performance
across phases. The pseudo-code for constructing the supervision set in Eq. (2), which incor-
porates both PLG and CBR components, is provided in Appendix A. Final Objective.The final training objective ofCLIMB-3D, incorporating ER, PLG, and THENGANE ET AL.: CLIMB-3D7
CBR, is formulated in Eq. (1) as:
L(Dt
ER;Î¦t ) = 1
|Dt
ER| âˆ‘
(P,Yt )âˆˆDt
ER
1
|Yt | âˆ‘
(mt
j,ct
j)âˆˆYt
 
Lmask(mt
j,Ë†mt
j)+w â€²â€²
c Â·Lcls(ct
j,Ë†ct
j)

,(3)
wherew â€²â€²
c =w â€²
c Â·Î» cls denotes the scaled weight vector, andÎ» cls is a hyperparameter for bal-
ancing the loss terms. 3.3 Benchmarking Incremental Scenarios
While CI-3DIS methods offer a wide range of practical applications, they frequently rely
on the assumption of uniform sample distribution, which rarely holds in real-world settings. In practice, the number of object categories, denoted byC, is often large and characterised
by substantial variability in category frequency, shape, structure, and size. To address these
challenges, we propose three incremental learning scenarios, each designed to capture a
different facet of real-world complexity. The design of these scenarios is detailed below; for
further information and illustrations, refer to Appendix E of the supplementary material. Split-A: Frequency Scenarios.This scenario acknowledges that datasets are often la-
belled based on the frequency of categories. To accommodate this, we propose a split where
the model learns from the most frequent categories and subsequently incorporates the less
frequent ones in later stages. By prioritising the training of frequently occurring categories,
the model can establish a strong foundation before expanding to handle rare categories. Split-B: Semantic Scenarios.Beyond frequency, semantic similarity is crucial in real-
world deployments. While objects often share visual or functional traits, models may en-
counter semantically different categories in new environments. To simulate this challenge,
we introduce theSplit-Bscenario. Here, categories are grouped based on their semantic
relationships, and the model is incrementally trained on one semantic group at a time. This
setup encourages the model to generalise across semantically similar categories and adapt
more effectively when exposed to new ones. Unlike theSplit-Ascenario, which organises
learning based on category frequency, theSplit-Bscenario may contain both frequent and
infrequent categories within a single task, focusing on semantic continuity and transfer. Split-C: Random Scenarios.In some cases, data labelling is driven by the availability of
objects rather than predefined criteria. To capture this, we introduce theSplit-Cscenario,
which represents a fully random setting where any class can appear in any task, resulting in
varying levels of class imbalance. By exposing the model to such diverse and unpredictable
distributions, we aim to improve its robustness in real-world situations where labelled data
availability is inconsistent. These incremental scenarios are designed to provide a more realistic representation of
object distributions, frequencies, and dynamics encountered in the real world. 4 Experiments
4.1 Setup
Datasets.We evaluateCLIMB-3Don ScanNet200 [57], which comprises 200 object cat-
egories and exhibits significant class imbalance, which makes it ideal for simulating and 8THENGANE ET AL.: CLIMB-3D
assessing real-world scenarios. In addition, we compare our approach against existing in-
cremental learning methods using ScanNetV2 [16] in the 3DSS setting. For this evaluation,
we follow the standard training and validation splits defined in prior works [74] to ensure
consistency with existing methods. Evaluation Metrics.We evaluate our method usingMean Average Precision(mAP), a
standard metric for 3DIS which provides a comprehensive measure of segmentation quality
by accounting for both precision and recall. For comparison with existing 3DSS methods,
we report themean Intersection over Union(mIoU), which quantifies the overlap between
predicted and ground truth segments. To assess the modelâ€™s ability to mitigate catastrophic
forgetting in incremental settings, we use theForgetting Percentage Points(FPP) metric, as
defined in [47]. FPP captures performance degradation by measuring the accuracy drop on
the initially seen categories between the first and final training phases. Detailed descriptions of the incremental scenarios (Split-A,Split-B,Split-C) and im-
plementation are provided in Appendix B. 4.2 Results and Discussion
Table 1: Comparison between the proposed method and the baseline in the 3DIS setting,
evaluated using mAP25, mAP50, mAP, and FPP after training across all phase. Scenarios Methods Average Precisionâ†‘FPPâ†“
mAP25 mAP50 mAP mAP 25 mAP50
Baseline 16.46 14.29 10.44 51.30 46.82Split-A CLIMB-3D(Ours) 35.69 31.05 22.72 3.44 2.63
Baseline 17.22 15.07 10.93 46.27 42.1Split-B CLIMB-3D(Ours) 35.48 31.56 23.69 8.00 5.51
Baseline 25.65 21.08 14.85 31.68 28.84Split-C CLIMB-3D(Ours) 31.59 26.78 18.93 9.10 7.89
To evaluate our proposed approach, we construct a baseline using ER [5] for the 3DIS
setting, as no existing incremental baselines are available for this task. In contrast, for the
3DSS setting, where incremental baselines do exist, we compare our method against three
prior approaches [43, 60, 74]. Results on CI-3DIS.Tab. 1 compares CLIMB-3D against the ER baseline across the three
CI-3DIS scenarios, evaluated after all phases using mAP 25, mAP50, overall mAP, and FPP. In theSplit-Ascenario, characterised by significant distribution shifts, our method achieves
gains of +19.23%, +16.76%, and +12.28% in mAP 25, mAP 50, and overall mAP, respec-
tively, while drastically reducing forgetting with FPP scores of 3.44% (mAP 25) and 2.63%
(mAP50), both over 45 points lower than the baseline. Under theSplit-Bscenario, with
semantically related new categories, improvements of +18.26%, +16.49%, and +12.76%
are observed alongside a reduction in forgetting to 8.00% and 5.51% from baseline levels
above 42%. For theSplit-Cscenario, featuring increasing geometric complexity, CLIMB-
3D maintains solid gains of +5.94%, +5.70%, and +4.08% across mAP metrics, with forget-
ting reduced by over 22 points. Overall, these results demonstrate robust forward transfer,
minimal forgetting, and stable learning, highlighting the methodâ€™s effectiveness for scalable
continual 3D semantic understanding. THENGANE ET AL.: CLIMB-3D9
Table 2: Comparison between the proposed method and existing baselines in the 3DSS set-
ting on ScanNetV2 [16], evaluated using mIoU. Methods Phase=1 Phase=2 All
EWC [60] 17.75 13.22 16.62
LwF [43] 30.38 13.37 26.13
Yanget al. [74] 34.16 13.43 28.98
CLIMB-3D(Ours) 69.39 32.56 59.38
Results on CI-3DSS.Although our primary focus is on CI-3DIS, we also evaluate our
method under the CI-3DSS setting to enable comparisons with existing approaches. To do
so, we adapt our predictions to assign each point the label corresponding to the highest-
confidence mask and exclude background classes (e.g., floor and wall), as these are not part
of the semantic segmentation targets. Tab. 2 presents a comparison between our method and
existing baselines on the ScanNet V2 dataset [16], evaluated using mIoU across two training
phases and overall. Although originally designed for instance segmentation, our method
generalises effectively to semantic segmentation, achieving substantial gains of +35.23%
mIoU in Phase 1 and +19.1% in Phase 2. Overall, it achieves 59.38% mIoU, significantly
higher than theâˆ¼30% mIoU of prior methods, highlighting its robustness and transferability. Refer to Appendix C, D, and F of the supplementary material for detailed analyses of per-
phase performance, rare-class evaluation, and qualitative result comparisons, respectively. 4.3 Ablation
Table 3: Ablation study illustrating the impact of each component in a three-phase setup. Each split (s) and corresponding number indicates data introduced at that phase. Best results
are highlighted inbold. Row Modules p=1â†‘p=2â†‘p=3â†‘ FPPâ†“
s1 s1 s2 All s1 s2 s3 All
1. Oracle - - - - 55.14 30.77 25.30 37.68 -
2. NaÃ¯ve 56.82 0.00 28.09 14.15 0.00 0.00 19.67 5.80 56.82
3. + ER 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28 46.44
4. + PLG 56.82 50.0034.3942.13 49.78 11.41 26.47 29.28 7.04
5. + CBR 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05 2.63
We conduct an ablation study to evaluate the contribution of each component in our
framework. As an upper bound, we report the performance of anOraclemodel, which is
trained jointly on the full dataset. For the incremental setting, we follow the previously
defined splits and first train the model naively across phases. We then incrementally add
each module to isolate its impact. Tab. 3 presents results for theSplit-Ascenario using both
mAP50 and FPP metrics. NaÃ¯ve Training.When trained naÃ¯vely without any dedicated modules, the model suffers
from severe catastrophic forgetting, as evident in row 2. The model entirely forgets pre-
viously learned classes upon entering new phases, with performance dropping to zero for
earlier splits and FPP rising to 56.82. 10THENGANE ET AL.: CLIMB-3D
Effect of ER.Adding exemplar replay (row 3) partially alleviates forgetting by maintaining
a buffer of past examples. This yields notable gains fors1in Phase 2 (+18.51%) and Phase 3
(+10.38%), and also improvess2in Phase 3 (+9.43%). However, the overall forgetting
remains substantial (FPP: 46.44), showing ER alone is insufficient. Effect of PLG.The addition of the pseudo-label generator (PLG), which generates labels for
previous classes by retaining a copy of the model from an earlier phase, facilitates knowledge
retention and forward transfer. As shown in row 4, PLG significantly reduces forgetting and
enhances performance on current tasks. Fors1, it improves mAP 50 by 31.49% in Phase 2
and 39.40% in Phase 3 over exemplar replay. Overall, PLG yields a 15.00% increase in
performance and reduces forgetting by 39.40%. Effect of CBR.Finally, class-balanced re-weighting (CBR) mitigates class imbalance dur-
ing both pseudo-labelling and current task learning by adjusting each classâ€™s contribution
based on its frequency. As shown in row 5, CBR enhancess1retention over PLG in Phases 2
and 3 (+4.67% and +4.41%), and further improves performance ons2ands3. It also achieves
the lowest FPP of 2.63, reflecting strong forgetting mitigation. Overall, CBR offers the best
trade-off between retaining past knowledge and acquiring new tasks, outperforming PLG
and ER by 4.41% and 43.81%, respectively. 4.4 Limitations & Future Work
While CLIMB-3D achieves strong performance across CI-3DIS and CI-3DSS settings, sev-
eral limitations remain and warrant discussion. The experiments are currently limited to
indoor datasets (ScanNet200 and ScanNetV2), therefore evaluating performance on outdoor
scenes would help assess generalisability across diverse environments. The current setup
also considers only three incremental phases, whereas real-world applications often involve
longer sequences; incorporating such extended setups would better reflect practical chal-
lenges. Moreover, the model operates in a uni-modal setting. Integrating multi-modal cues,
such as vision-language models, could further enhance performance, as suggested by recent
2D and 3D studies [25, 65, 66]. 5 Conclusion
This work addresses the challenge of catastrophic forgetting in class-incremental 3D in-
stance segmentation by introducing a modular framework that integrates exemplar replay
(ER), a pseudo-label generator (PLG), and class-balanced re-weighting (CBR). The pro-
posed method incrementally adapts to new classes while preserving prior knowledge, all
without requiring access to the full dataset. Extensive experiments on a three-phase bench-
mark validate the individual and combined effectiveness of each component: ER enables
efficient memory-based retention; PLG facilitates knowledge preservation through pseudo-
supervision; and CBR mitigates class imbalance to enhance learning stability. Together,
these components significantly reduce forgetting and improve segmentation performance
across all phases. By achieving a strong balance between stability and plasticity, our ap-
proach advances continual 3D scene understanding and sets a new baseline for future re-
search in this area. THENGANE ET AL.: CLIMB-3D11
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne
Tuytelaars. Memory aware synapses: Learning what (not) to forget. InECCV, pages
139â€“154, 2018. [2] Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long-tailed
recognition via weight balancing. InCVPR, pages 6897â€“6907, 2022. [3] Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying
imbalanced data. InJoint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 770â€“785.

[78] Biao Zhang and Peter Wonka. Point cloud instance segmentation using probabilistic
embeddings. InCVPR, pages 8883â€“8892, 2021. [79] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsuper-
vised lifelong learning.Neural Networks, 149:95â€“106, 2022. [80] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang,
and Jiaya Jia. Understanding imbalanced semantic segmentation through neural col-
lapse. InCVPR, pages 19550â€“19560, 2023. [81] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch
network with cumulative learning for long-tailed visual recognition. InCVPR, pages
9719â€“9728, 2020. [82] Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang
Jiang. Balanced contrastive learning for long-tailed visual recognition. InCVPR, pages
6908â€“6917, 2022. THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced 3D Instance
Segmentation
Supplementary Material
Appendix A Illustrating Incremental scenarios
Frequent Scenario Semantic Scenario
background
frequent common
rare semantic 
cluster 1
semantic 
cluster 2
semantic 
cluster 3
background
random 
cluster 1
random 
cluster 2
random 
cluster 3
background
Random Scenario
Figure 3: Tasks are grouped into incremental scenarios based on object frequency, semantic
similarity, and random assignment. , , and denote different tasks; shapes indicate
object categories; marks the background.Left:Grouped by category frequency.Middle:
Grouped by semantic similarity (e.g., similar shapes).Right:Randomly grouped, mixing
semantic and frequency variations. Appendix B Per Phase Analysis
We extend the analysis from Tab."
What is the focus of the supplementary material provided in the document chunk?,"In this supplementary material, we first demonstrate the performance gains on rare classes achieved by incorporating the IC module in Appendix C. Next, we provide detailed split information for all scenarios, based on class names, in Appendix D. Finally, we present a qualitative comparison between the baseline method and our proposed approach in Appendix E.","1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

[78] Biao Zhang and Peter Wonka. Point cloud instance segmentation using probabilistic
embeddings. InCVPR, pages 8883â€“8892, 2021. [79] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsuper-
vised lifelong learning.Neural Networks, 149:95â€“106, 2022. [80] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang,
and Jiaya Jia. Understanding imbalanced semantic segmentation through neural col-
lapse. InCVPR, pages 19550â€“19560, 2023. [81] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch
network with cumulative learning for long-tailed visual recognition. InCVPR, pages
9719â€“9728, 2020. [82] Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang
Jiang. Balanced contrastive learning for long-tailed visual recognition. InCVPR, pages
6908â€“6917, 2022. THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced 3D Instance
Segmentation
Supplementary Material
Appendix A Illustrating Incremental scenarios
Frequent Scenario Semantic Scenario
background
frequent common
rare semantic 
cluster 1
semantic 
cluster 2
semantic 
cluster 3
background
random 
cluster 1
random 
cluster 2
random 
cluster 3
background
Random Scenario
Figure 3: Tasks are grouped into incremental scenarios based on object frequency, semantic
similarity, and random assignment. , , and denote different tasks; shapes indicate
object categories; marks the background.Left:Grouped by category frequency.Middle:
Grouped by semantic similarity (e.g., similar shapes).Right:Randomly grouped, mixing
semantic and frequency variations. Appendix B Per Phase Analysis
We extend the analysis from Tab.

In this supplementary material, we first demonstrate the performance gains on rare classes achieved by incorporating the IC module in Appendix C. Next, we provide detailed split information for all scenarios, based on class names, in Appendix D. Finally, we present a qualitative comparison between the baseline method and our proposed approach in Appendix E.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig."
Where can detailed split information for all scenarios be found in the document chunk?,"Next, we provide detailed split information for all scenarios, based on class names, in Appendix D.","1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

[78] Biao Zhang and Peter Wonka. Point cloud instance segmentation using probabilistic
embeddings. InCVPR, pages 8883â€“8892, 2021. [79] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsuper-
vised lifelong learning.Neural Networks, 149:95â€“106, 2022. [80] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang,
and Jiaya Jia. Understanding imbalanced semantic segmentation through neural col-
lapse. InCVPR, pages 19550â€“19560, 2023. [81] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch
network with cumulative learning for long-tailed visual recognition. InCVPR, pages
9719â€“9728, 2020. [82] Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang
Jiang. Balanced contrastive learning for long-tailed visual recognition. InCVPR, pages
6908â€“6917, 2022. THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced 3D Instance
Segmentation
Supplementary Material
Appendix A Illustrating Incremental scenarios
Frequent Scenario Semantic Scenario
background
frequent common
rare semantic 
cluster 1
semantic 
cluster 2
semantic 
cluster 3
background
random 
cluster 1
random 
cluster 2
random 
cluster 3
background
Random Scenario
Figure 3: Tasks are grouped into incremental scenarios based on object frequency, semantic
similarity, and random assignment. , , and denote different tasks; shapes indicate
object categories; marks the background.Left:Grouped by category frequency.Middle:
Grouped by semantic similarity (e.g., similar shapes).Right:Randomly grouped, mixing
semantic and frequency variations. Appendix B Per Phase Analysis
We extend the analysis from Tab.

Next, we provide detailed split information for all scenarios, based on class names, in Appendix D.

5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks."
What is the average improvement in mAP 50 for rare classes in Phase 2 of Split-A when the IC module is applied?,"As illustrated in Tab. 5, the IC module substantially improves performance on rare classes in terms of mAP 50 in Phase 2 of Split-A. For instance, classes like recycling bin and trash bin, seen only 3 and 7 times, respectively, shows significant improvement when the IC module is applied. Overall, the IC module provides an average boost of 8.32%, highlighting its effectiveness in mitigating class imbalance.","1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

As illustrated in Tab. 5, the IC module substantially improves performance on rare classes in terms of mAP 50 in Phase 2 of Split-A. For instance, classes like recycling bin and trash bin, seen only 3 and 7 times, respectively, shows significant improvement when the IC module is applied. Overall, the IC module provides an average boost of 8.32%, highlighting its effectiveness in mitigating class imbalance.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

[78] Biao Zhang and Peter Wonka. Point cloud instance segmentation using probabilistic
embeddings. InCVPR, pages 8883â€“8892, 2021. [79] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsuper-
vised lifelong learning.Neural Networks, 149:95â€“106, 2022. [80] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang,
and Jiaya Jia. Understanding imbalanced semantic segmentation through neural col-
lapse. InCVPR, pages 19550â€“19560, 2023. [81] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch
network with cumulative learning for long-tailed visual recognition. InCVPR, pages
9719â€“9728, 2020. [82] Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang
Jiang. Balanced contrastive learning for long-tailed visual recognition. InCVPR, pages
6908â€“6917, 2022. THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced 3D Instance
Segmentation
Supplementary Material
Appendix A Illustrating Incremental scenarios
Frequent Scenario Semantic Scenario
background
frequent common
rare semantic 
cluster 1
semantic 
cluster 2
semantic 
cluster 3
background
random 
cluster 1
random 
cluster 2
random 
cluster 3
background
Random Scenario
Figure 3: Tasks are grouped into incremental scenarios based on object frequency, semantic
similarity, and random assignment. , , and denote different tasks; shapes indicate
object categories; marks the background.Left:Grouped by category frequency.Middle:
Grouped by semantic similarity (e.g., similar shapes).Right:Randomly grouped, mixing
semantic and frequency variations. Appendix B Per Phase Analysis
We extend the analysis from Tab."
How does the IC module impact the performance of infrequent classes in Phase 3?,"Similarly, Tab. 6 presents results for Phase 3, demonstrating significant gains for infrequent classes. For example, even though the classes such as piano, bucket, and laundry basket are observed only once, IC module improves the performance by 52.30%, 10.40%, and 13.60%, respectively.","Similarly, Tab. 6 presents results for Phase 3, demonstrating significant gains for infrequent classes. For example, even though the classes such as piano, bucket, and laundry basket are observed only once, IC module improves the performance by 52.30%, 10.40%, and 13.60%, respectively.

1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

[78] Biao Zhang and Peter Wonka. Point cloud instance segmentation using probabilistic
embeddings. InCVPR, pages 8883â€“8892, 2021. [79] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsuper-
vised lifelong learning.Neural Networks, 149:95â€“106, 2022. [80] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang,
and Jiaya Jia. Understanding imbalanced semantic segmentation through neural col-
lapse. InCVPR, pages 19550â€“19560, 2023. [81] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch
network with cumulative learning for long-tailed visual recognition. InCVPR, pages
9719â€“9728, 2020. [82] Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang
Jiang. Balanced contrastive learning for long-tailed visual recognition. InCVPR, pages
6908â€“6917, 2022. THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced 3D Instance
Segmentation
Supplementary Material
Appendix A Illustrating Incremental scenarios
Frequent Scenario Semantic Scenario
background
frequent common
rare semantic 
cluster 1
semantic 
cluster 2
semantic 
cluster 3
background
random 
cluster 1
random 
cluster 2
random 
cluster 3
background
Random Scenario
Figure 3: Tasks are grouped into incremental scenarios based on object frequency, semantic
similarity, and random assignment. , , and denote different tasks; shapes indicate
object categories; marks the background.Left:Grouped by category frequency.Middle:
Grouped by semantic similarity (e.g., similar shapes).Right:Randomly grouped, mixing
semantic and frequency variations. Appendix B Per Phase Analysis
We extend the analysis from Tab."
What is the performance improvement percentage for ER+KD when the IC module is added into the framework?,"On average, the addition of the proposed IC module into the framework outperforms ER+KD by 12.13%.","1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

On average, the addition of the proposed IC module into the framework outperforms ER+KD by 12.13%.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks."
Which classes are not focused on by the ER+KD module but are compensated for by the IC module due to underrepresentation?,"The ER+KD module does not focus on rare classes like shower and toaster which results in low performance, but the IC module compensates for this imbalance by focusing on underrepresented categories.","The ER+KD module does not focus on rare classes like shower and toaster which results in low performance, but the IC module compensates for this imbalance by focusing on underrepresented categories.

It may be distributed unchanged freely in print or electronic forms. 2THENGANE ET AL.: CLIMB-3D
1 Introduction
Table Chair Couch Pillow Coffee Table Sofa Chair Recycling BinCabinet Unknown
Task-1 Task-2 Task-3
Figure 1:Overview of the CI-3DIS setting.New object categories are introduced incre-
mentally with each task. After every phase, the model must recognise both newly added
and previously learned classes. For instance, in Task 2, categories such asPillow,Coffee
Table, andSofa Chairare introduced, while the model is expected to retain recognition
of earlier classes likeTable,Chair, andCouch. 3D instance segmentation (3DIS) is a fundamental task in computer vision that involves
identifying precise object boundaries and class labels in 3D space, with broad applications in
graphics, robotics, and augmented reality [4, 39]. Traditional 3DIS methods, including top-
down [28, 70, 78], bottom-up [26, 73], and transformer-based approaches [59], perform well
under the assumption that all classes are available and balanced during training. However,
this assumption does not hold in real-world environments, where new object classes emerge
over time and exhibit natural imbalance. This gap motivates the need for Class-Incremental Learning (CIL) [17, 54], which sup-
ports learning new categories while retaining knowledge of previous ones [51]. While CIL
has seen success in 2D image tasks [1, 43, 54, 60], extensions to 3D point clouds remain
limited, often focusing on object-level classification [13, 18, 48]. Recent works on scene-
level class-incremental 3DIS (CI-3DIS) [4] and class-incremental semantic segmentation
(CI-3DSS) [74] show promise, but depend heavily on large exemplar [56] and overlook class
imbalance, limiting their practicality. To address this, we proposeCLIMB-3D, a unified framework forCLass-incremental
IMBalance-aware3DIS that jointly tackles catastrophic forgetting and class imbalance. The
framework begins with Exemplar Replay (ER), storing a small number of samples from
past classes for later replay. However, this alone does not yield promising results under the
strict memory constraints required in CI-3DIS. To address this, we introduce a Pseudo-Label
Generator (PLG), which uses a frozen model from previous task to generate supervision for
earlier classes. However, we observed that PLG tends to favour frequent classes while ig-
noring under-represented ones. To mitigate this, we propose a Class-Balanced Re-weighting
(CBR), which estimates class frequencies from the pseudo-labels to enhance the learning of
rare classes without accessing past data. Together, these components form a practical and
effective solution for real-world 3DIS. To evaluate CLIMB-3D, we designed three benchmark scenarios for CI-3DIS based on
the ScanNet200 dataset [57]. These scenarios simulate incremental learning setup under nat-
ural class imbalance, where new classes emerge based on (A) object frequency, (B) semantic
similarity, or (C) random grouping. Additionally, for comparison with existing methods,
we evaluate our approach on CI-3DSS using the ScanNetV2 dataset [16]. Experimental re-
sults show that our method significantly reduces forgetting and improves performance across
both frequent and rare categories, outperforming previous methods in both settings. Fig. 1
illustrates the CI-3DIS setup. THENGANE ET AL.: CLIMB-3D3
In summary, our contributions are: (i) a novel problem setting of imbalanced class-
incremental 3DIS with an effective method to balance learning and mitigate forgetting; (ii)
three benchmarks modelling continual object emergence with natural imbalance; and (iii)
strong experimental results, achieving up to 16.76% mAP improvement over baselines. 2 Related Work
3D Instance Segmentation.Various methods have been proposed for 3DIS. Grouping-
based approaches adopt a bottom-up pipeline that learns latent embeddings for point cluster-
ing [9, 21, 22, 29, 37, 44, 70, 78]. In contrast, proposal-based methods follow a top-down
strategy by detecting 3D boxes and segmenting objects within them [19, 26, 46, 73, 76]. Recently, transformer-based models [68] have also been applied to 3DIS and 3DSS [59, 64],
inspired by advances in 2D vision [10, 11]. However, these approaches require full anno-
tations for all classes and are not designed for progressive learning, where only new class
annotations are available and previous data is inaccessible. Another line of work aims to re-
duce annotation costs by proposing weakly supervised 3DIS methods based on sparse cues
[12, 27, 72]. While effective with limited annotations, these methods assume a fixed set of
classes and are susceptible to catastrophic forgetting in incremental settings. Incremental Learning.Continual learning involves training models sequentially to miti-
gate catastrophic forgetting. One common strategy is model regularisation, which constrains
parameter updates using techniques such as elastic weight consolidation or knowledge dis-
tillation [1, 24, 35, 43, 60]. Another approach is exemplar replay, where past data is stored
or generated to rehearse older tasks while learning new ones [5, 7, 30, 54]. A third direc-
tion involves dynamically expanding the model architecture or using modular sub-networks
to accommodate new tasks without interfering with old ones [31, 42, 53, 58, 71, 79]. Re-
cent efforts in class-incremental 3D segmentation [63, 74] have shown early promise but
often rely on basic architectures and focus primarily on semantic segmentation. Other works
[4, 36] explore continual learning in 3D settings, though they often depend on large memory
buffers. In contrast, our work introduces a tailored framework for 3D instance segmentation
that effectively transfers knowledge across tasks, even under class imbalance. Long-tailed Recognition.Imbalanced class distributions lead to poor recognition of rare
(long-tail) classes. To address this, re-sampling methods balance the data via input [8, 20, 50,
52, 61] or feature space [3, 14, 40], avoiding naÃ¯ve under-/over-sampling. Loss re-weighting
offers another direction, using class-based [6, 15, 23, 32, 33, 69] or per-example [45, 55, 62]
adjustments to ensure fair contribution. Parameter regularisation improves generalisation
through weight constraints [2], albeit requiring careful tuning. Other approaches leverage
transfer learning [77, 81], self-supervision [41, 75], or contrastive learning [34, 41, 82] to
improve rare-class representations. Long-tailed recognition is well-studied in 2D with large-
scale datasets [15, 49, 67]; in 3D, ScanNet200 [16, 57] enables similar efforts. Prior 3D
work focused on re-weighting, re-sampling, and transfer learning [57], while regularisation
remains underexplored. CeCo [80] balances class centres via auxiliary loss but overlooks
sampling and augmentation. Lahoud et al. [38] propose adaptive classifier regularisation
for 3D segmentation, outperforming prior approaches without threshold tuning. However,
neither method considers incremental learning. In contrast, we jointly address long-tail and
continual 3D semantic segmentation. 4THENGANE ET AL.: CLIMB-3D
Feature 
Backbone
Transformer 
Decoder
Feature 
Backbone
Transformer 
Decoder
(ð‘·,ð’€ð‘¡)
Current model (ð›·
ð‘¡
)
Pseudo Label Generator (PLG) 
Selection
à·¨ð‘Œð¾
ð‘¡
â„’CIâˆ’3DIS
à·¨ð‘Œð¾
ð‘¡,ð‘¤
à´¤ð‘Œð¾
ð‘¡
à· ð‘Œð‘¡
(ð·ð‘¡)
Current Dataset 
Exemplar 
Replay (ER)
+
~
Class-Balanced  
Re-weighting 
(CBR)
1
ð‘“(ð‘)
Figure 2: Overview ofCLIMB-3Dfor CI-3DIS. The model incrementally learns new classes
across sequential phases. During taskt, point cloudsPand their corresponding labelsY t are
sampled from a combination of the current training datasetD t and Exemplar Replay (ER),
which maintains a small memory of past examples. These are then passed to the current
modelÎ¦ t to produce predicted labelsY t . The Pseudo-Label Generator (PLG) selects the
top-Kpredictions from the previous modelÎ¦ tâˆ’1 . These pseudo-labels are then weighted
based on class frequencyf(c)using Class-Balanced Re-weighting (CBR), and the top-Kre-
weighted labels are selected to form the balanced pseudo-label set Â¯Yt . This pseudo-label set
is then concatenated with the ground-truth labels to form a final augmented supervision set
Yt for taskt, which is used to optimise the modelÎ¦ t using Eq. (3). 3 Methodology
3.1 Problem Formulation
3DIS.The objective of this task is to detect and segment each object instance within a point
cloud. Formally, the training dataset is denoted asD={(P i,Yi)}N
i=1, whereNis the number
of training samples. Each inputP i âˆˆR MÃ—6 is a coloured point cloud consisting ofMpoints,
where each point is represented by its 3D coordinates and RGB values. The corresponding
annotationY i ={(m i,j ,ci,j )}Ji
j=1 containsJ i object instances, wheremi,j âˆˆ {0,1}M is a binary
mask indicating which points belong to thej-th instance, andc i,j âˆˆ C={1,...,C}is the
semantic class label of that instance. GivenP i, the modelÎ¦predicts instance-level outputs
Ë†Yi ={(Ë†mi,j ,Ë†ci,j )}
Ë†Ji
j=1, where Ë†mi,j and Ë†ci,j denote the predicted mask and category label for
thej-th instance, respectively. The number of predicted instances Ë†Ji varies depending on the
modelâ€™s inference. The model is optimised using the following objective:
L3DIS(D;Î¦) = 1
|D| âˆ‘
(P,Y)âˆˆD
1
|Y| âˆ‘
(mj,cj)âˆˆY
(Lmask(mj,Ë†mj)+Î»Â·L cls(cj,Ë†cj)),(1)
whereL cls andL mask are the average classification and mask losses over all instances, andÎ»
controls their trade-off. CI-3DIS.Unlike conventional 3DIS, CI-3DIS involves sequential learning, where object
categories are introduced incrementally overTtraining tasks. Each tasktintroduces a dis-
joint set of classesC t , withC= ST
t=1 Ct andC t âˆ©C tâ€²
=/ 0 fortÌ¸=t â€². During each taskt, the THENGANE ET AL.: CLIMB-3D5
model receives a datasetD t ={(P i,Yt
i)}N
i=1, where each coloured point cloudP i âˆˆR MÃ—6
containsMpoints, and the corresponding annotationY t
i ={(m t
i,j ,ct
i,j )}
Jt
i
j=1 includes instance
masksm t
i,j âˆˆ {0,1}M and semantic class labelsc t
i,j âˆˆ Ct . The modelÎ¦ t , initialized from
Î¦tâˆ’1 , is trained onD t to predict instance-level outputs Ë†Yt
i ={(Ë†mt
i,j ,Ë†ct
i,j )}
Ë†Jt
i
j=1, where each
predicted semantic label Ë†ct
i,j belongs to the cumulative set of all classes observed so far:St
k=1 Ck. The key challenge in this setup is to learn new classes without forgetting those
encountered in previous tasks, despite limited supervision and the absence of past labels. 3.2 Method: CLIMB-3D
Overview.As shown in Fig. 2 and formulated in Sec. 3.1, the proposed framework for
CI-3DIS follows aphase-wisetraining strategy. In each phase, the model is exposed to a
carefully curated subset of the dataset, simulating real-world scenarios discussed in Sec. 3.3. Training naÃ¯vely on such phased data leads tocatastrophic forgetting[51], where the model
forgets prior knowledge when learning new tasks. To address this, we first incorporateEx-
emplar Replay(ER) [5], a strategy inspired by 2D incremental methods [1, 43, 54] and
recently extended to 3D settings [4], to mitigate forgetting by storing a small set of repre-
sentative samples from earlier phases. However, ER alone is insufficient to achieve robust
performance. To address this, two additional components are introduced: aPseudo-Label
Generator(PLG), which leverages a frozen model to generate supervision signals for previ-
ously seen classes, and aClass-Balanced Re-weighting(CBR) module that compensates for
class imbalance across phases. Each of these components is described in detail below 1. Exemplar Replay (ER).Inspired by Buzzega et al. [5], ER addresses the issue of catas-
trophic forgetting by allowing the model to retain a subset of data from earlier phases. During phaset, the model is trained not only on the current task dataD t , but also on a
set of exemplarsE 1:tâˆ’1, collected from previous phases. These exemplars are accumulated
incrementally:E 1:tâˆ’1 =E 1 âˆªÂ·Â·Â·âˆªE tâˆ’1 .Here,E t denotes the exemplar subset stored after
phaset, and|E t |denotes the exemplar replay size. The combined replay dataset is denoted
as:D t
ER =D t âˆªE 1:tâˆ’1.Epoch training consists of two stages: first, the model is trained on
the current data,D t ; then it is trained using exemplars fromE 1:tâˆ’1. While previous CI-3DIS approaches rely on large exemplar sets to mitigate forgetting [4],
such strategies become impractical under memory constraints. Consequently, although ER
aids knowledge retention, it is insufficient on its own for robust performance with a limited
exemplar budget. Therefore, PLG and CBR are introduced to better preserve past represen-
tations and enhance generalisation across phases. Pseudo-Label Generator (PLG).In the CI-3DIS setting, during phaset(wheret>1),
although ground-truth labels for previously seen classes are unavailable, the model from the
previous phaseÎ¦ tâˆ’1 , which preserves knowledge of past tasks, is retained. We use this
model to generate pseudo-labels for previously seen classes, thereby providing approximate
supervision during training. This allows the current modelÎ¦t , to retain prior knowledge and
reduce forgetting, even without access to ground-truth annotations. Given a point cloud and label pair(P,Y t ), fromD t , the previous modelÎ¦ tâˆ’1 , generates
pseudo-labels for the previously seen classes as Ë†Y1:tâˆ’1 =Î¦ tâˆ’1 (P), which we denote as ËœYt
1For clarity, modifications introduced by each component (ER, PLG, and CBR) are highlighted in green, blue,
and red, respectively. 6THENGANE ET AL.: CLIMB-3D
for brevity. However, we observe that utilising all predictions fromÎ¦tâˆ’1 introduces noisy or
incorrect labels, which degrade overall performance. To mitigate this, we select the top-K
most confident instance predictions, denoted as ËœYt
K, and concatenate them with the ground-
truth labels of the current task to obtain the supervision set: Â¯Yt =Y t âˆ¥ ËœYt
K. Following this,
the loss is computed over both the current taskâ€™s ground-truth labels and the top-Kconfi-
dent pseudo-labels from earlier phases, enabling the model to retain prior knowledge while
adapting to new classes. Class-Balanced Re-weighting (CBR).While ER and PLG contribute to preserving prior
knowledge, we observe that the modelâ€™s predictions are biased towards frequent classes,
resulting in the forgetting of rare categories. This issue is further amplified by the top-K
pseudo-labels selected by PLG, which predominantly represent dominant classes in the data. To address this, we propose aClass-Balanced Re-weighting(CBR) scheme that compensates
for class imbalance using object frequency statistics. At taskt, only the current datasetD t
and its label distributionp t (c)over classescâˆˆ Ct are available. Datasets and class distribu-
tions from earlier tasks{D i,p i(c)}tâˆ’1
i=1 are no longer accessible, which makes it challenging
to directly account for class imbalance across all previously seen tasks. Therefore, we pro-
pose to leverage the pseudo-label predictions of the frozen modelÎ¦ tâˆ’1 as a proxy for the
class distribution across previously learned categories. During each training iteration, the previous modelÎ¦ tâˆ’1 is applied to the current inputP
to generate pseudo-labels ËœYt , from which we accumulate class-wise frequency statistics for
previously learned classes. LetC1:t = St
i=1 Ci represent the union of all classes seen up to task
t. At the end of each epoch, we compute the overall class frequencyfâˆˆR |C1:t | by combining
the pseudo-label distribution Ëœpt (c)derived from ËœYt with the ground-truth label distribution
pt (c)from the current task. The resulting frequency vectorf= [f(c)] câˆˆC1:t is then used to
compute class-wise weightsw= [w(c)] câˆˆC1:t , which guide balanced pseudo-label selection
and help the model focus on rare classes. In the next epoch, the modelÎ¦ tâˆ’1 produces soft pseudo-label predictions ËœYt (as de-
scribed in PLG) , where each component ËœYt [c]denotes the confidence score for classcâˆˆ
C1:tâˆ’1. To promote balanced pseudo-label selection, class-wise re-weighting is applied using
w(c) = 1
f(c)+Îµ , whereÎµis a small constant for numerical stability. The adjusted predictions
are computed by applying the class-wise weights as: ËœYt,w [c] =w(c)Â· ËœYt [c],âˆ€câˆˆ C 1:tâˆ’1. Top-Kselection is then applied to both the original and the re-weighted scores, yielding
two pseudo-label sets: ËœYt
K and ËœYt,w
K , respectively. The final augmented target set, Yt , is
constructed by concatenating the ground-truth labelsY t with both sets of pseudo-labels:
Yt =Y t âˆ¥ ËœYt
K âˆ¥ ËœYt,w
K .(2)
Although the above re-weighting mitigates bias from prior tasks, it does not address class
imbalance within the current taskt. To resolve this, the class-balanced weightsw c are ex-
tended to incorporate statistics from both previously seen and current classes, i.e., overC 1:t
rather than onlyC 1:tâˆ’1. The updated weights, denoted asw â€²
c, are used to re-weights both
pseudo-label selection and classification loss, ensuring balanced supervision across all seen
categories, including rare ones. This unified strategy reduces bias and enhances performance
across phases. The pseudo-code for constructing the supervision set in Eq. (2), which incor-
porates both PLG and CBR components, is provided in Appendix A. Final Objective.The final training objective ofCLIMB-3D, incorporating ER, PLG, and THENGANE ET AL.: CLIMB-3D7
CBR, is formulated in Eq. (1) as:
L(Dt
ER;Î¦t ) = 1
|Dt
ER| âˆ‘
(P,Yt )âˆˆDt
ER
1
|Yt | âˆ‘
(mt
j,ct
j)âˆˆYt
 
Lmask(mt
j,Ë†mt
j)+w â€²â€²
c Â·Lcls(ct
j,Ë†ct
j)

,(3)
wherew â€²â€²
c =w â€²
c Â·Î» cls denotes the scaled weight vector, andÎ» cls is a hyperparameter for bal-
ancing the loss terms. 3.3 Benchmarking Incremental Scenarios
While CI-3DIS methods offer a wide range of practical applications, they frequently rely
on the assumption of uniform sample distribution, which rarely holds in real-world settings. In practice, the number of object categories, denoted byC, is often large and characterised
by substantial variability in category frequency, shape, structure, and size. To address these
challenges, we propose three incremental learning scenarios, each designed to capture a
different facet of real-world complexity. The design of these scenarios is detailed below; for
further information and illustrations, refer to Appendix E of the supplementary material. Split-A: Frequency Scenarios.This scenario acknowledges that datasets are often la-
belled based on the frequency of categories. To accommodate this, we propose a split where
the model learns from the most frequent categories and subsequently incorporates the less
frequent ones in later stages. By prioritising the training of frequently occurring categories,
the model can establish a strong foundation before expanding to handle rare categories. Split-B: Semantic Scenarios.Beyond frequency, semantic similarity is crucial in real-
world deployments. While objects often share visual or functional traits, models may en-
counter semantically different categories in new environments. To simulate this challenge,
we introduce theSplit-Bscenario. Here, categories are grouped based on their semantic
relationships, and the model is incrementally trained on one semantic group at a time. This
setup encourages the model to generalise across semantically similar categories and adapt
more effectively when exposed to new ones. Unlike theSplit-Ascenario, which organises
learning based on category frequency, theSplit-Bscenario may contain both frequent and
infrequent categories within a single task, focusing on semantic continuity and transfer. Split-C: Random Scenarios.In some cases, data labelling is driven by the availability of
objects rather than predefined criteria. To capture this, we introduce theSplit-Cscenario,
which represents a fully random setting where any class can appear in any task, resulting in
varying levels of class imbalance. By exposing the model to such diverse and unpredictable
distributions, we aim to improve its robustness in real-world situations where labelled data
availability is inconsistent. These incremental scenarios are designed to provide a more realistic representation of
object distributions, frequencies, and dynamics encountered in the real world. 4 Experiments
4.1 Setup
Datasets.We evaluateCLIMB-3Don ScanNet200 [57], which comprises 200 object cat-
egories and exhibits significant class imbalance, which makes it ideal for simulating and 8THENGANE ET AL.: CLIMB-3D
assessing real-world scenarios. In addition, we compare our approach against existing in-
cremental learning methods using ScanNetV2 [16] in the 3DSS setting. For this evaluation,
we follow the standard training and validation splits defined in prior works [74] to ensure
consistency with existing methods. Evaluation Metrics.We evaluate our method usingMean Average Precision(mAP), a
standard metric for 3DIS which provides a comprehensive measure of segmentation quality
by accounting for both precision and recall. For comparison with existing 3DSS methods,
we report themean Intersection over Union(mIoU), which quantifies the overlap between
predicted and ground truth segments. To assess the modelâ€™s ability to mitigate catastrophic
forgetting in incremental settings, we use theForgetting Percentage Points(FPP) metric, as
defined in [47]. FPP captures performance degradation by measuring the accuracy drop on
the initially seen categories between the first and final training phases. Detailed descriptions of the incremental scenarios (Split-A,Split-B,Split-C) and im-
plementation are provided in Appendix B. 4.2 Results and Discussion
Table 1: Comparison between the proposed method and the baseline in the 3DIS setting,
evaluated using mAP25, mAP50, mAP, and FPP after training across all phase. Scenarios Methods Average Precisionâ†‘FPPâ†“
mAP25 mAP50 mAP mAP 25 mAP50
Baseline 16.46 14.29 10.44 51.30 46.82Split-A CLIMB-3D(Ours) 35.69 31.05 22.72 3.44 2.63
Baseline 17.22 15.07 10.93 46.27 42.1Split-B CLIMB-3D(Ours) 35.48 31.56 23.69 8.00 5.51
Baseline 25.65 21.08 14.85 31.68 28.84Split-C CLIMB-3D(Ours) 31.59 26.78 18.93 9.10 7.89
To evaluate our proposed approach, we construct a baseline using ER [5] for the 3DIS
setting, as no existing incremental baselines are available for this task. In contrast, for the
3DSS setting, where incremental baselines do exist, we compare our method against three
prior approaches [43, 60, 74]. Results on CI-3DIS.Tab. 1 compares CLIMB-3D against the ER baseline across the three
CI-3DIS scenarios, evaluated after all phases using mAP 25, mAP50, overall mAP, and FPP. In theSplit-Ascenario, characterised by significant distribution shifts, our method achieves
gains of +19.23%, +16.76%, and +12.28% in mAP 25, mAP 50, and overall mAP, respec-
tively, while drastically reducing forgetting with FPP scores of 3.44% (mAP 25) and 2.63%
(mAP50), both over 45 points lower than the baseline. Under theSplit-Bscenario, with
semantically related new categories, improvements of +18.26%, +16.49%, and +12.76%
are observed alongside a reduction in forgetting to 8.00% and 5.51% from baseline levels
above 42%. For theSplit-Cscenario, featuring increasing geometric complexity, CLIMB-
3D maintains solid gains of +5.94%, +5.70%, and +4.08% across mAP metrics, with forget-
ting reduced by over 22 points. Overall, these results demonstrate robust forward transfer,
minimal forgetting, and stable learning, highlighting the methodâ€™s effectiveness for scalable
continual 3D semantic understanding. THENGANE ET AL.: CLIMB-3D9
Table 2: Comparison between the proposed method and existing baselines in the 3DSS set-
ting on ScanNetV2 [16], evaluated using mIoU. Methods Phase=1 Phase=2 All
EWC [60] 17.75 13.22 16.62
LwF [43] 30.38 13.37 26.13
Yanget al. [74] 34.16 13.43 28.98
CLIMB-3D(Ours) 69.39 32.56 59.38
Results on CI-3DSS.Although our primary focus is on CI-3DIS, we also evaluate our
method under the CI-3DSS setting to enable comparisons with existing approaches. To do
so, we adapt our predictions to assign each point the label corresponding to the highest-
confidence mask and exclude background classes (e.g., floor and wall), as these are not part
of the semantic segmentation targets. Tab. 2 presents a comparison between our method and
existing baselines on the ScanNet V2 dataset [16], evaluated using mIoU across two training
phases and overall. Although originally designed for instance segmentation, our method
generalises effectively to semantic segmentation, achieving substantial gains of +35.23%
mIoU in Phase 1 and +19.1% in Phase 2. Overall, it achieves 59.38% mIoU, significantly
higher than theâˆ¼30% mIoU of prior methods, highlighting its robustness and transferability. Refer to Appendix C, D, and F of the supplementary material for detailed analyses of per-
phase performance, rare-class evaluation, and qualitative result comparisons, respectively. 4.3 Ablation
Table 3: Ablation study illustrating the impact of each component in a three-phase setup. Each split (s) and corresponding number indicates data introduced at that phase. Best results
are highlighted inbold. Row Modules p=1â†‘p=2â†‘p=3â†‘ FPPâ†“
s1 s1 s2 All s1 s2 s3 All
1. Oracle - - - - 55.14 30.77 25.30 37.68 -
2. NaÃ¯ve 56.82 0.00 28.09 14.15 0.00 0.00 19.67 5.80 56.82
3. + ER 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28 46.44
4. + PLG 56.82 50.0034.3942.13 49.78 11.41 26.47 29.28 7.04
5. + CBR 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05 2.63
We conduct an ablation study to evaluate the contribution of each component in our
framework. As an upper bound, we report the performance of anOraclemodel, which is
trained jointly on the full dataset. For the incremental setting, we follow the previously
defined splits and first train the model naively across phases. We then incrementally add
each module to isolate its impact. Tab. 3 presents results for theSplit-Ascenario using both
mAP50 and FPP metrics. NaÃ¯ve Training.When trained naÃ¯vely without any dedicated modules, the model suffers
from severe catastrophic forgetting, as evident in row 2. The model entirely forgets pre-
viously learned classes upon entering new phases, with performance dropping to zero for
earlier splits and FPP rising to 56.82. 10THENGANE ET AL.: CLIMB-3D
Effect of ER.Adding exemplar replay (row 3) partially alleviates forgetting by maintaining
a buffer of past examples. This yields notable gains fors1in Phase 2 (+18.51%) and Phase 3
(+10.38%), and also improvess2in Phase 3 (+9.43%). However, the overall forgetting
remains substantial (FPP: 46.44), showing ER alone is insufficient. Effect of PLG.The addition of the pseudo-label generator (PLG), which generates labels for
previous classes by retaining a copy of the model from an earlier phase, facilitates knowledge
retention and forward transfer. As shown in row 4, PLG significantly reduces forgetting and
enhances performance on current tasks. Fors1, it improves mAP 50 by 31.49% in Phase 2
and 39.40% in Phase 3 over exemplar replay. Overall, PLG yields a 15.00% increase in
performance and reduces forgetting by 39.40%. Effect of CBR.Finally, class-balanced re-weighting (CBR) mitigates class imbalance dur-
ing both pseudo-labelling and current task learning by adjusting each classâ€™s contribution
based on its frequency. As shown in row 5, CBR enhancess1retention over PLG in Phases 2
and 3 (+4.67% and +4.41%), and further improves performance ons2ands3. It also achieves
the lowest FPP of 2.63, reflecting strong forgetting mitigation. Overall, CBR offers the best
trade-off between retaining past knowledge and acquiring new tasks, outperforming PLG
and ER by 4.41% and 43.81%, respectively. 4.4 Limitations & Future Work
While CLIMB-3D achieves strong performance across CI-3DIS and CI-3DSS settings, sev-
eral limitations remain and warrant discussion. The experiments are currently limited to
indoor datasets (ScanNet200 and ScanNetV2), therefore evaluating performance on outdoor
scenes would help assess generalisability across diverse environments. The current setup
also considers only three incremental phases, whereas real-world applications often involve
longer sequences; incorporating such extended setups would better reflect practical chal-
lenges. Moreover, the model operates in a uni-modal setting. Integrating multi-modal cues,
such as vision-language models, could further enhance performance, as suggested by recent
2D and 3D studies [25, 65, 66]. 5 Conclusion
This work addresses the challenge of catastrophic forgetting in class-incremental 3D in-
stance segmentation by introducing a modular framework that integrates exemplar replay
(ER), a pseudo-label generator (PLG), and class-balanced re-weighting (CBR). The pro-
posed method incrementally adapts to new classes while preserving prior knowledge, all
without requiring access to the full dataset. Extensive experiments on a three-phase bench-
mark validate the individual and combined effectiveness of each component: ER enables
efficient memory-based retention; PLG facilitates knowledge preservation through pseudo-
supervision; and CBR mitigates class imbalance to enhance learning stability. Together,
these components significantly reduce forgetting and improve segmentation performance
across all phases. By achieving a strong balance between stability and plasticity, our ap-
proach advances continual 3D scene understanding and sets a new baseline for future re-
search in this area. THENGANE ET AL.: CLIMB-3D11
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne
Tuytelaars. Memory aware synapses: Learning what (not) to forget. InECCV, pages
139â€“154, 2018. [2] Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long-tailed
recognition via weight balancing. InCVPR, pages 6897â€“6907, 2022. [3] Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying
imbalanced data. InJoint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 770â€“785.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab."
What are the classes that show significant improvement in mAP 50 in Phase 2 of Split-A when the IC module is applied?,"As illustrated in Tab. 5, the IC module substantially improves performance on rare classes in terms of mAP 50 in Phase 2 of Split-A. For instance, classes like recycling bin and trash bin, seen only 3 and 7 times, respectively, shows significant improvement when the IC module is applied.","1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

As illustrated in Tab. 5, the IC module substantially improves performance on rare classes in terms of mAP 50 in Phase 2 of Split-A. For instance, classes like recycling bin and trash bin, seen only 3 and 7 times, respectively, shows significant improvement when the IC module is applied.

5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks."
What are the classes that are observed only once but experience performance improvements with the IC module in Phase 3?,"Similarly, Tab. 6 presents results for Phase 3, demonstrating significant gains for infrequent classes. For example, even though the classes such as piano, bucket, and laundry basket are observed only once, IC module improves the performance by 52.30%, 10.40%, and 13.60%, respectively.","5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

Similarly, Tab. 6 presents results for Phase 3, demonstrating significant gains for infrequent classes. For example, even though the classes such as piano, bucket, and laundry basket are observed only once, IC module improves the performance by 52.30%, 10.40%, and 13.60%, respectively.

[78] Biao Zhang and Peter Wonka. Point cloud instance segmentation using probabilistic
embeddings. InCVPR, pages 8883â€“8892, 2021. [79] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsuper-
vised lifelong learning.Neural Networks, 149:95â€“106, 2022. [80] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang,
and Jiaya Jia. Understanding imbalanced semantic segmentation through neural col-
lapse. InCVPR, pages 19550â€“19560, 2023. [81] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch
network with cumulative learning for long-tailed visual recognition. InCVPR, pages
9719â€“9728, 2020. [82] Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang
Jiang. Balanced contrastive learning for long-tailed visual recognition. InCVPR, pages
6908â€“6917, 2022. THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced 3D Instance
Segmentation
Supplementary Material
Appendix A Illustrating Incremental scenarios
Frequent Scenario Semantic Scenario
background
frequent common
rare semantic 
cluster 1
semantic 
cluster 2
semantic 
cluster 3
background
random 
cluster 1
random 
cluster 2
random 
cluster 3
background
Random Scenario
Figure 3: Tasks are grouped into incremental scenarios based on object frequency, semantic
similarity, and random assignment. , , and denote different tasks; shapes indicate
object categories; marks the background.Left:Grouped by category frequency.Middle:
Grouped by semantic similarity (e.g., similar shapes).Right:Randomly grouped, mixing
semantic and frequency variations. Appendix B Per Phase Analysis
We extend the analysis from Tab.

1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab."
What is the average performance improvement in mAP 50 for infrequent classes in Phase 3 with the addition of the IC module?,"Similarly, Tab. 6 presents results for Phase 3, demonstrating significant gains for infrequent classes.","[78] Biao Zhang and Peter Wonka. Point cloud instance segmentation using probabilistic
embeddings. InCVPR, pages 8883â€“8892, 2021. [79] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsuper-
vised lifelong learning.Neural Networks, 149:95â€“106, 2022. [80] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang,
and Jiaya Jia. Understanding imbalanced semantic segmentation through neural col-
lapse. InCVPR, pages 19550â€“19560, 2023. [81] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch
network with cumulative learning for long-tailed visual recognition. InCVPR, pages
9719â€“9728, 2020. [82] Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang
Jiang. Balanced contrastive learning for long-tailed visual recognition. InCVPR, pages
6908â€“6917, 2022. THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced 3D Instance
Segmentation
Supplementary Material
Appendix A Illustrating Incremental scenarios
Frequent Scenario Semantic Scenario
background
frequent common
rare semantic 
cluster 1
semantic 
cluster 2
semantic 
cluster 3
background
random 
cluster 1
random 
cluster 2
random 
cluster 3
background
Random Scenario
Figure 3: Tasks are grouped into incremental scenarios based on object frequency, semantic
similarity, and random assignment. , , and denote different tasks; shapes indicate
object categories; marks the background.Left:Grouped by category frequency.Middle:
Grouped by semantic similarity (e.g., similar shapes).Right:Randomly grouped, mixing
semantic and frequency variations. Appendix B Per Phase Analysis
We extend the analysis from Tab.

1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

Similarly, Tab. 6 presents results for Phase 3, demonstrating significant gains for infrequent classes."
Which module compensates for the imbalance in performance caused by the ER+KD module's neglect of certain classes?,"The ER+KD module does not focus on rare classes like shower and toaster which results in low performance, but the IC module compensates for this imbalance by focusing on underrepresented categories.","5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

It may be distributed unchanged freely in print or electronic forms. 2THENGANE ET AL.: CLIMB-3D
1 Introduction
Table Chair Couch Pillow Coffee Table Sofa Chair Recycling BinCabinet Unknown
Task-1 Task-2 Task-3
Figure 1:Overview of the CI-3DIS setting.New object categories are introduced incre-
mentally with each task. After every phase, the model must recognise both newly added
and previously learned classes. For instance, in Task 2, categories such asPillow,Coffee
Table, andSofa Chairare introduced, while the model is expected to retain recognition
of earlier classes likeTable,Chair, andCouch. 3D instance segmentation (3DIS) is a fundamental task in computer vision that involves
identifying precise object boundaries and class labels in 3D space, with broad applications in
graphics, robotics, and augmented reality [4, 39]. Traditional 3DIS methods, including top-
down [28, 70, 78], bottom-up [26, 73], and transformer-based approaches [59], perform well
under the assumption that all classes are available and balanced during training. However,
this assumption does not hold in real-world environments, where new object classes emerge
over time and exhibit natural imbalance. This gap motivates the need for Class-Incremental Learning (CIL) [17, 54], which sup-
ports learning new categories while retaining knowledge of previous ones [51]. While CIL
has seen success in 2D image tasks [1, 43, 54, 60], extensions to 3D point clouds remain
limited, often focusing on object-level classification [13, 18, 48]. Recent works on scene-
level class-incremental 3DIS (CI-3DIS) [4] and class-incremental semantic segmentation
(CI-3DSS) [74] show promise, but depend heavily on large exemplar [56] and overlook class
imbalance, limiting their practicality. To address this, we proposeCLIMB-3D, a unified framework forCLass-incremental
IMBalance-aware3DIS that jointly tackles catastrophic forgetting and class imbalance. The
framework begins with Exemplar Replay (ER), storing a small number of samples from
past classes for later replay. However, this alone does not yield promising results under the
strict memory constraints required in CI-3DIS. To address this, we introduce a Pseudo-Label
Generator (PLG), which uses a frozen model from previous task to generate supervision for
earlier classes. However, we observed that PLG tends to favour frequent classes while ig-
noring under-represented ones. To mitigate this, we propose a Class-Balanced Re-weighting
(CBR), which estimates class frequencies from the pseudo-labels to enhance the learning of
rare classes without accessing past data. Together, these components form a practical and
effective solution for real-world 3DIS. To evaluate CLIMB-3D, we designed three benchmark scenarios for CI-3DIS based on
the ScanNet200 dataset [57]. These scenarios simulate incremental learning setup under nat-
ural class imbalance, where new classes emerge based on (A) object frequency, (B) semantic
similarity, or (C) random grouping. Additionally, for comparison with existing methods,
we evaluate our approach on CI-3DSS using the ScanNetV2 dataset [16]. Experimental re-
sults show that our method significantly reduces forgetting and improves performance across
both frequent and rare categories, outperforming previous methods in both settings. Fig. 1
illustrates the CI-3DIS setup. THENGANE ET AL.: CLIMB-3D3
In summary, our contributions are: (i) a novel problem setting of imbalanced class-
incremental 3DIS with an effective method to balance learning and mitigate forgetting; (ii)
three benchmarks modelling continual object emergence with natural imbalance; and (iii)
strong experimental results, achieving up to 16.76% mAP improvement over baselines. 2 Related Work
3D Instance Segmentation.Various methods have been proposed for 3DIS. Grouping-
based approaches adopt a bottom-up pipeline that learns latent embeddings for point cluster-
ing [9, 21, 22, 29, 37, 44, 70, 78]. In contrast, proposal-based methods follow a top-down
strategy by detecting 3D boxes and segmenting objects within them [19, 26, 46, 73, 76]. Recently, transformer-based models [68] have also been applied to 3DIS and 3DSS [59, 64],
inspired by advances in 2D vision [10, 11]. However, these approaches require full anno-
tations for all classes and are not designed for progressive learning, where only new class
annotations are available and previous data is inaccessible. Another line of work aims to re-
duce annotation costs by proposing weakly supervised 3DIS methods based on sparse cues
[12, 27, 72]. While effective with limited annotations, these methods assume a fixed set of
classes and are susceptible to catastrophic forgetting in incremental settings. Incremental Learning.Continual learning involves training models sequentially to miti-
gate catastrophic forgetting. One common strategy is model regularisation, which constrains
parameter updates using techniques such as elastic weight consolidation or knowledge dis-
tillation [1, 24, 35, 43, 60]. Another approach is exemplar replay, where past data is stored
or generated to rehearse older tasks while learning new ones [5, 7, 30, 54]. A third direc-
tion involves dynamically expanding the model architecture or using modular sub-networks
to accommodate new tasks without interfering with old ones [31, 42, 53, 58, 71, 79]. Re-
cent efforts in class-incremental 3D segmentation [63, 74] have shown early promise but
often rely on basic architectures and focus primarily on semantic segmentation. Other works
[4, 36] explore continual learning in 3D settings, though they often depend on large memory
buffers. In contrast, our work introduces a tailored framework for 3D instance segmentation
that effectively transfers knowledge across tasks, even under class imbalance. Long-tailed Recognition.Imbalanced class distributions lead to poor recognition of rare
(long-tail) classes. To address this, re-sampling methods balance the data via input [8, 20, 50,
52, 61] or feature space [3, 14, 40], avoiding naÃ¯ve under-/over-sampling. Loss re-weighting
offers another direction, using class-based [6, 15, 23, 32, 33, 69] or per-example [45, 55, 62]
adjustments to ensure fair contribution. Parameter regularisation improves generalisation
through weight constraints [2], albeit requiring careful tuning. Other approaches leverage
transfer learning [77, 81], self-supervision [41, 75], or contrastive learning [34, 41, 82] to
improve rare-class representations. Long-tailed recognition is well-studied in 2D with large-
scale datasets [15, 49, 67]; in 3D, ScanNet200 [16, 57] enables similar efforts. Prior 3D
work focused on re-weighting, re-sampling, and transfer learning [57], while regularisation
remains underexplored. CeCo [80] balances class centres via auxiliary loss but overlooks
sampling and augmentation. Lahoud et al. [38] propose adaptive classifier regularisation
for 3D segmentation, outperforming prior approaches without threshold tuning. However,
neither method considers incremental learning. In contrast, we jointly address long-tail and
continual 3D semantic segmentation. 4THENGANE ET AL.: CLIMB-3D
Feature 
Backbone
Transformer 
Decoder
Feature 
Backbone
Transformer 
Decoder
(ð‘·,ð’€ð‘¡)
Current model (ð›·
ð‘¡
)
Pseudo Label Generator (PLG) 
Selection
à·¨ð‘Œð¾
ð‘¡
â„’CIâˆ’3DIS
à·¨ð‘Œð¾
ð‘¡,ð‘¤
à´¤ð‘Œð¾
ð‘¡
à· ð‘Œð‘¡
(ð·ð‘¡)
Current Dataset 
Exemplar 
Replay (ER)
+
~
Class-Balanced  
Re-weighting 
(CBR)
1
ð‘“(ð‘)
Figure 2: Overview ofCLIMB-3Dfor CI-3DIS. The model incrementally learns new classes
across sequential phases. During taskt, point cloudsPand their corresponding labelsY t are
sampled from a combination of the current training datasetD t and Exemplar Replay (ER),
which maintains a small memory of past examples. These are then passed to the current
modelÎ¦ t to produce predicted labelsY t . The Pseudo-Label Generator (PLG) selects the
top-Kpredictions from the previous modelÎ¦ tâˆ’1 . These pseudo-labels are then weighted
based on class frequencyf(c)using Class-Balanced Re-weighting (CBR), and the top-Kre-
weighted labels are selected to form the balanced pseudo-label set Â¯Yt . This pseudo-label set
is then concatenated with the ground-truth labels to form a final augmented supervision set
Yt for taskt, which is used to optimise the modelÎ¦ t using Eq. (3). 3 Methodology
3.1 Problem Formulation
3DIS.The objective of this task is to detect and segment each object instance within a point
cloud. Formally, the training dataset is denoted asD={(P i,Yi)}N
i=1, whereNis the number
of training samples. Each inputP i âˆˆR MÃ—6 is a coloured point cloud consisting ofMpoints,
where each point is represented by its 3D coordinates and RGB values. The corresponding
annotationY i ={(m i,j ,ci,j )}Ji
j=1 containsJ i object instances, wheremi,j âˆˆ {0,1}M is a binary
mask indicating which points belong to thej-th instance, andc i,j âˆˆ C={1,...,C}is the
semantic class label of that instance. GivenP i, the modelÎ¦predicts instance-level outputs
Ë†Yi ={(Ë†mi,j ,Ë†ci,j )}
Ë†Ji
j=1, where Ë†mi,j and Ë†ci,j denote the predicted mask and category label for
thej-th instance, respectively. The number of predicted instances Ë†Ji varies depending on the
modelâ€™s inference. The model is optimised using the following objective:
L3DIS(D;Î¦) = 1
|D| âˆ‘
(P,Y)âˆˆD
1
|Y| âˆ‘
(mj,cj)âˆˆY
(Lmask(mj,Ë†mj)+Î»Â·L cls(cj,Ë†cj)),(1)
whereL cls andL mask are the average classification and mask losses over all instances, andÎ»
controls their trade-off. CI-3DIS.Unlike conventional 3DIS, CI-3DIS involves sequential learning, where object
categories are introduced incrementally overTtraining tasks. Each tasktintroduces a dis-
joint set of classesC t , withC= ST
t=1 Ct andC t âˆ©C tâ€²
=/ 0 fortÌ¸=t â€². During each taskt, the THENGANE ET AL.: CLIMB-3D5
model receives a datasetD t ={(P i,Yt
i)}N
i=1, where each coloured point cloudP i âˆˆR MÃ—6
containsMpoints, and the corresponding annotationY t
i ={(m t
i,j ,ct
i,j )}
Jt
i
j=1 includes instance
masksm t
i,j âˆˆ {0,1}M and semantic class labelsc t
i,j âˆˆ Ct . The modelÎ¦ t , initialized from
Î¦tâˆ’1 , is trained onD t to predict instance-level outputs Ë†Yt
i ={(Ë†mt
i,j ,Ë†ct
i,j )}
Ë†Jt
i
j=1, where each
predicted semantic label Ë†ct
i,j belongs to the cumulative set of all classes observed so far:St
k=1 Ck. The key challenge in this setup is to learn new classes without forgetting those
encountered in previous tasks, despite limited supervision and the absence of past labels. 3.2 Method: CLIMB-3D
Overview.As shown in Fig. 2 and formulated in Sec. 3.1, the proposed framework for
CI-3DIS follows aphase-wisetraining strategy. In each phase, the model is exposed to a
carefully curated subset of the dataset, simulating real-world scenarios discussed in Sec. 3.3. Training naÃ¯vely on such phased data leads tocatastrophic forgetting[51], where the model
forgets prior knowledge when learning new tasks. To address this, we first incorporateEx-
emplar Replay(ER) [5], a strategy inspired by 2D incremental methods [1, 43, 54] and
recently extended to 3D settings [4], to mitigate forgetting by storing a small set of repre-
sentative samples from earlier phases. However, ER alone is insufficient to achieve robust
performance. To address this, two additional components are introduced: aPseudo-Label
Generator(PLG), which leverages a frozen model to generate supervision signals for previ-
ously seen classes, and aClass-Balanced Re-weighting(CBR) module that compensates for
class imbalance across phases. Each of these components is described in detail below 1. Exemplar Replay (ER).Inspired by Buzzega et al. [5], ER addresses the issue of catas-
trophic forgetting by allowing the model to retain a subset of data from earlier phases. During phaset, the model is trained not only on the current task dataD t , but also on a
set of exemplarsE 1:tâˆ’1, collected from previous phases. These exemplars are accumulated
incrementally:E 1:tâˆ’1 =E 1 âˆªÂ·Â·Â·âˆªE tâˆ’1 .Here,E t denotes the exemplar subset stored after
phaset, and|E t |denotes the exemplar replay size. The combined replay dataset is denoted
as:D t
ER =D t âˆªE 1:tâˆ’1.Epoch training consists of two stages: first, the model is trained on
the current data,D t ; then it is trained using exemplars fromE 1:tâˆ’1. While previous CI-3DIS approaches rely on large exemplar sets to mitigate forgetting [4],
such strategies become impractical under memory constraints. Consequently, although ER
aids knowledge retention, it is insufficient on its own for robust performance with a limited
exemplar budget. Therefore, PLG and CBR are introduced to better preserve past represen-
tations and enhance generalisation across phases. Pseudo-Label Generator (PLG).In the CI-3DIS setting, during phaset(wheret>1),
although ground-truth labels for previously seen classes are unavailable, the model from the
previous phaseÎ¦ tâˆ’1 , which preserves knowledge of past tasks, is retained. We use this
model to generate pseudo-labels for previously seen classes, thereby providing approximate
supervision during training. This allows the current modelÎ¦t , to retain prior knowledge and
reduce forgetting, even without access to ground-truth annotations. Given a point cloud and label pair(P,Y t ), fromD t , the previous modelÎ¦ tâˆ’1 , generates
pseudo-labels for the previously seen classes as Ë†Y1:tâˆ’1 =Î¦ tâˆ’1 (P), which we denote as ËœYt
1For clarity, modifications introduced by each component (ER, PLG, and CBR) are highlighted in green, blue,
and red, respectively. 6THENGANE ET AL.: CLIMB-3D
for brevity. However, we observe that utilising all predictions fromÎ¦tâˆ’1 introduces noisy or
incorrect labels, which degrade overall performance. To mitigate this, we select the top-K
most confident instance predictions, denoted as ËœYt
K, and concatenate them with the ground-
truth labels of the current task to obtain the supervision set: Â¯Yt =Y t âˆ¥ ËœYt
K. Following this,
the loss is computed over both the current taskâ€™s ground-truth labels and the top-Kconfi-
dent pseudo-labels from earlier phases, enabling the model to retain prior knowledge while
adapting to new classes. Class-Balanced Re-weighting (CBR).While ER and PLG contribute to preserving prior
knowledge, we observe that the modelâ€™s predictions are biased towards frequent classes,
resulting in the forgetting of rare categories. This issue is further amplified by the top-K
pseudo-labels selected by PLG, which predominantly represent dominant classes in the data. To address this, we propose aClass-Balanced Re-weighting(CBR) scheme that compensates
for class imbalance using object frequency statistics. At taskt, only the current datasetD t
and its label distributionp t (c)over classescâˆˆ Ct are available. Datasets and class distribu-
tions from earlier tasks{D i,p i(c)}tâˆ’1
i=1 are no longer accessible, which makes it challenging
to directly account for class imbalance across all previously seen tasks. Therefore, we pro-
pose to leverage the pseudo-label predictions of the frozen modelÎ¦ tâˆ’1 as a proxy for the
class distribution across previously learned categories. During each training iteration, the previous modelÎ¦ tâˆ’1 is applied to the current inputP
to generate pseudo-labels ËœYt , from which we accumulate class-wise frequency statistics for
previously learned classes. LetC1:t = St
i=1 Ci represent the union of all classes seen up to task
t. At the end of each epoch, we compute the overall class frequencyfâˆˆR |C1:t | by combining
the pseudo-label distribution Ëœpt (c)derived from ËœYt with the ground-truth label distribution
pt (c)from the current task. The resulting frequency vectorf= [f(c)] câˆˆC1:t is then used to
compute class-wise weightsw= [w(c)] câˆˆC1:t , which guide balanced pseudo-label selection
and help the model focus on rare classes. In the next epoch, the modelÎ¦ tâˆ’1 produces soft pseudo-label predictions ËœYt (as de-
scribed in PLG) , where each component ËœYt [c]denotes the confidence score for classcâˆˆ
C1:tâˆ’1. To promote balanced pseudo-label selection, class-wise re-weighting is applied using
w(c) = 1
f(c)+Îµ , whereÎµis a small constant for numerical stability. The adjusted predictions
are computed by applying the class-wise weights as: ËœYt,w [c] =w(c)Â· ËœYt [c],âˆ€câˆˆ C 1:tâˆ’1. Top-Kselection is then applied to both the original and the re-weighted scores, yielding
two pseudo-label sets: ËœYt
K and ËœYt,w
K , respectively. The final augmented target set, Yt , is
constructed by concatenating the ground-truth labelsY t with both sets of pseudo-labels:
Yt =Y t âˆ¥ ËœYt
K âˆ¥ ËœYt,w
K .(2)
Although the above re-weighting mitigates bias from prior tasks, it does not address class
imbalance within the current taskt. To resolve this, the class-balanced weightsw c are ex-
tended to incorporate statistics from both previously seen and current classes, i.e., overC 1:t
rather than onlyC 1:tâˆ’1. The updated weights, denoted asw â€²
c, are used to re-weights both
pseudo-label selection and classification loss, ensuring balanced supervision across all seen
categories, including rare ones. This unified strategy reduces bias and enhances performance
across phases. The pseudo-code for constructing the supervision set in Eq. (2), which incor-
porates both PLG and CBR components, is provided in Appendix A. Final Objective.The final training objective ofCLIMB-3D, incorporating ER, PLG, and THENGANE ET AL.: CLIMB-3D7
CBR, is formulated in Eq. (1) as:
L(Dt
ER;Î¦t ) = 1
|Dt
ER| âˆ‘
(P,Yt )âˆˆDt
ER
1
|Yt | âˆ‘
(mt
j,ct
j)âˆˆYt
 
Lmask(mt
j,Ë†mt
j)+w â€²â€²
c Â·Lcls(ct
j,Ë†ct
j)

,(3)
wherew â€²â€²
c =w â€²
c Â·Î» cls denotes the scaled weight vector, andÎ» cls is a hyperparameter for bal-
ancing the loss terms. 3.3 Benchmarking Incremental Scenarios
While CI-3DIS methods offer a wide range of practical applications, they frequently rely
on the assumption of uniform sample distribution, which rarely holds in real-world settings. In practice, the number of object categories, denoted byC, is often large and characterised
by substantial variability in category frequency, shape, structure, and size. To address these
challenges, we propose three incremental learning scenarios, each designed to capture a
different facet of real-world complexity. The design of these scenarios is detailed below; for
further information and illustrations, refer to Appendix E of the supplementary material. Split-A: Frequency Scenarios.This scenario acknowledges that datasets are often la-
belled based on the frequency of categories. To accommodate this, we propose a split where
the model learns from the most frequent categories and subsequently incorporates the less
frequent ones in later stages. By prioritising the training of frequently occurring categories,
the model can establish a strong foundation before expanding to handle rare categories. Split-B: Semantic Scenarios.Beyond frequency, semantic similarity is crucial in real-
world deployments. While objects often share visual or functional traits, models may en-
counter semantically different categories in new environments. To simulate this challenge,
we introduce theSplit-Bscenario. Here, categories are grouped based on their semantic
relationships, and the model is incrementally trained on one semantic group at a time. This
setup encourages the model to generalise across semantically similar categories and adapt
more effectively when exposed to new ones. Unlike theSplit-Ascenario, which organises
learning based on category frequency, theSplit-Bscenario may contain both frequent and
infrequent categories within a single task, focusing on semantic continuity and transfer. Split-C: Random Scenarios.In some cases, data labelling is driven by the availability of
objects rather than predefined criteria. To capture this, we introduce theSplit-Cscenario,
which represents a fully random setting where any class can appear in any task, resulting in
varying levels of class imbalance. By exposing the model to such diverse and unpredictable
distributions, we aim to improve its robustness in real-world situations where labelled data
availability is inconsistent. These incremental scenarios are designed to provide a more realistic representation of
object distributions, frequencies, and dynamics encountered in the real world. 4 Experiments
4.1 Setup
Datasets.We evaluateCLIMB-3Don ScanNet200 [57], which comprises 200 object cat-
egories and exhibits significant class imbalance, which makes it ideal for simulating and 8THENGANE ET AL.: CLIMB-3D
assessing real-world scenarios. In addition, we compare our approach against existing in-
cremental learning methods using ScanNetV2 [16] in the 3DSS setting. For this evaluation,
we follow the standard training and validation splits defined in prior works [74] to ensure
consistency with existing methods. Evaluation Metrics.We evaluate our method usingMean Average Precision(mAP), a
standard metric for 3DIS which provides a comprehensive measure of segmentation quality
by accounting for both precision and recall. For comparison with existing 3DSS methods,
we report themean Intersection over Union(mIoU), which quantifies the overlap between
predicted and ground truth segments. To assess the modelâ€™s ability to mitigate catastrophic
forgetting in incremental settings, we use theForgetting Percentage Points(FPP) metric, as
defined in [47]. FPP captures performance degradation by measuring the accuracy drop on
the initially seen categories between the first and final training phases. Detailed descriptions of the incremental scenarios (Split-A,Split-B,Split-C) and im-
plementation are provided in Appendix B. 4.2 Results and Discussion
Table 1: Comparison between the proposed method and the baseline in the 3DIS setting,
evaluated using mAP25, mAP50, mAP, and FPP after training across all phase. Scenarios Methods Average Precisionâ†‘FPPâ†“
mAP25 mAP50 mAP mAP 25 mAP50
Baseline 16.46 14.29 10.44 51.30 46.82Split-A CLIMB-3D(Ours) 35.69 31.05 22.72 3.44 2.63
Baseline 17.22 15.07 10.93 46.27 42.1Split-B CLIMB-3D(Ours) 35.48 31.56 23.69 8.00 5.51
Baseline 25.65 21.08 14.85 31.68 28.84Split-C CLIMB-3D(Ours) 31.59 26.78 18.93 9.10 7.89
To evaluate our proposed approach, we construct a baseline using ER [5] for the 3DIS
setting, as no existing incremental baselines are available for this task. In contrast, for the
3DSS setting, where incremental baselines do exist, we compare our method against three
prior approaches [43, 60, 74]. Results on CI-3DIS.Tab. 1 compares CLIMB-3D against the ER baseline across the three
CI-3DIS scenarios, evaluated after all phases using mAP 25, mAP50, overall mAP, and FPP. In theSplit-Ascenario, characterised by significant distribution shifts, our method achieves
gains of +19.23%, +16.76%, and +12.28% in mAP 25, mAP 50, and overall mAP, respec-
tively, while drastically reducing forgetting with FPP scores of 3.44% (mAP 25) and 2.63%
(mAP50), both over 45 points lower than the baseline. Under theSplit-Bscenario, with
semantically related new categories, improvements of +18.26%, +16.49%, and +12.76%
are observed alongside a reduction in forgetting to 8.00% and 5.51% from baseline levels
above 42%. For theSplit-Cscenario, featuring increasing geometric complexity, CLIMB-
3D maintains solid gains of +5.94%, +5.70%, and +4.08% across mAP metrics, with forget-
ting reduced by over 22 points. Overall, these results demonstrate robust forward transfer,
minimal forgetting, and stable learning, highlighting the methodâ€™s effectiveness for scalable
continual 3D semantic understanding. THENGANE ET AL.: CLIMB-3D9
Table 2: Comparison between the proposed method and existing baselines in the 3DSS set-
ting on ScanNetV2 [16], evaluated using mIoU. Methods Phase=1 Phase=2 All
EWC [60] 17.75 13.22 16.62
LwF [43] 30.38 13.37 26.13
Yanget al. [74] 34.16 13.43 28.98
CLIMB-3D(Ours) 69.39 32.56 59.38
Results on CI-3DSS.Although our primary focus is on CI-3DIS, we also evaluate our
method under the CI-3DSS setting to enable comparisons with existing approaches. To do
so, we adapt our predictions to assign each point the label corresponding to the highest-
confidence mask and exclude background classes (e.g., floor and wall), as these are not part
of the semantic segmentation targets. Tab. 2 presents a comparison between our method and
existing baselines on the ScanNet V2 dataset [16], evaluated using mIoU across two training
phases and overall. Although originally designed for instance segmentation, our method
generalises effectively to semantic segmentation, achieving substantial gains of +35.23%
mIoU in Phase 1 and +19.1% in Phase 2. Overall, it achieves 59.38% mIoU, significantly
higher than theâˆ¼30% mIoU of prior methods, highlighting its robustness and transferability. Refer to Appendix C, D, and F of the supplementary material for detailed analyses of per-
phase performance, rare-class evaluation, and qualitative result comparisons, respectively. 4.3 Ablation
Table 3: Ablation study illustrating the impact of each component in a three-phase setup. Each split (s) and corresponding number indicates data introduced at that phase. Best results
are highlighted inbold. Row Modules p=1â†‘p=2â†‘p=3â†‘ FPPâ†“
s1 s1 s2 All s1 s2 s3 All
1. Oracle - - - - 55.14 30.77 25.30 37.68 -
2. NaÃ¯ve 56.82 0.00 28.09 14.15 0.00 0.00 19.67 5.80 56.82
3. + ER 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28 46.44
4. + PLG 56.82 50.0034.3942.13 49.78 11.41 26.47 29.28 7.04
5. + CBR 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05 2.63
We conduct an ablation study to evaluate the contribution of each component in our
framework. As an upper bound, we report the performance of anOraclemodel, which is
trained jointly on the full dataset. For the incremental setting, we follow the previously
defined splits and first train the model naively across phases. We then incrementally add
each module to isolate its impact. Tab. 3 presents results for theSplit-Ascenario using both
mAP50 and FPP metrics. NaÃ¯ve Training.When trained naÃ¯vely without any dedicated modules, the model suffers
from severe catastrophic forgetting, as evident in row 2. The model entirely forgets pre-
viously learned classes upon entering new phases, with performance dropping to zero for
earlier splits and FPP rising to 56.82. 10THENGANE ET AL.: CLIMB-3D
Effect of ER.Adding exemplar replay (row 3) partially alleviates forgetting by maintaining
a buffer of past examples. This yields notable gains fors1in Phase 2 (+18.51%) and Phase 3
(+10.38%), and also improvess2in Phase 3 (+9.43%). However, the overall forgetting
remains substantial (FPP: 46.44), showing ER alone is insufficient. Effect of PLG.The addition of the pseudo-label generator (PLG), which generates labels for
previous classes by retaining a copy of the model from an earlier phase, facilitates knowledge
retention and forward transfer. As shown in row 4, PLG significantly reduces forgetting and
enhances performance on current tasks. Fors1, it improves mAP 50 by 31.49% in Phase 2
and 39.40% in Phase 3 over exemplar replay. Overall, PLG yields a 15.00% increase in
performance and reduces forgetting by 39.40%. Effect of CBR.Finally, class-balanced re-weighting (CBR) mitigates class imbalance dur-
ing both pseudo-labelling and current task learning by adjusting each classâ€™s contribution
based on its frequency. As shown in row 5, CBR enhancess1retention over PLG in Phases 2
and 3 (+4.67% and +4.41%), and further improves performance ons2ands3. It also achieves
the lowest FPP of 2.63, reflecting strong forgetting mitigation. Overall, CBR offers the best
trade-off between retaining past knowledge and acquiring new tasks, outperforming PLG
and ER by 4.41% and 43.81%, respectively. 4.4 Limitations & Future Work
While CLIMB-3D achieves strong performance across CI-3DIS and CI-3DSS settings, sev-
eral limitations remain and warrant discussion. The experiments are currently limited to
indoor datasets (ScanNet200 and ScanNetV2), therefore evaluating performance on outdoor
scenes would help assess generalisability across diverse environments. The current setup
also considers only three incremental phases, whereas real-world applications often involve
longer sequences; incorporating such extended setups would better reflect practical chal-
lenges. Moreover, the model operates in a uni-modal setting. Integrating multi-modal cues,
such as vision-language models, could further enhance performance, as suggested by recent
2D and 3D studies [25, 65, 66]. 5 Conclusion
This work addresses the challenge of catastrophic forgetting in class-incremental 3D in-
stance segmentation by introducing a modular framework that integrates exemplar replay
(ER), a pseudo-label generator (PLG), and class-balanced re-weighting (CBR). The pro-
posed method incrementally adapts to new classes while preserving prior knowledge, all
without requiring access to the full dataset. Extensive experiments on a three-phase bench-
mark validate the individual and combined effectiveness of each component: ER enables
efficient memory-based retention; PLG facilitates knowledge preservation through pseudo-
supervision; and CBR mitigates class imbalance to enhance learning stability. Together,
these components significantly reduce forgetting and improve segmentation performance
across all phases. By achieving a strong balance between stability and plasticity, our ap-
proach advances continual 3D scene understanding and sets a new baseline for future re-
search in this area. THENGANE ET AL.: CLIMB-3D11
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne
Tuytelaars. Memory aware synapses: Learning what (not) to forget. InECCV, pages
139â€“154, 2018. [2] Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long-tailed
recognition via weight balancing. InCVPR, pages 6897â€“6907, 2022. [3] Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying
imbalanced data. InJoint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 770â€“785.

The ER+KD module does not focus on rare classes like shower and toaster which results in low performance, but the IC module compensates for this imbalance by focusing on underrepresented categories.

1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab."
How does the IC module impact the performance of classes observed 1-20 times per epoch in Phase 2 of Split-A?,"As illustrated in Tab. 5, the IC module substantially improves performance on rare classes in terms of mAP 50 in Phase 2 of Split-A. For instance, classes like recycling bin and trash bin, seen only 3 and 7 times, respectively, shows significant improvement when the IC module is applied.","As illustrated in Tab. 5, the IC module substantially improves performance on rare classes in terms of mAP 50 in Phase 2 of Split-A. For instance, classes like recycling bin and trash bin, seen only 3 and 7 times, respectively, shows significant improvement when the IC module is applied.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks."
What is the primary focus of the IC module in mitigating class imbalance?,"Overall, the IC module provides an average boost of 8.32%, highlighting its effectiveness in mitigating class imbalance.","1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

Springer, 2020. 12THENGANE ET AL.: CLIMB-3D
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced
loss based on effective number of samples. InCVPR, pages 9268â€“9277, 2019.

Overall, the IC module provides an average boost of 8.32%, highlighting its effectiveness in mitigating class imbalance."
Which module outperforms ER+KD by 12.13% on average when integrated into the framework?,"On average, the addition of the proposed IC module into the framework outperforms ER+KD by 12.13%.","5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks.

On average, the addition of the proposed IC module into the framework outperforms ER+KD by 12.13%.

1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig."
What is highlighted in Fig. 6 for Split-C?,"Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in random order.","Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in random order.

1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig."
How does the proposed method perform compared to the baseline in row 5?,"Similarly, in row 5, the washing machine is segmented into two instances by the baseline.","Similarly, in row 5, the washing machine is segmented into two instances by the baseline.

[78] Biao Zhang and Peter Wonka. Point cloud instance segmentation using probabilistic
embeddings. InCVPR, pages 8883â€“8892, 2021. [79] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsuper-
vised lifelong learning.Neural Networks, 149:95â€“106, 2022. [80] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang,
and Jiaya Jia. Understanding imbalanced semantic segmentation through neural col-
lapse. InCVPR, pages 19550â€“19560, 2023. [81] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch
network with cumulative learning for long-tailed visual recognition. InCVPR, pages
9719â€“9728, 2020. [82] Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang
Jiang. Balanced contrastive learning for long-tailed visual recognition. InCVPR, pages
6908â€“6917, 2022. THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced 3D Instance
Segmentation
Supplementary Material
Appendix A Illustrating Incremental scenarios
Frequent Scenario Semantic Scenario
background
frequent common
rare semantic 
cluster 1
semantic 
cluster 2
semantic 
cluster 3
background
random 
cluster 1
random 
cluster 2
random 
cluster 3
background
Random Scenario
Figure 3: Tasks are grouped into incremental scenarios based on object frequency, semantic
similarity, and random assignment. , , and denote different tasks; shapes indicate
object categories; marks the background.Left:Grouped by category frequency.Middle:
Grouped by semantic similarity (e.g., similar shapes).Right:Randomly grouped, mixing
semantic and frequency variations. Appendix B Per Phase Analysis
We extend the analysis from Tab.

5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig."
"What is the key difference between Split-A, Split-B, and Split-C in terms of tasks?","The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks: Task 1, Task 2, and Task 3.","5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks.

[78] Biao Zhang and Peter Wonka. Point cloud instance segmentation using probabilistic
embeddings. InCVPR, pages 8883â€“8892, 2021. [79] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsuper-
vised lifelong learning.Neural Networks, 149:95â€“106, 2022. [80] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang,
and Jiaya Jia. Understanding imbalanced semantic segmentation through neural col-
lapse. InCVPR, pages 19550â€“19560, 2023. [81] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch
network with cumulative learning for long-tailed visual recognition. InCVPR, pages
9719â€“9728, 2020. [82] Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang
Jiang. Balanced contrastive learning for long-tailed visual recognition. InCVPR, pages
6908â€“6917, 2022. THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced 3D Instance
Segmentation
Supplementary Material
Appendix A Illustrating Incremental scenarios
Frequent Scenario Semantic Scenario
background
frequent common
rare semantic 
cluster 1
semantic 
cluster 2
semantic 
cluster 3
background
random 
cluster 1
random 
cluster 2
random 
cluster 3
background
Random Scenario
Figure 3: Tasks are grouped into incremental scenarios based on object frequency, semantic
similarity, and random assignment. , , and denote different tasks; shapes indicate
object categories; marks the background.Left:Grouped by category frequency.Middle:
Grouped by semantic similarity (e.g., similar shapes).Right:Randomly grouped, mixing
semantic and frequency variations. Appendix B Per Phase Analysis
We extend the analysis from Tab.

The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks: Task 1, Task 2, and Task 3.

1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab."
How does the proposed method handle instances that the baseline method underperforms on?,"highlighting instances where the baseline method underperforms, marked with red circles.","1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

highlighting instances where the baseline method underperforms, marked with red circles.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks."
What is the significance of the red circles in the evaluation of Split-B?,"Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed framework on theSplit-Bevaluation after learning all tasks.","5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed framework on theSplit-Bevaluation after learning all tasks.

1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab."
How are the classes grouped in Table 7 for each proposed scenario on the ScanNet200 dataset?,Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset,"5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks.

Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

[78] Biao Zhang and Peter Wonka. Point cloud instance segmentation using probabilistic
embeddings. InCVPR, pages 8883â€“8892, 2021. [79] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsuper-
vised lifelong learning.Neural Networks, 149:95â€“106, 2022. [80] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang,
and Jiaya Jia. Understanding imbalanced semantic segmentation through neural col-
lapse. InCVPR, pages 19550â€“19560, 2023. [81] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch
network with cumulative learning for long-tailed visual recognition. InCVPR, pages
9719â€“9728, 2020. [82] Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang
Jiang. Balanced contrastive learning for long-tailed visual recognition. InCVPR, pages
6908â€“6917, 2022. THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced 3D Instance
Segmentation
Supplementary Material
Appendix A Illustrating Incremental scenarios
Frequent Scenario Semantic Scenario
background
frequent common
rare semantic 
cluster 1
semantic 
cluster 2
semantic 
cluster 3
background
random 
cluster 1
random 
cluster 2
random 
cluster 3
background
Random Scenario
Figure 3: Tasks are grouped into incremental scenarios based on object frequency, semantic
similarity, and random assignment. , , and denote different tasks; shapes indicate
object categories; marks the background.Left:Grouped by category frequency.Middle:
Grouped by semantic similarity (e.g., similar shapes).Right:Randomly grouped, mixing
semantic and frequency variations. Appendix B Per Phase Analysis
We extend the analysis from Tab."
What does the baseline method often miss or incorrectly segment in its results?,The baseline method often misses instances or splits a single instance into multiple parts.,"5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks.

The baseline method often misses instances or splits a single instance into multiple parts.

[78] Biao Zhang and Peter Wonka. Point cloud instance segmentation using probabilistic
embeddings. InCVPR, pages 8883â€“8892, 2021. [79] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsuper-
vised lifelong learning.Neural Networks, 149:95â€“106, 2022. [80] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang,
and Jiaya Jia. Understanding imbalanced semantic segmentation through neural col-
lapse. InCVPR, pages 19550â€“19560, 2023. [81] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch
network with cumulative learning for long-tailed visual recognition. InCVPR, pages
9719â€“9728, 2020. [82] Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang
Jiang. Balanced contrastive learning for long-tailed visual recognition. InCVPR, pages
6908â€“6917, 2022. THENGANE ET AL.: CLIMB-3D1
CLIMB-3D: Class-Incremental Imbalanced 3D Instance
Segmentation
Supplementary Material
Appendix A Illustrating Incremental scenarios
Frequent Scenario Semantic Scenario
background
frequent common
rare semantic 
cluster 1
semantic 
cluster 2
semantic 
cluster 3
background
random 
cluster 1
random 
cluster 2
random 
cluster 3
background
Random Scenario
Figure 3: Tasks are grouped into incremental scenarios based on object frequency, semantic
similarity, and random assignment. , , and denote different tasks; shapes indicate
object categories; marks the background.Left:Grouped by category frequency.Middle:
Grouped by semantic similarity (e.g., similar shapes).Right:Randomly grouped, mixing
semantic and frequency variations. Appendix B Per Phase Analysis
We extend the analysis from Tab."
How does the proposed method's performance compare to the baseline in the evaluation of Split-A?,"Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed framework on theSplit-Aevaluation after learning all tasks.","1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed framework on theSplit-Aevaluation after learning all tasks.

5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig."
What is the main focus of the results presented on Split-B in comparison to the baseline method?,"Similarly, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5","5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

Similarly, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5

5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks.

PMLR,
2021. THENGANE ET AL.: CLIMB-3D13
[29] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. InCVPR, pages
4867â€“4876, 2020."
What does the proposed method's performance demonstrate in relation to the ground truth in the evaluation of Split-C?,"Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed framework on theSplit-Cevaluation after learning all tasks.","1 to Tab. 4 to highlight the impact of our proposed method
on individual splits across various scenarios. The results clearly demonstrate that our model
consistently retains knowledge of previous tasks better than the baseline. ForSplit-A, our
model shows improvement throughout the phase. In Phase 3 of (s2), although both the base-
line and our method exhibit a performance drop, our method reduces forgetting significantly
compared to the baseline. TheSplit-Bscenario, while more complex thanSplit-A, achieves
comparable results due to semantic similarity among classes within the same task. In Phase
2, our model achieves overall all 43.13% mAP50 compared to 24.53% on baseline, a similar
trend is observed in Phase 3, where our method not only consistently improves learning but
also enhances retention of previous information. After all three tasks, our method achieves
an overall performance of 31.56% AP50, compared to 15.07% for the baseline. In theSplit-
Cscenario, the first-stage model struggles due to the increased complexity introduced by
random grouping. In Phase 2, while the baseline focuses on learning the current task, it suf-
fers from severe forgetting of prior knowledge. Conversely, our method balances new task
learning with the retention of earlier information. By Phase 3, the model effectively con- 2THENGANE ET AL.: CLIMB-3D
Table 4: Comparison of results in terms of mAP50 with the proposed CLIMB-3D across three
different scenarios. Each scenario is trained over three phases (phase=1,2,3), introducing
a single splitsat a time. Results highlighted in orange correspond to the proposed method,
and the best results for each scenario are shown inbold. Scenarios Methods phase=1 phase=2 phase=3
s1 s1 s2 All s1 s2 s3 All
Baseline 56.82 18.51 32.81 25.72 10.38 9.43 24.27 14.28Split-A CLIMB-3D 56.82 54.67 33.75 44.13 54.19 12.02 26.55 31.05
Baseline 51.57 13.3242.2124.53 9.55 12.4526.7815.07Split-B CLIMB-3D 51.57 46.74 37.45 43.13 46.06 15.95 26.68 31.56
Baseline 36.40 7.7437.6222.32 7.55 15.9640.4121.08Split-C CLIMB-3D 36.40 32.63 33.38 33.00 28.51 17.11 34.64 26.78
solidatess1and maintains strong performance across all task splits. Overall, our proposed
method improves mAP by 5.6%. In this supplementary material, we first demonstrate the performance gains on rare classes
achieved by incorporating the IC module in Appendix C. Next, we provide detailed split in-
formation for all scenarios, based on class names, in Appendix D. Finally, we present a qual-
itative comparison between the baseline method and our proposed approach in Appendix
E. Appendix C Evaluation on Rare Categories
The proposed imbalance correction (IC) module, as detailed in Section 4.2, is designed to
address the performance gap for rare classes. To assess its impact, we compare its perfor-
mance with the framework which has exemplar replay (ER) and knowledge distillation (KD). Specifically, we focus on its ability to improve performance for rare classes, which the model
encounters infrequently compared to more common classes. The results, shown in Tab.

5 and Tab. 6, correspond to evaluations onSplit-AforPhase
2andPhase 3, respectively. InPhase 2, we evaluate classes seen 1â€“20 times per epoch,
whilePhase 3targets even less frequent classes, with observations limited to 1â€“10 times per
epoch. As illustrated in Tab. 5, the IC module substantially improves performance on rare
classes in terms of mAP 50 in Phase 2 ofSplit-A. For instance, classes likerecycling
binandtrash bin, seen only 3 and 7 times, respectively, shows significant improvement
when the IC module is applied. Overall, the IC module provides an average boost of 8.32%,
highlighting its effectiveness in mitigating class imbalance. Similarly, Tab. 6 presents results forPhase 3, demonstrating significant gains for infre-
quent classes. For example, even though the classes such aspiano,bucket, andlaundry
basketare observed only once, IC module improves the performance by 52.30%, 10.40%,
and 13.60%, respectively. The ER+KD module does not focus on rare classes likeshower
andtoasterwhich results in low performance, but the IC module compensates for this imbal-
ance by focusing on underrepresented categories. On average, the addition of the proposed THENGANE ET AL.: CLIMB-3D3
Table 5: Results for classes observed by the model 1â€“20 times during an epoch, evaluated
onSplit-Afor Phase 2, in terms of mAP 50. Classes Seen Count ER + PLG ER + PLG + CBR
paper towel dispenser 2 73.10 74.90
recycling bin 3 55.80 60.50
ladder 5 53.90 57.10
trash bin 7 31.50 57.30
bulletin board 8 23.30 38.20
shelf 11 48.00 50.50
dresser 12 44.00 55.80
copier 12 93.30 94.50
object 12 3.10 3.30
stairs 13 51.70 67.70
bathtub 16 80.30 86.60
oven 16 1.50 3.30
divider 18 36.40 45.00
column 20 57.30 75.00
Average - 46.66 54.98
IC module into the framework outperforms ER+KD by 12.13%. Appendix D Incremental Scenarios Phases
Tab. 7 presents the task splits for each proposed scenario introduced in Section 4.3 using the
ScanNet200 dataset. The three scenarios,Split-A,Split-B, andSplit-C, are each divided
into three tasks: Task 1, Task 2, and Task 3. Notably, the order of classes in these tasks is
random. Appendix E Qualitative Results
In this section, we present a qualitative comparison of the proposed framework with the
baseline method. Fig. 4 illustrates the results on theSplit-Aevaluation after learning all
tasks, comparing the performance of the baseline method and our proposed approach. As
shown in the figure, our method demonstrates superior instance segmentation performance
compared to the baseline. For example, in row 1, the baseline method fails to segment
thesink, while in row 3, thesofainstance is missed. Overall, our framework consistently
outperforms the baseline, with several missed instances by the baseline highlighted in red 4THENGANE ET AL.: CLIMB-3D
Table 6: Results for classes observed by the model 1â€“10 times during an epoch, evaluated
onSplit-Afor Phase 3, in terms of mAP 50. Classes Seen Count ER+KD ER+KD+IC
piano 1 7.10 59.40
bucket 1 21.10 31.50
laundry basket 1 3.80 17.40
dresser 2 55.00 55.40
paper towel dispenser 2 32.50 35.50
cup 2 24.70 30.30
bar 2 35.40 39.50
divider 2 28.60 42.40
case of water bottles 2 0.00 1.70
shower 3 0.00 45.50
mirror 8 56.00 68.80
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
bathroom counter 3 3.90 20.30
ottoman 4 32.60 36.20
storage bin 3 5.10 10.50
dishwasher 3 47.40 66.20
trash bin 4 1.10 2.70
backpack 5 74.50 76.70
copier 5 94.00 96.80
sofa chair 6 14.10 43.50
file cabinet 6 49.20 57.60
tv stand 7 67.70 68.60
mirror 8 56.00 68.80
blackboard 8 57.10 82.80
clothes dryer 9 1.70 3.20
toaster 9 0.10 25.90
wardrobe 10 22.80 58.80
jacket 10 1.20 4.10
Average - 32.08 44.21
circles. In Fig.

Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed framework on theSplit-Cevaluation after learning all tasks.

5, we present the results onSplit-B, highlighting instances where the baseline
method underperforms, marked with red circles. For example, in row 2, the baseline method THENGANE ET AL.: CLIMB-3D5
Table 7: Classes grouped by tasks for each proposed scenario on the ScanNet200 dataset
labels. The three scenariosSplit-A,Split-A, andSplit-Care each divided into three tasks:
Task 1, Task 2, and Task 3. Split_A Split_B Split_CTask 1 Task 2 Task 3 Task 1 Task 2 Task 3 Task 1 Task 2 Task 3chair wall pillow tv stand cushion paper broom fan racktable floor picture curtain end table plate towel stove music standcouch door book blinds dining table soap dispenserfireplace tv beddesk cabinet box shower curtain keyboard bucketblanket dustpan soap dishoffice chair shelf lamp bookshelf bag clock dining table sink closet doorbed window towel tv toilet paper guitar shelf toaster basketsink bookshelf clothes kitchen cabinet printer toilet paper holderrail doorframe chairtoilet curtain cushion pillow blanket speaker bathroom counter wall toilet papermonitor kitchen cabinet plant lamp microwave cup plunger mattress ballarmchair counter bag dresser shoe paper towel roll bin stand monitorcoffee table ceiling backpack monitor computer tower bar armchair copier bathroom cabinetrefrigerator whiteboard toilet paperobject bottle toaster trash bin ironing board shoetv shower curtain blanket ceiling bin ironing board dishwasher radiator blackboardnightstand closet shoe board ottoman soap dish lamp keyboard ventdresser computer tower bottle stove bench toilet paper dispenserprojector toaster oven bagstool board basket closet wall basket fire extinguisherpotted plant paper bag paperbathtub mirror fan couch fan ball coat rack structure projector screenend table shower paper office chair laptop hat end table picture pillardining table blinds person kitchen counter person shower curtain rodtissue box purse range hoodkeyboard rack plate shower paper towel dispenser paper cutterstairs tray coffee makerprinter blackboard container closet oven tray fire extinguisher couch handicap bartv stand rail soap dispenser doorframe rack toaster oven case of water bottles telephone pillowtrash can radiator telephone sofa chair piano mouse water bottle shower curtain rod decorationstairs wardrobe bucket mailbox suitcase toilet seat cover dispenserledge trash can printermicrowave column clock nightstand rail storage containershower head closet wall objectstove ladder stand washing machine container scaleguitar case cart mirrorbin bathroom stall light picture telephone tissue box kitchen cabinet hat ottomanottoman shower wall pipe book stand light switch poster paper cutter water pitcherbench mat guitar sink light crate candle storage organizer refrigeratorwashing machine windowsill toilet paper holderrecycling bin laundry basket power outletbowl vacuum cleaner dividercopier bulletin board speaker table pipe sign plate mouse toiletsofa chair doorframe bicycle backpack seat projector person paper towel roll washing machinefile cabinet shower curtain rod cupshower wall column candle storage bin laundry detergent matlaptop paper cutter jacket toilet bicycle plunger microwave calendar scalepaper towel dispenser shower door paper towel rollcopier ladder stuffed animal office chair wardrobe dresseroven pillar machine counter jacket headphones clothes dryer whiteboard bookshelfpiano ledge soap dish stool storage bin broom headphones laundry basket tv standsuitcase light switch fire extinguisherrefrigerator coffee maker guitar casetoilet seat cover dispenser shower door closet rodrecycling bin closet door ball window dishwasher dustpan bathroom stall door curtain plantlaundry basket shower floor hatfile cabinet machine hair dryerspeaker folded chair counterclothes dryer projector screen water coolerchair mat water bottle keyboard piano suitcase benchseat divider mouse wall windowsill handicap bar cushion hair dryer ceilingstorage bin closet wall scale plant bulletin board purse table mini fridge pianocoffee maker bathroom stall door power outletcoffee table fireplace vent nightstand dumbbell closetdishwasher stair rail decorationstairs mini fridge shower floorbathroom vanity oven cabinetbar bathroom cabinet sign armchair water cooler water pitcherlaptop luggage cuptoaster closet rod projector cabinet shower door bowl shower wall bar laundry hamperironing board structure vacuum cleanerbathroom vanity pillar paper bagdesk pipe light switchfireplace coat rack candle bathroom stall ledge alarm clockcomputer tower bathroom stall cd casekitchen counter storage organizer plungermirror furniture music stand soap dispenser blinds backpacktoilet paper dispenser stuffed animalblackboard cart laundry detergentcontainer toilet paper dispenser windowsillmini fridge headphones trash can decoration dumbbellbicycle coffee table boxtray broom stair rail closet door tube light dish rack booktoaster oven guitar case box vacuum cleaner cd case clothes guitar mailboxtoilet seat cover dispenser hair dryertowel dish rack closet rod machine seat sofa chairfurniture water bottle door range hood coffee kettle furniture clock shower curtaincart purse clothes projector screen shower headstair rail alarm clock bulletin boardstorage container vent whiteboard divider keyboard pianotoilet paper holder board cratetissue box water pitcher bed bathroom counter case of water bottlesfloor file cabinet tubecrate bowl floor laundry hamper coat rackbucket ceiling light windowdish rack paper bag bathtub bathroom stall door folded chairstool ladder power outletrange hood alarm clock desk ceiling light fire alarm door paper towel dispenser power stripdustpan laundry detergent wardrobe trash bin power stripsign shower floor bathtubhandicap bar object clothes dryer bathroom cabinet calendarrecycling bin stuffed animal columnmailbox ceiling light radiator structure poster shower water cooler fire alarmmusic stand dumbbell shelf storage organizer luggagejacket coffee kettle storage containerbathroom counter tube potted plant bottle kitchen counterbathroom vanity cd case mattresslaundry hamper coffee kettletrash bin shower headkeyboard piano case of water bottlesfolded chair fire alarmluggage power stripmattress calendar posterpotted plant
incorrectly identifies the same sofa as separate instances. Similarly, in row 5, the washing
machine is segmented into two instances by the baseline. In contrast, the proposed method
delivers results that closely align with the ground truth, demonstrating its superior perfor-
mance
Similarly, Fig. 6 highlights the results onSplit-C, where classes are encountered in
random order. The comparison emphasizes the advantages of our method, as highlighted
by red circles. The baseline method often misses instances or splits a single instance into 6THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 4: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Aevaluation after learning all tasks. multiple parts. In contrast, our approach consistently produces results that are closely aligned
with the ground truth, further underscoring its effectiveness. THENGANE ET AL.: CLIMB-3D7
Ground Truth Baseline Ours
Figure 5: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Bevaluation after learning all tasks. 8THENGANE ET AL.: CLIMB-3D
Ground Truth Baseline Ours
Figure 6: Qualitative comparison of ground truth, the baseline method, and our proposed
framework on theSplit-Cevaluation after learning all tasks."
