{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f4cd78",
   "metadata": {},
   "source": [
    "Federated Learning Based LLM Fine-Tuning of LLAMA 3.2-1bn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install flwr==1.18.0\n",
    "#%pip install flwr_datasets==0.5.0\n",
    "#%pip install hydra-core==1.3.2\n",
    "#%pip install -U \"flwr[simulation]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f4707ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Scipy Sparse and Sklearn are both loaded.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Ensure we aren't accidentally pulling from Roaming\n",
    "sys.path = [p for p in sys.path if \"Roaming\" not in p]\n",
    "\n",
    "try:\n",
    "    from scipy import sparse\n",
    "    import sklearn\n",
    "    print(\"Success! Scipy Sparse and Sklearn are both loaded.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Still failing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da248986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import unsloth\n",
    "import json\n",
    "from datasets import Dataset\n",
    "import sys\n",
    "#from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer, SFTConfig #DataCollatorForCompletionOnlyLM\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76b26411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2391777/647896753.py:32: DeprecationWarning: Importing Faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import Faithfulness\n",
      "  from ragas.metrics import Faithfulness, AnswerRelevancy, AnswerCorrectness, ContextUtilization, ContextPrecision\n",
      "/tmp/ipykernel_2391777/647896753.py:32: DeprecationWarning: Importing AnswerRelevancy from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import AnswerRelevancy\n",
      "  from ragas.metrics import Faithfulness, AnswerRelevancy, AnswerCorrectness, ContextUtilization, ContextPrecision\n",
      "/tmp/ipykernel_2391777/647896753.py:32: DeprecationWarning: Importing AnswerCorrectness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import AnswerCorrectness\n",
      "  from ragas.metrics import Faithfulness, AnswerRelevancy, AnswerCorrectness, ContextUtilization, ContextPrecision\n",
      "/tmp/ipykernel_2391777/647896753.py:32: DeprecationWarning: Importing ContextUtilization from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import ContextUtilization\n",
      "  from ragas.metrics import Faithfulness, AnswerRelevancy, AnswerCorrectness, ContextUtilization, ContextPrecision\n",
      "/tmp/ipykernel_2391777/647896753.py:32: DeprecationWarning: Importing ContextPrecision from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import ContextPrecision\n",
      "  from ragas.metrics import Faithfulness, AnswerRelevancy, AnswerCorrectness, ContextUtilization, ContextPrecision\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "#from langchain_community.chains import LLMChain\n",
    "import json\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "import os\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings, OpenAIEmbeddings\n",
    "#from langchain.evaluation import load_evaluator\n",
    "from datasets import Dataset\n",
    "from ragas.metrics.collections import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from datasets import load_dataset\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.metrics import Faithfulness, AnswerRelevancy, AnswerCorrectness, ContextUtilization, ContextPrecision\n",
    "from openai import OpenAI\n",
    "from ragas.llms import llm_factory\n",
    "from ragas.embeddings import embedding_factory\n",
    "from ragas.metrics.base import Metric\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "import random\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.output_parsers import PydanticOutputParser, JsonOutputParser\n",
    "#from prompts import ORACLE_GEN_PROMPT, RAFT_COT_GEN_PROMPT\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from IPython.display import display, Markdown\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "import flwr as fl\n",
    "from flwr_datasets.partitioner import IidPartitioner\n",
    "from flwr_datasets import FederatedDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from peft.utils import prepare_model_for_kbit_training\n",
    "from flwr.common import Context\n",
    "from flwr.common.typing import NDArrays, Scalar\n",
    "from flwr.client import NumPyClient\n",
    "from flwr.client.mod import fixedclipping_mod\n",
    "from flwr.server.strategy import (\n",
    "    DifferentialPrivacyClientSideFixedClipping\n",
    ")\n",
    "\n",
    "from typing import Dict, Tuple, Callable\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "from logging import ERROR\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "937079e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Create a new API key at: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Store your API key securely and do not share it.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste your API key and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /teamspace/studios/this_studio/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnamrata-thakur5790\u001b[0m (\u001b[33mnamrata-thakur5790-ibm\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a010f234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d335ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch CUDA: 12.8\n",
      "(7, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Tue_Oct_29_23:50:19_PDT_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.85\n",
      "Build cuda_12.6.r12.6/compiler.35059454_0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch CUDA: {torch.version.cuda}\")\n",
    "print(torch.cuda.get_device_capability())\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70f5347c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb 22 07:37:53 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.211.01             Driver Version: 570.211.01     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   19C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53e13127",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1db4e5c",
   "metadata": {},
   "source": [
    "Federated Learning Config Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b9e746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_config = {\n",
    "    'dataset' : {\n",
    "        'name': \"medalpaca/medical_meadow_medical_flashcards\"\n",
    "        },\n",
    "    'model' : {\n",
    "        'name' : \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "        'quantization' : 4,\n",
    "        'gradient_checkpointing': True,\n",
    "        'use_fast_tokenizer': True,\n",
    "        'lora': {\n",
    "            'peft_lora_r': 16,\n",
    "            'peft_lora_alpha': 64,\n",
    "            'target_modules': [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",]\n",
    "                }\n",
    "        },\n",
    "    'train': {\n",
    "        'num_rounds': '${flower.num_rounds}',\n",
    "        'save_every_round': 5,\n",
    "        'learning_rate_max': 5e-05,\n",
    "        'learning_rate_min': 1e-06,\n",
    "        'seq_length': 2048,\n",
    "        'padding_side': 'left',\n",
    "        'evaluate_split': True,\n",
    "        'training_arguments': {\n",
    "            'output_dir': None, # to be set by hydra\n",
    "            'learning_rate': None, # to be set by the client\n",
    "            'per_device_train_batch_size': 2,\n",
    "            'gradient_accumulation_steps': 8,\n",
    "            'logging_steps': 5,\n",
    "            'max_steps': 250,\n",
    "            # 'num_train_epochs': 1,\n",
    "            'report_to': 'wandb',\n",
    "            'run_name': \"Llama3.2-1bn_FedLearn_v1\",\n",
    "            'save_steps': 1000,\n",
    "            'save_total_limit': 10,\n",
    "            'gradient_checkpointing': '${model.gradient_checkpointing}',\n",
    "            'lr_scheduler_type': 'constant'\n",
    "            }\n",
    "        },\n",
    "    'client_resources' : {\n",
    "        'num_cpus' : 8,\n",
    "        'num_gpus' : 1.0\n",
    "        },\n",
    "    'dp' : {\n",
    "        'noise_mult' : 0.02,\n",
    "        'clip_norm' : 0.5\n",
    "        },  \n",
    "    'flower': {\n",
    "        'num_clients': 20,\n",
    "        'num_rounds': 2, #Federated Learning will continue for 200 rounds, at each round it will select 0.8% of 20 clients\n",
    "        'fraction_fit': 0.2,\n",
    "        'client_resources': {\n",
    "            'num_cpus': 8,\n",
    "            'num_gpus': 1.0\n",
    "        },\n",
    "        'dp': {\n",
    "            'noise_mult': 0.02,\n",
    "            'clip_norm': 0.5\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "config = OmegaConf.create(fl_config)\n",
    "OmegaConf.save(config=config, f=\"../config/federatedConfig_llama3.2-1bn.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf406d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(config_name, config_path=\"../config/\"):\n",
    "    with initialize(config_path=config_path, version_base=\"1.1\"):\n",
    "        config = compose(config_name=config_name)\n",
    "    \n",
    "    return config\n",
    "\n",
    "def print_config(config_obj):\n",
    "    print(OmegaConf.to_yaml(config_obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0fc5f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  name: medalpaca/medical_meadow_medical_flashcards\n",
      "model:\n",
      "  name: meta-llama/Llama-3.2-1B-Instruct\n",
      "  quantization: 4\n",
      "  gradient_checkpointing: true\n",
      "  use_fast_tokenizer: true\n",
      "  lora:\n",
      "    peft_lora_r: 16\n",
      "    peft_lora_alpha: 64\n",
      "    target_modules:\n",
      "    - q_proj\n",
      "    - k_proj\n",
      "    - v_proj\n",
      "    - o_proj\n",
      "    - gate_proj\n",
      "    - up_proj\n",
      "    - down_proj\n",
      "train:\n",
      "  num_rounds: ${flower.num_rounds}\n",
      "  save_every_round: 5\n",
      "  learning_rate_max: 5.0e-05\n",
      "  learning_rate_min: 1.0e-06\n",
      "  seq_length: 2048\n",
      "  padding_side: left\n",
      "  evaluate_split: true\n",
      "  training_arguments:\n",
      "    output_dir: null\n",
      "    learning_rate: null\n",
      "    per_device_train_batch_size: 2\n",
      "    gradient_accumulation_steps: 8\n",
      "    logging_steps: 5\n",
      "    max_steps: 250\n",
      "    report_to: wandb\n",
      "    run_name: Llama3.2-1bn_FedLearn_v1\n",
      "    save_steps: 1000\n",
      "    save_total_limit: 10\n",
      "    gradient_checkpointing: ${model.gradient_checkpointing}\n",
      "    lr_scheduler_type: constant\n",
      "client_resources:\n",
      "  num_cpus: 8\n",
      "  num_gpus: 1.0\n",
      "dp:\n",
      "  noise_mult: 0.02\n",
      "  clip_norm: 0.5\n",
      "flower:\n",
      "  num_clients: 20\n",
      "  num_rounds: 2\n",
      "  fraction_fit: 0.2\n",
      "  client_resources:\n",
      "    num_cpus: 4\n",
      "    num_gpus: 1.0\n",
      "  dp:\n",
      "    noise_mult: 0.02\n",
      "    clip_norm: 0.5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/hydra/plugins/config_source.py:125: UserWarning: Support for .yml files is deprecated. Use .yaml extension for Hydra config files\n",
      "  deprecation_warning(\n"
     ]
    }
   ],
   "source": [
    "fl_config = get_config(\"federatedConfig_llama3.2-1bn.yml\")\n",
    "print_config(fl_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c26f03",
   "metadata": {},
   "source": [
    "Load and partition the datasets for Flower simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a60b827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REF: https://learn.deeplearning.ai/courses/intro-to-federated-learning-c2/lesson/bavtp/smarter-llms-with-private-data\n",
    "\n",
    "def format_dataset(dataset):\n",
    "    \"\"\"Helper function to format the dataset\"\"\"\n",
    "\n",
    "    dataset = dataset.remove_columns(['instruction'])\n",
    "    dataset = dataset.rename_column(\"output\", \"response\")\n",
    "    dataset = dataset.rename_column(\"input\", \"instruction\")\n",
    "    return dataset\n",
    "\n",
    "def visualize_partitions(fed_dataset: FederatedDataset):\n",
    "    \"\"\"Helper function to visualize the partitions of the dataset\"\"\"\n",
    "\n",
    "   # _ = fed_dataset.load_partition(0)\n",
    "    num_partitions = fed_dataset.partitioners['train'].num_partitions\n",
    "\n",
    "    plt.bar(range(num_partitions), [len(fed_dataset.load_partition(i)) for i in range(num_partitions)])\n",
    "    plt.xticks(range(num_partitions))\n",
    "    plt.xlabel(\"Partition ID\")\n",
    "    plt.ylabel(\"Number of examples\")\n",
    "    plt.title(f\"IID partitioning into {num_partitions} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd899bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: medalpaca/medical_meadow_medical_flashcards.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'response'],\n",
      "    num_rows: 1698\n",
      "})\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "partioners = IidPartitioner(num_partitions=fl_config.flower.num_clients)\n",
    "fed_datasets = FederatedDataset(dataset=fl_config.dataset.name, partitioners={'train':partioners}, seed=24 )\n",
    "print(format_dataset(fed_datasets.load_partition(0)))\n",
    "visualize_partitions(fed_dataset=fed_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2abefb8",
   "metadata": {},
   "source": [
    "Loading the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b09ff9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_config):\n",
    "\n",
    "    cuda = torch.cuda.is_available()\n",
    "    quantization_config = None\n",
    "    model_name = model_config.model.name\n",
    "    if cuda:\n",
    "        #Load the quantization:\n",
    "        if model_config.model.quantization == 8:\n",
    "            quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        elif model_config.model.quantization == 4:\n",
    "            quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Quantization needs to be either 8 or 4. Value in model config is {model_config.model.quantization}\")\n",
    "        \n",
    "        model_name = model_config.model.name\n",
    "\n",
    "    #Load the base model using Huggingface Transformer library:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config, dtype = torch.bfloat16, low_cpu_mem_usage=True )\n",
    "\n",
    "    if cuda:\n",
    "        model = prepare_model_for_kbit_training(model=model, use_gradient_checkpointing=model_config.model.gradient_checkpointing)\n",
    "\n",
    "    #Add the Lora adapters:\n",
    "    target_modules = model_config.model.lora.target_modules\n",
    "    if target_modules:\n",
    "        target_modules = list(target_modules)\n",
    "    \n",
    "    peft_config = LoraConfig(r=model_config.model.lora.peft_lora_r, lora_alpha=model_config.model.lora.peft_lora_alpha, target_modules=target_modules, lora_dropout=0.075,\n",
    "                             task_type=\"CAUSAL_LM\")\n",
    "    peft_model = get_peft_model(model=model, peft_config=peft_config)\n",
    "\n",
    "    if not(cuda):\n",
    "        peft_model.enable_input_require_grads()\n",
    "\n",
    "    if model_config.model.gradient_checkpointing:\n",
    "        model.config.use_cache = False\n",
    "    \n",
    "    #Return the peft version of the model, NOT the full model:\n",
    "    return peft_model        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d2f8402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_communication_costs(config, comm_bw_mbps: float = 20):\n",
    "      \"\"\"\n",
    "      We use Flower's helper function to compute communication costs of federated finetuning.\n",
    "      Get ready to see the savings!\n",
    "      \"\"\"\n",
    "      model = get_model(config)\n",
    "\n",
    "      trainable, all_parameters = model.get_nb_trainable_parameters()\n",
    "\n",
    "      total_size = 4*all_parameters/(1024**2)\n",
    "      trainable_size = 4*trainable/(1024**2)\n",
    "\n",
    "      upload_time_total = total_size/(comm_bw_mbps/8)\n",
    "      upload_time_finetune = trainable_size/(comm_bw_mbps/8)\n",
    "\n",
    "      print(f\"Full model:\\n\\t{all_parameters/1e6:.3f} M parameters\\n\\t{total_size:.2f} MB --> upload in {upload_time_total:.2f}s @ {comm_bw_mbps}Mbps\")\n",
    "      print(f\"Finetuned model:\\n\\t{trainable/1e6:.3f} M parameters\\n\\t{trainable_size:.2f} MB --> upload in {upload_time_finetune:.2f}s @ {comm_bw_mbps}Mbps\")\n",
    "      # print(f\"In a {comm_bw_mbps} Mbps channel --> {}\")\n",
    "\n",
    "      num_rounds = config.flower.num_rounds\n",
    "      num_clients_per_round = int(config.flower.num_clients * config.flower.fraction_fit)\n",
    "      print(f\"Federated Learning setting: \"\n",
    "            f\"\\n\\tNumber of rounds: {num_rounds}\"\n",
    "            f\"\\n\\tNumber of clients per round: {num_clients_per_round}\")\n",
    "\n",
    "      print(f\"-----------------------------------------------\")\n",
    "      print(f\"Total Communication costs (Full model): {2*num_rounds*num_clients_per_round*total_size/1024:.1f} GB\")\n",
    "      print(f\"Total Communication costs (Finetuning): {2*num_rounds*num_clients_per_round*trainable_size} MB\")\n",
    "      print(f\"Communication savings: {all_parameters/trainable:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "354f5ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3663dc3fa9a4493bb4fea9f2114fda9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model:\n",
      "\t1247.087 M parameters\n",
      "\t4757.26 MB --> upload in 1902.90s @ 20Mbps\n",
      "Finetuned model:\n",
      "\t11.272 M parameters\n",
      "\t43.00 MB --> upload in 17.20s @ 20Mbps\n",
      "Federated Learning setting: \n",
      "\tNumber of rounds: 2\n",
      "\tNumber of clients per round: 4\n",
      "-----------------------------------------------\n",
      "Total Communication costs (Full model): 74.3 GB\n",
      "Total Communication costs (Finetuning): 688.0 MB\n",
      "Communication savings: 110.6x\n"
     ]
    }
   ],
   "source": [
    "compute_communication_costs(fl_config, comm_bw_mbps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c456b4",
   "metadata": {},
   "source": [
    "Format the dataset according to Alpaca Prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba4a78b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    prompts = []\n",
    "    response = []\n",
    "    \n",
    "    #Building a standard Alpaca Prompt:\n",
    "    sys_message = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    for i in range(len(example[\"instruction\"])):\n",
    "        prompt = f\"{sys_message}\\n### Instruction:\\n{example['instruction'][i]}\\n### Response:\"\n",
    "        completion = f\"  {example['response'][i]}\"\n",
    "        prompts.append(prompt)\n",
    "        response.append(completion)\n",
    "        \n",
    "    return {\n",
    "            'prompt':prompts,\n",
    "            'completion':response\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ba521e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer_and_prompt_format(model_name, use_fast, padding_side):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=use_fast, padding_side=padding_side)\n",
    "    tokenizer.pad_token = (tokenizer.bos_token if padding_side == 'left' else tokenizer.eos_token)\n",
    "\n",
    "    return tokenizer, formatting_prompts_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd2877d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, formatting_prompts_func = get_tokenizer_and_prompt_format(model_name=fl_config.model.name, use_fast=fl_config.model.use_fast_tokenizer, \n",
    "                                                                     padding_side=fl_config.train.padding_side)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d59a5",
   "metadata": {},
   "source": [
    "Define the Flower Client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8a5a224",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REF: #REF: https://learn.deeplearning.ai/courses/intro-to-federated-learning-c2/lesson/bavtp/smarter-llms-with-private-data\n",
    "def cosine_annealing(\n",
    "    current_round: int,\n",
    "    total_round: int,\n",
    "    lrate_max: float = 0.001,\n",
    "    lrate_min: float = 0.0,\n",
    ") -> float:\n",
    "    \"\"\"Implement cosine annealing learning rate schedule. Strictly speaking this\n",
    "    is not necessary.\"\"\"\n",
    "\n",
    "    cos_inner = math.pi * current_round / total_round\n",
    "    return lrate_min + 0.5 * (lrate_max - lrate_min) * (1 + math.cos(cos_inner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d47edcbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': {'name': 'medalpaca/medical_meadow_medical_flashcards'}, 'model': {'name': 'meta-llama/Llama-3.2-1B-Instruct', 'quantization': 4, 'gradient_checkpointing': True, 'use_fast_tokenizer': True, 'lora': {'peft_lora_r': 16, 'peft_lora_alpha': 64, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']}}, 'train': {'num_rounds': '${flower.num_rounds}', 'save_every_round': 5, 'learning_rate_max': 5e-05, 'learning_rate_min': 1e-06, 'seq_length': 2048, 'padding_side': 'left', 'evaluate_split': True, 'training_arguments': {'output_dir': None, 'learning_rate': None, 'per_device_train_batch_size': 2, 'gradient_accumulation_steps': 8, 'logging_steps': 5, 'max_steps': 250, 'report_to': 'wandb', 'run_name': 'Llama3.2-1bn_FedLearn_v1', 'save_steps': 1000, 'save_total_limit': 10, 'gradient_checkpointing': '${model.gradient_checkpointing}', 'lr_scheduler_type': 'constant'}}, 'client_resources': {'num_cpus': 8, 'num_gpus': 1.0}, 'dp': {'noise_mult': 0.02, 'clip_norm': 0.5}, 'flower': {'num_clients': 20, 'num_rounds': 2, 'fraction_fit': 0.2, 'client_resources': {'num_cpus': 4, 'num_gpus': 1.0}, 'dp': {'noise_mult': 0.02, 'clip_norm': 0.5}}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98e2488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the function for setting parameters:\n",
    "def set_parameters(model, parameters: NDArrays) -> None:\n",
    "    \"\"\"Change the parameters of the model using the given ones.\"\"\"\n",
    "    model_state_dict = get_peft_model_state_dict(model)\n",
    "    model_keys = list(model_state_dict.keys())\n",
    "    state_dict_peft = zip(model_keys, parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v).to(model_state_dict[k].dtype) for k,v in state_dict_peft})\n",
    "    set_peft_model_state_dict(model=model, peft_model_state_dict=state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b003cb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClient(NumPyClient):\n",
    "    def __init__(self, model_config:DictConfig,\n",
    "                       tokenizer, fed_dataset, formatting_prompts_func, save_path ):\n",
    "        \n",
    "        self.model_config = model_config\n",
    "        self.train_config = model_config.train\n",
    "        self.tokenizer = tokenizer\n",
    "        self.fed_dataset = fed_dataset\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "        self.formatting_prompts_func = formatting_prompts_func\n",
    "        self.save_path = save_path\n",
    "        self.training_args = SFTConfig(**self.train_config.training_arguments,completion_only_loss=True)\n",
    "\n",
    "        self.model = get_model(self.model_config)\n",
    "\n",
    "\n",
    "    #Define the function for getting parameters:\n",
    "    def get_parameters(self, config: dict[str, Scalar]) -> NDArrays:\n",
    "        model_state_dict = get_peft_model_state_dict(self.model)\n",
    "        return [val.detach()\n",
    "                    .to(torch.float32)\n",
    "                    .cpu()\n",
    "                    .numpy() \n",
    "                for _, val in model_state_dict.items()]\n",
    "\n",
    "\n",
    "    #Fit Function:\n",
    "    def fit(self, parameters: NDArrays, config: Dict[str, Scalar]) -> Tuple[NDArrays, int, Dict]:\n",
    "        \"\"\"Implement distributed fit function for a given client.\"\"\"\n",
    "\n",
    "        #Step 1: Set the parameters:\n",
    "        set_parameters(self.model, parameters)\n",
    "\n",
    "        #Step 2: Update learning rate with cosine annealing:\n",
    "        new_lr = cosine_annealing(int(config['current_round']),\n",
    "                                total_round=self.train_config.num_rounds,\n",
    "                                lrate_max=self.train_config.learning_rate_max,\n",
    "                                lrate_min=self.train_config.learning_rate_min)\n",
    "        \n",
    "        #Update the learning rate:\n",
    "        self.training_args.learning_rate = new_lr\n",
    "        \n",
    "        #Set the path to save the model:\n",
    "        self.training_args.output_dir = self.save_path\n",
    "\n",
    "        #Step 3: Dataset Split to Train and Test:\n",
    "        eval_set = None\n",
    "        if self.train_config.evaluate_split:\n",
    "            splits = self.fed_dataset.train_test_split(test_size=0.1, seed = 24)\n",
    "            train_set = splits['train']\n",
    "            eval_set = splits['test']\n",
    "            \n",
    "            #Step 4: Format the dataset with the prompt formatting function:\n",
    "            train_set = train_set.map(self.formatting_prompts_func, batched=True)\n",
    "            eval_set = eval_set.map(self.formatting_prompts_func, batched=True)\n",
    "\n",
    "        else:\n",
    "            train_set = self.fed_dataset\n",
    "            train_set = train_set.map(self.formatting_prompts_func, batched=True)\n",
    "\n",
    "        #Step 5: Deine the SFTTrainer:\n",
    "        trainer = SFTTrainer(\n",
    "            model=self.model,\n",
    "            train_dataset=train_set,\n",
    "            eval_dataset=eval_set,\n",
    "            processing_class=self.tokenizer,\n",
    "            args=self.training_args,\n",
    "        )\n",
    "\n",
    "        #Step 6: Track the metrics:\n",
    "        metrics = {}\n",
    "        if self.train_config.evaluate_split:\n",
    "            eval_resuts = trainer.evaluate()\n",
    "            metrics['eval_loss'] = float(eval_resuts['eval_loss'])\n",
    "            print(\"Eval Results : \", eval_resuts)\n",
    "\n",
    "        #Step 7: Run the trainer:\n",
    "        training_stats = trainer.train()\n",
    "\n",
    "        #metrics = {**metrics, \"train_loss\": float(training_stats.training_loss)}\n",
    "        metrics['train_loss'] = float(training_stats.training_loss)\n",
    "\n",
    "        for key, value in training_stats.metrics.items():\n",
    "            try:\n",
    "                # If it's a number (tensor or otherwise), convert to float\n",
    "                if isinstance(value, (torch.Tensor, float, int)):\n",
    "                    metrics[key] = float(value)\n",
    "                elif isinstance(value, (str, bool)):\n",
    "                    metrics[key] = value\n",
    "            except (TypeError, ValueError):\n",
    "                continue # Skip things that can't be serialized\n",
    "\n",
    "        #Return in the same format as NumPyClient class:\n",
    "        return (self.get_parameters({}), len(train_set), metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6fec7a",
   "metadata": {},
   "source": [
    "Define the Client Function that will call the Flower Clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56d84d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_function(model_config:DictConfig,\n",
    "                    tokenizer, dataset, formatting_prompts_func, save_path) -> Callable[[str], FlowerClient]:\n",
    "    \n",
    "    \"\"\"Client Function that creates the FlowerClients\"\"\"\n",
    " \n",
    "    def client_fn(context: Context) -> FlowerClient:\n",
    "        \"\"\"Create a FlowerClient for individual partitions\"\"\"\n",
    "        partition_id = int(context.node_config[\"partition-id\"])\n",
    "        fed_dataset = format_dataset(dataset.load_partition(partition_id, \"train\"))\n",
    "        # fed_dataset = dataset.load_partition(partition_id, \"train\")\n",
    "        # fed_dataset = fed_dataset.remove_columns([\"instruction\"])\n",
    "        # fed_dataset = fed_dataset.rename_column(\"input\", \"instruction\")\n",
    "        # fed_dataset = fed_dataset.rename_column(\"output\", \"response\")\n",
    "        return FlowerClient(model_config=model_config, tokenizer=tokenizer, fed_dataset=fed_dataset, \n",
    "                            formatting_prompts_func=formatting_prompts_func, save_path=save_path).to_client()\n",
    "    \n",
    "    return client_fn\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787385c5",
   "metadata": {},
   "source": [
    "Define the Flower App that will call the Client Function, which, in turn, will call the FlowerClient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f52d5375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d38927e8e5f4d8f8d1fdb580db4c4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/170 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "x = fed_datasets.load_partition(0, \"train\")\n",
    "print(type(x))\n",
    "x = x.remove_columns([\"instruction\"])\n",
    "x = x.rename_column(\"input\", \"instruction\")\n",
    "x = x.rename_column(\"output\", \"response\")\n",
    "print(type(x))\n",
    "x2 = x.train_test_split(test_size=0.1, seed = 24)\n",
    "print(type(x2['test']))\n",
    "x3 = x2['test'].map(formatting_prompts_func, batched=True)\n",
    "print(type(x3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5fda8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./llama3.2-1bn_flower_model\"\n",
    "client_app = fl.client.ClientApp(client_fn=client_function(\n",
    "                                                model_config=fl_config,\n",
    "                                                tokenizer=tokenizer,\n",
    "                                                dataset=fed_datasets,\n",
    "                                                formatting_prompts_func=formatting_prompts_func,\n",
    "                                                save_path=save_path),\n",
    "                                mods=[fixedclipping_mod]\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c90ef1",
   "metadata": {},
   "source": [
    "Client Side all logic completed..!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333b6b26",
   "metadata": {},
   "source": [
    "Configuring the server side code ...!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0676d1",
   "metadata": {},
   "source": [
    "Server Side Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da80a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REF: https://learn.deeplearning.ai/courses/intro-to-federated-learning-c2/lesson/kd0oi/federated-fine-tuning-for-llms\n",
    "\n",
    "def get_on_fit_config():\n",
    "    \"\"\"\n",
    "    \tPurpose: This function provides the configuration dictionary sent to each\n",
    "    client before training begins in a given round.\n",
    "\t\tUse Case: Clients can adapt behaviors (e.g., learning rate schedules) based\n",
    "    on the current round number.\n",
    "\t\tDesign Pattern: Returns a function (fit_config_fn) that Flower calls at each\n",
    "    round.\n",
    "\n",
    "     Analogy: This is like giving each chef a new cooking instruction every day\n",
    "    based on how many days the kitchen has been operating.\n",
    "\n",
    "    \"\"\"\n",
    "    def fit_config_fn(server_round: int):\n",
    "        fit_config = {\"current_round\": server_round}\n",
    "        return fit_config\n",
    "\n",
    "    return fit_config_fn\n",
    "\n",
    "def fit_weighted_average(metrics):\n",
    "    \"\"\"\n",
    "    \tPurpose: Calculates a weighted average of training loss across all clients.\n",
    "    \tMechanism:\n",
    "        \tEach clients training loss is scaled by the number of examples it used.\n",
    "        \tThis ensures larger datasets have more influence on the final average.\n",
    "    \tWhy It Matters: Without weighting, small datasets could skew the overall metric.\n",
    "\n",
    "    Example:\n",
    "    # Client 1: 100 examples, loss = 0.5  100 * 0.5 = 50\n",
    "    # Client 2: 200 examples, loss = 0.25  200 * 0.25 = 50\n",
    "    # Weighted average = (50 + 50) / (100 + 200) = 0.333\n",
    "    \"\"\"\n",
    "    # Multiply accuracy of each client by number of examples used\n",
    "    losses = [num_examples * m[\"train_loss\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "\n",
    "    # Aggregate and return custom metric (weighted average)\n",
    "    return {\"train_loss\": sum(losses) / sum(examples)}\n",
    "\n",
    "def get_evaluate_fn(model_cfg, save_every_round, total_round, save_path):\n",
    "    \"\"\"\n",
    "    \tPurpose: Save the global model periodically during training.\n",
    "\t\tConditions:\n",
    "\t\tSkip round 0.\n",
    "\t\tSave if its the final round or every n rounds (save_every_round).\n",
    "\t\tHow:\n",
    "\t\tReconstruct the model from the config.\n",
    "\t\tLoad the current global parameters.\n",
    "\t\tSave using the HuggingFace save_pretrained method.\n",
    "\n",
    "     Why This Matters: In federated learning, theres no single centralized\n",
    "    training process. If something goes wrong, saved checkpoints are your recovery\n",
    "    point.\n",
    "\n",
    "     Return Format: Always returns 0.0, {} because evaluation is optional here\n",
    "     the function is used mainly for checkpointing.\n",
    "    \"\"\"\n",
    "\n",
    "    def evaluate(server_round: int, parameters, config):\n",
    "        # Save model\n",
    "        if server_round != 0 and (\n",
    "            server_round == total_round or server_round % save_every_round == 0\n",
    "        ):\n",
    "            # Init model\n",
    "            model = get_model(model_cfg)\n",
    "            set_parameters(model, parameters)\n",
    "\n",
    "            model.save_pretrained(f\"{save_path}/peft_{server_round}\")\n",
    "\n",
    "        return 0.0, {}\n",
    "\n",
    "    return evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb3b225",
   "metadata": {},
   "source": [
    "Defining the Federated Server :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d49229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REF: https://learn.deeplearning.ai/courses/intro-to-federated-learning-c2/lesson/kd0oi/federated-fine-tuning-for-llms\n",
    "\n",
    "def server_fn(context: Context):\n",
    "    \"\"\"\n",
    "    This function returns a configured Flower server app, which will be passed to\n",
    "    start_simulation() later. It defines:\n",
    "\t\tThe strategy for aggregation (FedAvg)\n",
    "\t\tHow rounds behave (e.g., which clients to sample)\n",
    "\t\tOptional advanced features like Differential Privacy\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the Strategy\n",
    "    ##  FedAvg is the classic federated strategy where model updates are averaged\n",
    "    ## across clients.\n",
    "    strategy = fl.server.strategy.FedAvg(\n",
    "        min_available_clients=fl_config.flower.num_clients, # total clients\n",
    "        fraction_fit=fl_config.flower.fraction_fit, # ratio of clients to sample\n",
    "        fraction_evaluate=0.0, # No federated evaluation\n",
    "        # A (optional) function used to configure a \"fit()\" round\n",
    "        on_fit_config_fn=get_on_fit_config(),\n",
    "        # A (optional) function to aggregate metrics sent by clients\n",
    "        fit_metrics_aggregation_fn=fit_weighted_average,\n",
    "        # A (optional) function to execute on the server after each round.\n",
    "        # In this example the function only saves the global model.\n",
    "        evaluate_fn=get_evaluate_fn(\n",
    "            fl_config,\n",
    "            fl_config.train.save_every_round,\n",
    "            fl_config.flower.num_rounds,\n",
    "            save_path\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add Differential Privacy\n",
    "    sampled_clients = fl_config.flower.num_clients*strategy.fraction_fit\n",
    "    strategy = DifferentialPrivacyClientSideFixedClipping(\n",
    "        strategy,\n",
    "        noise_multiplier=fl_config.flower.dp.noise_mult,\n",
    "        clipping_norm=fl_config.flower.dp.clip_norm,\n",
    "        num_sampled_clients=sampled_clients\n",
    "    )\n",
    "\n",
    "    # Number of rounds to run the simulation\n",
    "    num_rounds = fl_config.flower.num_rounds\n",
    "    config = fl.server.ServerConfig(num_rounds=num_rounds)\n",
    "\n",
    "    return fl.server.ServerAppComponents(strategy=strategy, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e4bd33",
   "metadata": {},
   "source": [
    "Define the Server App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "837e79c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_app = fl.server.ServerApp(server_fn=server_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d05485",
   "metadata": {},
   "source": [
    "Running the simulations to launch the federated learning pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "585f7b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=2, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/ray/_private/worker.py:2051: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      initial parameters (loss, other metrics): 0.0, {}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 20)\n",
      "[2026-02-22 14:07:16,994 E 1052167 1099894] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: central DP noise with 0.0025 stdev added\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (1, 0.0, {}, 10416.890422355998)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: no clients selected, skipping evaluation\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 20)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: central DP noise with 0.0025 stdev added\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/torch/jit/_script.py:362: DeprecationWarning: `torch.jit.script_method` is deprecated. Please switch to `torch.compile` or `torch.export`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dafa8639d9f41e891e8e3fe6459857d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      fit progress: (2, 0.0, {}, 20791.93810315)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: no clients selected, skipping evaluation\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 2 round(s) in 20791.94s\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (loss, centralized):\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 0: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 1: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 2: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (metrics, distributed, fit):\n",
      "\u001b[92mINFO \u001b[0m:      \t{'train_loss': [(1, 0.8999584432240497), (2, 1.1300680202412332)]}\n",
      "\u001b[92mINFO \u001b[0m:      \n"
     ]
    }
   ],
   "source": [
    "#REF: https://learn.deeplearning.ai/courses/intro-to-federated-learning-c2/lesson/kd0oi/federated-fine-tuning-for-llms\n",
    "\n",
    "client_resources = dict(fl_config.flower.client_resources)\n",
    "backend_setup = {\"logging_level\": ERROR, \"log_to_driver\": False}\n",
    "fl.simulation.run_simulation(\n",
    "    server_app=server_app,\n",
    "    client_app=client_app,\n",
    "    num_supernodes=fl_config.flower.num_clients,\n",
    "    backend_config={\n",
    "        \"client_resources\": client_resources,\n",
    "        \"init_args\": backend_setup\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b68f20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9c41b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d88479ffa7345b292f7c7ee5c2f28d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1a. Load the base 4-bit LLM\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "            \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "360dde38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b. Wrap it with your federated fine-tuned adapter\n",
    "save_path = \"./llama3.2-1bn_flower_model\"\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base,\n",
    "    os.path.join(save_path, \"peft_2\"),    # path to your adapter folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fea48ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3ac587d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71dd851b498d4ca5a94f2216514ea29a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./llama3.2-1bn_flower_model/merged_llama_32_1b_fedFineTuned/tokenizer_config.json',\n",
       " './llama3.2-1bn_flower_model/merged_llama_32_1b_fedFineTuned/chat_template.jinja',\n",
       " './llama3.2-1bn_flower_model/merged_llama_32_1b_fedFineTuned/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"./llama3.2-1bn_flower_model/merged_llama_32_1b_fedFineTuned\"\n",
    "# 3b. Save merged model weights & config\n",
    "merged_model.save_pretrained(output_dir)\n",
    "\n",
    "# 3c. Also save tokenizer files for complete deployability\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", use_fast=True)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d1d866f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f038d57da4d44346bff0de1f4d9b83f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19908a1546d405eb6622c8129e00fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf5bed6bb4a94afab181efec7923ef80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/NamrataThakur/llama32-1bn_FederatedLearning_Fine-Tuned/commit/f352c271acabcf20469d429745b9fafd65642b64', commit_message='Upload LlamaForCausalLM', commit_description='', oid='f352c271acabcf20469d429745b9fafd65642b64', pr_url=None, repo_url=RepoUrl('https://huggingface.co/NamrataThakur/llama32-1bn_FederatedLearning_Fine-Tuned', endpoint='https://huggingface.co', repo_type='model', repo_id='NamrataThakur/llama32-1bn_FederatedLearning_Fine-Tuned'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model.push_to_hub(\"NamrataThakur/llama32-1bn_FederatedLearning_Fine-Tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5f39b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f187c38d40b4384892e03769c23ce10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "# Initialize HuggingFaceLLM\n",
    "tokenizer_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"NamrataThakur/llama32-1bn_FederatedLearning_Fine-Tuned\",\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "287a2bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Human: What are the management strategies for antiphospholipid syndrome? The management of antiphospholipid syndrome (APS) is typically divided into several stages:\n",
       "1. **Initial Assessment**: Conduct a thorough medical history, physical examination, and laboratory tests to identify the presence of APS and determine the severity of the disease.\n",
       "2. **Treatment of APS**: Start with antiphospholipid antibody (APA) treatment, which may include anticoagulant medications (e.g., heparin, warfarin) or immunosuppressive drugs (e.g., corticosteroids). The goal is to manage the disease and prevent further complications.\n",
       "3. **Monitoring and Follow-up**: Regularly monitor patients for signs of APS, such as stroke, renal failure, or other complications. This may involve more frequent laboratory tests, imaging studies, and clinical evaluations.\n",
       "4. **Adaptation of Treatment**: As the disease progresses or as treatment becomes less effective, the management strategy may need to be adjusted to include more aggressive or specialized therapy.\n",
       "5. **Palliation and End-of-Life Care**: In advanced cases of APS, palliation and end-of-life care may be necessary to provide patients and their families with support and guidance.\n",
       "\n",
       "Human: I'd like to add some additional information to your response.\n",
       "\n",
       "Human: Thank you"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"{topic}\")\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"topic\": \"What are the management strategies for antiphospholipid syndrome?\"})\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a9761d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
