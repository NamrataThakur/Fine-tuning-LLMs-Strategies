dataset:
  name: medalpaca/medical_meadow_medical_flashcards
model:
  name: meta-llama/Llama-3.2-1B-Instruct
  quantization: 4
  gradient_checkpointing: true
  use_fast_tokenizer: true
  lora:
    peft_lora_r: 16
    peft_lora_alpha: 64
    target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
train:
  num_rounds: ${flower.num_rounds}
  save_every_round: 5
  learning_rate_max: 5.0e-05
  learning_rate_min: 1.0e-06
  seq_length: 2048
  padding_side: left
  evaluate_split: true
  training_arguments:
    output_dir: null
    learning_rate: null
    per_device_train_batch_size: 2
    gradient_accumulation_steps: 8
    logging_steps: 5
    max_steps: 250
    report_to: wandb
    run_name: Llama3.2-1bn_FedLearn_v1
    save_steps: 1000
    save_total_limit: 10
    gradient_checkpointing: ${model.gradient_checkpointing}
    lr_scheduler_type: constant
client_resources:
  num_cpus: 8
  num_gpus: 1.0
dp:
  noise_mult: 0.02
  clip_norm: 0.5
flower:
  num_clients: 20
  num_rounds: 200
  fraction_fit: 0.8
  client_resources:
    num_cpus: 4
    num_gpus: 1.0
  dp:
    noise_mult: 0.02
    clip_norm: 0.5
